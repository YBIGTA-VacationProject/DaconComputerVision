{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "dacon_2.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "PBOpBGqCLPNS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "35a9feab-1c46-4667-a265-8f20917eff28"
      },
      "source": [
        "from google.colab import drive\r\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L_GG1i7eLX-b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d7ff9df0-5fba-41b2-fb2f-9b5b051688b7"
      },
      "source": [
        "from google.colab import output\r\n",
        "# !cp 파일1 파일2 # 파일1을 파일2로 복사 붙여넣기\r\n",
        "# !cp \"/content/gdrive/MyDrive/data/data_2.zip\" \"data_2.zip\"\r\n",
        "# data_2.zip을 현재 디렉터리에 압축해제\r\n",
        "\r\n",
        "\r\n",
        "!unzip \"/content/gdrive/MyDrive/data_2.zip\""
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  /content/gdrive/MyDrive/data_2.zip\n",
            "  inflating: dirty_mnist_2nd.zip     \n",
            "  inflating: dirty_mnist_2nd_answer.csv  \n",
            "  inflating: mnist_data.zip          \n",
            "  inflating: sample_submission.csv   \n",
            "  inflating: test_dirty_mnist_2nd.zip  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o7IZ9msZLZBe"
      },
      "source": [
        "from google.colab import output\r\n",
        "# 현재 디렉터리에 dirty_mnist라는 폴더 생성\r\n",
        "!mkdir \"./dirty_mnist\"\r\n",
        "#dirty_mnist.zip라는 zip파일을 dirty_mnist라는 폴더에 압축 풀기\r\n",
        "!unzip \"dirty_mnist_2nd.zip\" -d \"./dirty_mnist/\"\r\n",
        "# 현재 디렉터리에 test_dirty_mnist라는 폴더 생성\r\n",
        "!mkdir \"./test_dirty_mnist\"\r\n",
        "#test_dirty_mnist.zip라는 zip파일을 test_dirty_mnist라는 폴더에 압축 풀기\r\n",
        "!unzip \"test_dirty_mnist_2nd.zip\" -d \"./test_dirty_mnist/\"\r\n",
        "# 출력 결과 지우기\r\n",
        "output.clear()"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gclpm7WTLZ1r"
      },
      "source": [
        "import pandas as pd\r\n",
        "import numpy as np\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import cv2\r\n",
        "from tqdm import tqdm\r\n",
        "import glob\r\n",
        "import time\r\n",
        "import imutils\r\n",
        "import zipfile\r\n",
        "import os\r\n",
        "from PIL import Image\r\n",
        "from torchsummary import summary\r\n",
        "\r\n",
        "import torch\r\n",
        "import torch.nn as nn\r\n",
        "import torch.nn.functional as F\r\n",
        "import torchvision.models as models\r\n",
        "import torchvision.transforms as T\r\n",
        "from torch.utils.data import DataLoader, Dataset\r\n",
        "from google.colab import output\r\n",
        "from sklearn.model_selection import KFold\r\n",
        "\r\n",
        "from torch.hub import load_state_dict_from_url\r\n",
        "from torchvision.models import ResNet\r\n",
        "from typing import Type, Any, Callable, Union, List, Optional\r\n",
        "from torch import Tensor\r\n",
        "\r\n",
        "\r\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") # 디바이스 설정\r\n",
        "\r\n",
        "learning_rate = 0.001\r\n",
        "training_epochs = 15\r\n",
        "batch_size = 100\r\n",
        "drop_prob = 0.3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WK64zd39Llm4"
      },
      "source": [
        "dirty_mnist_answer = pd.read_csv(\"/content/dirty_mnist_2nd_answer.csv\")\r\n",
        "# dirty_mnist라는 디렉터리 속에 들어있는 파일들의 이름을 \r\n",
        "# namelist라는 변수에 저장\r\n",
        "namelist = os.listdir('./dirty_mnist/')\r\n",
        "\r\n",
        "# unmpy를 tensor로 변환하는 ToTensor 정의\r\n",
        "class ToTensor(object):\r\n",
        "    \"\"\"numpy array를 tensor(torch)로 변환합니다.\"\"\"\r\n",
        "    def __call__(self, sample):\r\n",
        "        image, label = sample['image'], sample['label']\r\n",
        "        # swap color axis because\r\n",
        "        # numpy image: H x W x C\r\n",
        "        # torch image: C X H X W\r\n",
        "        image = image.transpose((2, 0, 1))\r\n",
        "        return {'image': torch.FloatTensor(image),\r\n",
        "                'label': torch.FloatTensor(label)}\r\n",
        "# to_tensor 선언\r\n",
        "to_tensor = T.Compose([\r\n",
        "                      ToTensor()\r\n",
        "                    ])\r\n",
        "\r\n",
        "class DatasetMNIST(torch.utils.data.Dataset):\r\n",
        "    def __init__(self,\r\n",
        "                 dir_path,\r\n",
        "                 meta_df,\r\n",
        "                 transforms=to_tensor,#미리 선언한 to_tensor를 transforms로 받음\r\n",
        "                 augmentations=None):\r\n",
        "        \r\n",
        "        self.dir_path = dir_path # 데이터의 이미지가 저장된 디렉터리 경로\r\n",
        "        self.meta_df = meta_df # 데이터의 인덱스와 정답지가 들어있는 DataFrame\r\n",
        "\r\n",
        "        self.transforms = transforms# Transform\r\n",
        "        self.augmentations = augmentations # Augmentation\r\n",
        "        \r\n",
        "    def __len__(self):\r\n",
        "        return len(self.meta_df)\r\n",
        "    \r\n",
        "    def __getitem__(self, index):\r\n",
        "        # 폴더 경로 + 이미지 이름 + .png => 파일의 경로\r\n",
        "        # 참고) \"12\".zfill(5) => 000012\r\n",
        "        #       \"146\".zfill(5) => 000145\r\n",
        "        # cv2.IMREAD_GRAYSCALE : png파일을 채널이 1개인 GRAYSCALE로 읽음\r\n",
        "        image = cv2.imread(self.dir_path +\\\r\n",
        "                           str(self.meta_df.iloc[index,0]).zfill(5) + '.png',\r\n",
        "                           cv2.IMREAD_GRAYSCALE)\r\n",
        "        # 0 ~ 255의 값을 갖고 크기가 (256,256)인 numpy array를\r\n",
        "        # 0 ~ 1 사이의 실수를 갖고 크기가 (256,256,1)인 numpy array로 변환\r\n",
        "        image = (image/255).astype('float')[..., np.newaxis]\r\n",
        "        # 정답 numpy array생성(존재하면 1 없으면 0)\r\n",
        "        label = self.meta_df.iloc[index, 1:].values.astype('float')\r\n",
        "        sample = {'image': image, 'label': label}\r\n",
        "        # transform 적용\r\n",
        "        # numpy to tensor\r\n",
        "        if self.transforms:\r\n",
        "            sample = self.transforms(sample)\r\n",
        "\r\n",
        "        # sample 반환\r\n",
        "        return sample"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wBShkZsvHpac"
      },
      "source": [
        "class MnistDataset_v1(Dataset):\r\n",
        "    def __init__(self, imgs_dir=None, labels=None, transform=None, train=True):\r\n",
        "        self.imgs_dir = imgs_dir\r\n",
        "        self.labels = labels\r\n",
        "        self.transform = transform\r\n",
        "        self.train = train\r\n",
        "        pass\r\n",
        "    \r\n",
        "    def __len__(self):\r\n",
        "        # 데이터 총 샘플 수\r\n",
        "        return len(self.imgs_dir)\r\n",
        "    \r\n",
        "    def __getitem__(self, idx):\r\n",
        "        # 1개 샘플 get\r\n",
        "        img = cv2.imread(self.imgs_dir[idx], cv2.IMREAD_COLOR)\r\n",
        "        img = self.transform(img)\r\n",
        "        if self.train==True:\r\n",
        "            label = self.labels[idx]\r\n",
        "            return img, label\r\n",
        "        else:\r\n",
        "            return img\r\n",
        "        \r\n",
        "        pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RfombIfELzsf"
      },
      "source": [
        "def conv1x1(in_planes: int, out_planes: int, stride: int = 1) -> nn.Conv2d:\r\n",
        "    \"\"\"1x1 convolution\"\"\"\r\n",
        "    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)\r\n",
        "\r\n",
        "def conv3x3(in_planes: int, out_planes: int, stride: int = 1, groups: int = 1, dilation: int = 1) -> nn.Conv2d:\r\n",
        "    \"\"\"3x3 convolution with padding\"\"\"\r\n",
        "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\r\n",
        "                     padding=dilation, groups=groups, bias=False, dilation=dilation)\r\n",
        "    \r\n",
        "class BasicBlock(nn.Module):\r\n",
        "    expansion: int = 1\r\n",
        "\r\n",
        "    def __init__(\r\n",
        "        self,\r\n",
        "        inplanes: int,\r\n",
        "        planes: int,\r\n",
        "        stride: int = 1,\r\n",
        "        downsample: Optional[nn.Module] = None,\r\n",
        "        groups: int = 1,\r\n",
        "        base_width: int = 64,\r\n",
        "        dilation: int = 1,\r\n",
        "        norm_layer: Optional[Callable[..., nn.Module]] = None\r\n",
        "    ) -> None:\r\n",
        "        super(BasicBlock, self).__init__()\r\n",
        "        if norm_layer is None:\r\n",
        "            norm_layer = nn.BatchNorm2d\r\n",
        "        if groups != 1 or base_width != 64:\r\n",
        "            raise ValueError('BasicBlock only supports groups=1 and base_width=64')\r\n",
        "        if dilation > 1:\r\n",
        "            raise NotImplementedError(\"Dilation > 1 not supported in BasicBlock\")\r\n",
        "        # Both self.conv1 and self.downsample layers downsample the input when stride != 1\r\n",
        "        self.conv1 = conv3x3(inplanes, planes, stride)\r\n",
        "        self.bn1 = norm_layer(planes)\r\n",
        "        self.relu = nn.ReLU(inplace=True)\r\n",
        "        self.dropout1 = torch.nn.Dropout(p=0.3)\r\n",
        "        self.conv2 = conv3x3(planes, planes)\r\n",
        "        self.bn2 = norm_layer(planes)\r\n",
        "        self.dropout2 = torch.nn.Dropout(p=0.3)\r\n",
        "        self.downsample = downsample\r\n",
        "        self.stride = stride\r\n",
        "\r\n",
        "    def forward(self, x: Tensor) -> Tensor:\r\n",
        "        identity = x\r\n",
        "\r\n",
        "        out = self.conv1(x)\r\n",
        "        out = self.bn1(out)\r\n",
        "        out = self.relu(out)\r\n",
        "        out = self.dropout1(out)\r\n",
        "        out = self.conv2(out)\r\n",
        "        out = self.bn2(out)\r\n",
        "\r\n",
        "        if self.downsample is not None:\r\n",
        "            identity = self.downsample(x)\r\n",
        "\r\n",
        "        out += identity\r\n",
        "        out = self.relu(out)\r\n",
        "        out = self.dropout2(out)\r\n",
        "\r\n",
        "        return out\r\n",
        "\r\n",
        "class Bottleneck(nn.Module):\r\n",
        "    # Bottleneck in torchvision places the stride for downsampling at 3x3 convolution(self.conv2)\r\n",
        "    # while original implementation places the stride at the first 1x1 convolution(self.conv1)\r\n",
        "    # according to \"Deep residual learning for image recognition\"https://arxiv.org/abs/1512.03385.\r\n",
        "    # This variant is also known as ResNet V1.5 and improves accuracy according to\r\n",
        "    # https://ngc.nvidia.com/catalog/model-scripts/nvidia:resnet_50_v1_5_for_pytorch.\r\n",
        "\r\n",
        "    expansion: int = 4\r\n",
        "\r\n",
        "    def __init__(\r\n",
        "        self,\r\n",
        "        inplanes: int,\r\n",
        "        planes: int,\r\n",
        "        stride: int = 1,\r\n",
        "        downsample: Optional[nn.Module] = None,\r\n",
        "        groups: int = 1,\r\n",
        "        base_width: int = 64,\r\n",
        "        dilation: int = 1,\r\n",
        "        norm_layer: Optional[Callable[..., nn.Module]] = None\r\n",
        "    ) -> None:\r\n",
        "        super(Bottleneck, self).__init__()\r\n",
        "        if norm_layer is None:\r\n",
        "            norm_layer = nn.BatchNorm2d\r\n",
        "        width = int(planes * (base_width / 64.)) * groups\r\n",
        "        # Both self.conv2 and self.downsample layers downsample the input when stride != 1\r\n",
        "        self.conv1 = conv1x1(inplanes, width)\r\n",
        "        self.bn1 = norm_layer(width)\r\n",
        "        self.conv2 = conv3x3(width, width, stride, groups, dilation)\r\n",
        "        self.bn2 = norm_layer(width)\r\n",
        "        self.conv3 = conv1x1(width, planes * self.expansion)\r\n",
        "        self.bn3 = norm_layer(planes * self.expansion)\r\n",
        "        self.relu = nn.ReLU(inplace=True)\r\n",
        "        self.dropout1 = torch.nn.Dropout(p=0.3)\r\n",
        "        self.dropout2 = torch.nn.Dropout(p=0.3)\r\n",
        "        self.dropout3 = torch.nn.Dropout(p=0.3)\r\n",
        "\r\n",
        "        self.downsample = downsample\r\n",
        "        self.stride = stride\r\n",
        "\r\n",
        "    def forward(self, x: Tensor) -> Tensor:\r\n",
        "        identity = x\r\n",
        "\r\n",
        "        out = self.conv1(x)\r\n",
        "        out = self.bn1(out)\r\n",
        "        out = self.relu(out)\r\n",
        "        out = self.dropout1(out)\r\n",
        "        out = self.conv2(out)\r\n",
        "        out = self.bn2(out)\r\n",
        "        out = self.relu(out)\r\n",
        "        out = self.dropout2(out)\r\n",
        "        out = self.conv3(out)\r\n",
        "        out = self.bn3(out)\r\n",
        "\r\n",
        "        if self.downsample is not None:\r\n",
        "            identity = self.downsample(x)\r\n",
        "\r\n",
        "        out += identity\r\n",
        "        out = self.relu(out)\r\n",
        "        out = self.dropout3(out)\r\n",
        "        return out\r\n",
        "\r\n",
        "class ResNet(nn.Module):\r\n",
        "\r\n",
        "    def __init__(\r\n",
        "        self,\r\n",
        "        block: Type[Union[BasicBlock, Bottleneck]],\r\n",
        "        layers: List[int],\r\n",
        "        num_classes: int = 1000,\r\n",
        "        zero_init_residual: bool = False,\r\n",
        "        groups: int = 1,\r\n",
        "        width_per_group: int = 64,\r\n",
        "        replace_stride_with_dilation: Optional[List[bool]] = None,\r\n",
        "        norm_layer: Optional[Callable[..., nn.Module]] = None\r\n",
        "    ) -> None:\r\n",
        "        super(ResNet, self).__init__()\r\n",
        "        if norm_layer is None:\r\n",
        "            norm_layer = nn.BatchNorm2d\r\n",
        "        self._norm_layer = norm_layer\r\n",
        "\r\n",
        "        self.inplanes = 64\r\n",
        "        self.dilation = 1\r\n",
        "        if replace_stride_with_dilation is None:\r\n",
        "            # each element in the tuple indicates if we should replace\r\n",
        "            # the 2x2 stride with a dilated convolution instead\r\n",
        "            replace_stride_with_dilation = [False, False, False]\r\n",
        "        if len(replace_stride_with_dilation) != 3:\r\n",
        "            raise ValueError(\"replace_stride_with_dilation should be None \"\r\n",
        "                             \"or a 3-element tuple, got {}\".format(replace_stride_with_dilation))\r\n",
        "        self.groups = groups\r\n",
        "        self.base_width = width_per_group\r\n",
        "        self.conv1 = nn.Conv2d(3, self.inplanes, kernel_size=7, stride=2, padding=3,\r\n",
        "                               bias=False)\r\n",
        "        self.bn1 = norm_layer(self.inplanes)\r\n",
        "        self.relu = nn.ReLU(inplace=True)\r\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\r\n",
        "        self.layer1 = self._make_layer(block, 64, layers[0])\r\n",
        "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2,\r\n",
        "                                       dilate=replace_stride_with_dilation[0])\r\n",
        "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2,\r\n",
        "                                       dilate=replace_stride_with_dilation[1])\r\n",
        "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2,\r\n",
        "                                       dilate=replace_stride_with_dilation[2])\r\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\r\n",
        "        self.fc = nn.Linear(512 * block.expansion, num_classes)\r\n",
        "\r\n",
        "        for m in self.modules():\r\n",
        "            if isinstance(m, nn.Conv2d):\r\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\r\n",
        "            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\r\n",
        "                nn.init.constant_(m.weight, 1)\r\n",
        "                nn.init.constant_(m.bias, 0)\r\n",
        "\r\n",
        "        # Zero-initialize the last BN in each residual branch,\r\n",
        "        # so that the residual branch starts with zeros, and each residual block behaves like an identity.\r\n",
        "        # This improves the model by 0.2~0.3% according to https://arxiv.org/abs/1706.02677\r\n",
        "        if zero_init_residual:\r\n",
        "            for m in self.modules():\r\n",
        "                if isinstance(m, Bottleneck):\r\n",
        "                    nn.init.constant_(m.bn3.weight, 0)  # type: ignore[arg-type]\r\n",
        "                elif isinstance(m, BasicBlock):\r\n",
        "                    nn.init.constant_(m.bn2.weight, 0)  # type: ignore[arg-type]\r\n",
        "\r\n",
        "    def _make_layer(self, block: Type[Union[BasicBlock, Bottleneck]], planes: int, blocks: int,\r\n",
        "                    stride: int = 1, dilate: bool = False) -> nn.Sequential:\r\n",
        "        norm_layer = self._norm_layer\r\n",
        "        downsample = None\r\n",
        "        previous_dilation = self.dilation\r\n",
        "        if dilate:\r\n",
        "            self.dilation *= stride\r\n",
        "            stride = 1\r\n",
        "        if stride != 1 or self.inplanes != planes * block.expansion:\r\n",
        "            downsample = nn.Sequential(\r\n",
        "                conv1x1(self.inplanes, planes * block.expansion, stride),\r\n",
        "                norm_layer(planes * block.expansion),\r\n",
        "            )\r\n",
        "\r\n",
        "        layers = []\r\n",
        "        layers.append(block(self.inplanes, planes, stride, downsample, self.groups,\r\n",
        "                            self.base_width, previous_dilation, norm_layer))\r\n",
        "        self.inplanes = planes * block.expansion\r\n",
        "        for _ in range(1, blocks):\r\n",
        "            layers.append(block(self.inplanes, planes, groups=self.groups,\r\n",
        "                                base_width=self.base_width, dilation=self.dilation,\r\n",
        "                                norm_layer=norm_layer))\r\n",
        "\r\n",
        "        return nn.Sequential(*layers)\r\n",
        "\r\n",
        "    def _forward_impl(self, x: Tensor) -> Tensor:\r\n",
        "        # See note [TorchScript super()]\r\n",
        "        x = self.conv1(x)\r\n",
        "        x = self.bn1(x)\r\n",
        "        x = self.relu(x)\r\n",
        "        x = self.maxpool(x)\r\n",
        "\r\n",
        "        x = self.layer1(x)\r\n",
        "        x = self.layer2(x)\r\n",
        "        x = self.layer3(x)\r\n",
        "        x = self.layer4(x)\r\n",
        "\r\n",
        "        x = self.avgpool(x)\r\n",
        "        x = torch.flatten(x, 1)\r\n",
        "        x = self.fc(x)\r\n",
        "\r\n",
        "        return x\r\n",
        "\r\n",
        "    def forward(self, x: Tensor) -> Tensor:\r\n",
        "        return self._forward_impl(x)\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VCxvm3kXL2bL"
      },
      "source": [
        "from typing import Type, Any, Union\r\n",
        "def _resnet(\r\n",
        "    arch: str,\r\n",
        "    block: Type[Union[BasicBlock, Bottleneck]],\r\n",
        "    layers: List[int],\r\n",
        "    pretrained: bool,\r\n",
        "    progress: bool,\r\n",
        "    **kwargs: Any\r\n",
        ") -> ResNet:\r\n",
        "    model = ResNet(block, layers, **kwargs)\r\n",
        "    #model.load_state_dict(state_dict)\r\n",
        "    return model\r\n",
        "\r\n",
        "\r\n",
        "def resnext50_32x4d(pretrained: bool = False, progress: bool = True, **kwargs: Any) -> ResNet:\r\n",
        "    r\"\"\"ResNeXt-50 32x4d model from\r\n",
        "    `\"Aggregated Residual Transformation for Deep Neural Networks\" <https://arxiv.org/pdf/1611.05431.pdf>`_.\r\n",
        "    Args:\r\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\r\n",
        "        progress (bool): If True, displays a progress bar of the download to stderr\r\n",
        "    \"\"\"\r\n",
        "    kwargs['groups'] = 32\r\n",
        "    kwargs['width_per_group'] = 4\r\n",
        "    return _resnet('resnext50_32x4d', Bottleneck, [3, 4, 6, 3],\r\n",
        "                   pretrained, progress, **kwargs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dtkaq4PuL3w5"
      },
      "source": [
        "#resnext = torch.hub.load('pytorch/vision:v0.6.0', 'resnext50_32x4d', pretrained=True)\r\n",
        "\r\n",
        "class my_Resnext(nn.Module):\r\n",
        "    def __init__(self):\r\n",
        "        super(my_Resnext, self).__init__()\r\n",
        "        self.resnext = resnext50_32x4d()\r\n",
        "        self.FC = nn.Linear(1000, 26)\r\n",
        "        nn.init.xavier_normal_(self.FC.weight)\r\n",
        "      \r\n",
        "        \r\n",
        "\r\n",
        "    def forward(self, x):\r\n",
        "        x = self.resnext(x)\r\n",
        "        x = torch.sigmoid(self.FC(x))\r\n",
        "        return x\r\n",
        "\r\n",
        "model = my_Resnext()\r\n",
        "model.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QH0QzgwbJPH5",
        "outputId": "8ebed68b-193f-4d57-aeb3-fd0791faa339"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "my_Resnext(\n",
              "  (resnext): ResNet(\n",
              "    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
              "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (relu): ReLU(inplace=True)\n",
              "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "    (layer1): Sequential(\n",
              "      (0): Bottleneck(\n",
              "        (conv1): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
              "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (dropout1): Dropout(p=0.3, inplace=False)\n",
              "        (dropout2): Dropout(p=0.3, inplace=False)\n",
              "        (dropout3): Dropout(p=0.3, inplace=False)\n",
              "        (downsample): Sequential(\n",
              "          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "      )\n",
              "      (1): Bottleneck(\n",
              "        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
              "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (dropout1): Dropout(p=0.3, inplace=False)\n",
              "        (dropout2): Dropout(p=0.3, inplace=False)\n",
              "        (dropout3): Dropout(p=0.3, inplace=False)\n",
              "      )\n",
              "      (2): Bottleneck(\n",
              "        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
              "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (dropout1): Dropout(p=0.3, inplace=False)\n",
              "        (dropout2): Dropout(p=0.3, inplace=False)\n",
              "        (dropout3): Dropout(p=0.3, inplace=False)\n",
              "      )\n",
              "    )\n",
              "    (layer2): Sequential(\n",
              "      (0): Bottleneck(\n",
              "        (conv1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=32, bias=False)\n",
              "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (dropout1): Dropout(p=0.3, inplace=False)\n",
              "        (dropout2): Dropout(p=0.3, inplace=False)\n",
              "        (dropout3): Dropout(p=0.3, inplace=False)\n",
              "        (downsample): Sequential(\n",
              "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "      )\n",
              "      (1): Bottleneck(\n",
              "        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
              "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (dropout1): Dropout(p=0.3, inplace=False)\n",
              "        (dropout2): Dropout(p=0.3, inplace=False)\n",
              "        (dropout3): Dropout(p=0.3, inplace=False)\n",
              "      )\n",
              "      (2): Bottleneck(\n",
              "        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
              "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (dropout1): Dropout(p=0.3, inplace=False)\n",
              "        (dropout2): Dropout(p=0.3, inplace=False)\n",
              "        (dropout3): Dropout(p=0.3, inplace=False)\n",
              "      )\n",
              "      (3): Bottleneck(\n",
              "        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
              "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (dropout1): Dropout(p=0.3, inplace=False)\n",
              "        (dropout2): Dropout(p=0.3, inplace=False)\n",
              "        (dropout3): Dropout(p=0.3, inplace=False)\n",
              "      )\n",
              "    )\n",
              "    (layer3): Sequential(\n",
              "      (0): Bottleneck(\n",
              "        (conv1): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=32, bias=False)\n",
              "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (dropout1): Dropout(p=0.3, inplace=False)\n",
              "        (dropout2): Dropout(p=0.3, inplace=False)\n",
              "        (dropout3): Dropout(p=0.3, inplace=False)\n",
              "        (downsample): Sequential(\n",
              "          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "      )\n",
              "      (1): Bottleneck(\n",
              "        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
              "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (dropout1): Dropout(p=0.3, inplace=False)\n",
              "        (dropout2): Dropout(p=0.3, inplace=False)\n",
              "        (dropout3): Dropout(p=0.3, inplace=False)\n",
              "      )\n",
              "      (2): Bottleneck(\n",
              "        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
              "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (dropout1): Dropout(p=0.3, inplace=False)\n",
              "        (dropout2): Dropout(p=0.3, inplace=False)\n",
              "        (dropout3): Dropout(p=0.3, inplace=False)\n",
              "      )\n",
              "      (3): Bottleneck(\n",
              "        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
              "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (dropout1): Dropout(p=0.3, inplace=False)\n",
              "        (dropout2): Dropout(p=0.3, inplace=False)\n",
              "        (dropout3): Dropout(p=0.3, inplace=False)\n",
              "      )\n",
              "      (4): Bottleneck(\n",
              "        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
              "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (dropout1): Dropout(p=0.3, inplace=False)\n",
              "        (dropout2): Dropout(p=0.3, inplace=False)\n",
              "        (dropout3): Dropout(p=0.3, inplace=False)\n",
              "      )\n",
              "      (5): Bottleneck(\n",
              "        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
              "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (dropout1): Dropout(p=0.3, inplace=False)\n",
              "        (dropout2): Dropout(p=0.3, inplace=False)\n",
              "        (dropout3): Dropout(p=0.3, inplace=False)\n",
              "      )\n",
              "    )\n",
              "    (layer4): Sequential(\n",
              "      (0): Bottleneck(\n",
              "        (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=32, bias=False)\n",
              "        (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (dropout1): Dropout(p=0.3, inplace=False)\n",
              "        (dropout2): Dropout(p=0.3, inplace=False)\n",
              "        (dropout3): Dropout(p=0.3, inplace=False)\n",
              "        (downsample): Sequential(\n",
              "          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "      )\n",
              "      (1): Bottleneck(\n",
              "        (conv1): Conv2d(2048, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
              "        (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (dropout1): Dropout(p=0.3, inplace=False)\n",
              "        (dropout2): Dropout(p=0.3, inplace=False)\n",
              "        (dropout3): Dropout(p=0.3, inplace=False)\n",
              "      )\n",
              "      (2): Bottleneck(\n",
              "        (conv1): Conv2d(2048, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
              "        (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (dropout1): Dropout(p=0.3, inplace=False)\n",
              "        (dropout2): Dropout(p=0.3, inplace=False)\n",
              "        (dropout3): Dropout(p=0.3, inplace=False)\n",
              "      )\n",
              "    )\n",
              "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
              "    (fc): Linear(in_features=2048, out_features=1000, bias=True)\n",
              "  )\n",
              "  (FC): Linear(in_features=1000, out_features=26, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lOk8WcSRL5Aw",
        "outputId": "2e33e4bc-9841-4c04-a6b9-539d2927cb39"
      },
      "source": [
        "summary(model, input_size=(3, 28, 28))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 64, 14, 14]           9,408\n",
            "       BatchNorm2d-2           [-1, 64, 14, 14]             128\n",
            "              ReLU-3           [-1, 64, 14, 14]               0\n",
            "         MaxPool2d-4             [-1, 64, 7, 7]               0\n",
            "            Conv2d-5            [-1, 128, 7, 7]           8,192\n",
            "       BatchNorm2d-6            [-1, 128, 7, 7]             256\n",
            "              ReLU-7            [-1, 128, 7, 7]               0\n",
            "           Dropout-8            [-1, 128, 7, 7]               0\n",
            "            Conv2d-9            [-1, 128, 7, 7]           4,608\n",
            "      BatchNorm2d-10            [-1, 128, 7, 7]             256\n",
            "             ReLU-11            [-1, 128, 7, 7]               0\n",
            "          Dropout-12            [-1, 128, 7, 7]               0\n",
            "           Conv2d-13            [-1, 256, 7, 7]          32,768\n",
            "      BatchNorm2d-14            [-1, 256, 7, 7]             512\n",
            "           Conv2d-15            [-1, 256, 7, 7]          16,384\n",
            "      BatchNorm2d-16            [-1, 256, 7, 7]             512\n",
            "             ReLU-17            [-1, 256, 7, 7]               0\n",
            "          Dropout-18            [-1, 256, 7, 7]               0\n",
            "       Bottleneck-19            [-1, 256, 7, 7]               0\n",
            "           Conv2d-20            [-1, 128, 7, 7]          32,768\n",
            "      BatchNorm2d-21            [-1, 128, 7, 7]             256\n",
            "             ReLU-22            [-1, 128, 7, 7]               0\n",
            "          Dropout-23            [-1, 128, 7, 7]               0\n",
            "           Conv2d-24            [-1, 128, 7, 7]           4,608\n",
            "      BatchNorm2d-25            [-1, 128, 7, 7]             256\n",
            "             ReLU-26            [-1, 128, 7, 7]               0\n",
            "          Dropout-27            [-1, 128, 7, 7]               0\n",
            "           Conv2d-28            [-1, 256, 7, 7]          32,768\n",
            "      BatchNorm2d-29            [-1, 256, 7, 7]             512\n",
            "             ReLU-30            [-1, 256, 7, 7]               0\n",
            "          Dropout-31            [-1, 256, 7, 7]               0\n",
            "       Bottleneck-32            [-1, 256, 7, 7]               0\n",
            "           Conv2d-33            [-1, 128, 7, 7]          32,768\n",
            "      BatchNorm2d-34            [-1, 128, 7, 7]             256\n",
            "             ReLU-35            [-1, 128, 7, 7]               0\n",
            "          Dropout-36            [-1, 128, 7, 7]               0\n",
            "           Conv2d-37            [-1, 128, 7, 7]           4,608\n",
            "      BatchNorm2d-38            [-1, 128, 7, 7]             256\n",
            "             ReLU-39            [-1, 128, 7, 7]               0\n",
            "          Dropout-40            [-1, 128, 7, 7]               0\n",
            "           Conv2d-41            [-1, 256, 7, 7]          32,768\n",
            "      BatchNorm2d-42            [-1, 256, 7, 7]             512\n",
            "             ReLU-43            [-1, 256, 7, 7]               0\n",
            "          Dropout-44            [-1, 256, 7, 7]               0\n",
            "       Bottleneck-45            [-1, 256, 7, 7]               0\n",
            "           Conv2d-46            [-1, 256, 7, 7]          65,536\n",
            "      BatchNorm2d-47            [-1, 256, 7, 7]             512\n",
            "             ReLU-48            [-1, 256, 7, 7]               0\n",
            "          Dropout-49            [-1, 256, 7, 7]               0\n",
            "           Conv2d-50            [-1, 256, 4, 4]          18,432\n",
            "      BatchNorm2d-51            [-1, 256, 4, 4]             512\n",
            "             ReLU-52            [-1, 256, 4, 4]               0\n",
            "          Dropout-53            [-1, 256, 4, 4]               0\n",
            "           Conv2d-54            [-1, 512, 4, 4]         131,072\n",
            "      BatchNorm2d-55            [-1, 512, 4, 4]           1,024\n",
            "           Conv2d-56            [-1, 512, 4, 4]         131,072\n",
            "      BatchNorm2d-57            [-1, 512, 4, 4]           1,024\n",
            "             ReLU-58            [-1, 512, 4, 4]               0\n",
            "          Dropout-59            [-1, 512, 4, 4]               0\n",
            "       Bottleneck-60            [-1, 512, 4, 4]               0\n",
            "           Conv2d-61            [-1, 256, 4, 4]         131,072\n",
            "      BatchNorm2d-62            [-1, 256, 4, 4]             512\n",
            "             ReLU-63            [-1, 256, 4, 4]               0\n",
            "          Dropout-64            [-1, 256, 4, 4]               0\n",
            "           Conv2d-65            [-1, 256, 4, 4]          18,432\n",
            "      BatchNorm2d-66            [-1, 256, 4, 4]             512\n",
            "             ReLU-67            [-1, 256, 4, 4]               0\n",
            "          Dropout-68            [-1, 256, 4, 4]               0\n",
            "           Conv2d-69            [-1, 512, 4, 4]         131,072\n",
            "      BatchNorm2d-70            [-1, 512, 4, 4]           1,024\n",
            "             ReLU-71            [-1, 512, 4, 4]               0\n",
            "          Dropout-72            [-1, 512, 4, 4]               0\n",
            "       Bottleneck-73            [-1, 512, 4, 4]               0\n",
            "           Conv2d-74            [-1, 256, 4, 4]         131,072\n",
            "      BatchNorm2d-75            [-1, 256, 4, 4]             512\n",
            "             ReLU-76            [-1, 256, 4, 4]               0\n",
            "          Dropout-77            [-1, 256, 4, 4]               0\n",
            "           Conv2d-78            [-1, 256, 4, 4]          18,432\n",
            "      BatchNorm2d-79            [-1, 256, 4, 4]             512\n",
            "             ReLU-80            [-1, 256, 4, 4]               0\n",
            "          Dropout-81            [-1, 256, 4, 4]               0\n",
            "           Conv2d-82            [-1, 512, 4, 4]         131,072\n",
            "      BatchNorm2d-83            [-1, 512, 4, 4]           1,024\n",
            "             ReLU-84            [-1, 512, 4, 4]               0\n",
            "          Dropout-85            [-1, 512, 4, 4]               0\n",
            "       Bottleneck-86            [-1, 512, 4, 4]               0\n",
            "           Conv2d-87            [-1, 256, 4, 4]         131,072\n",
            "      BatchNorm2d-88            [-1, 256, 4, 4]             512\n",
            "             ReLU-89            [-1, 256, 4, 4]               0\n",
            "          Dropout-90            [-1, 256, 4, 4]               0\n",
            "           Conv2d-91            [-1, 256, 4, 4]          18,432\n",
            "      BatchNorm2d-92            [-1, 256, 4, 4]             512\n",
            "             ReLU-93            [-1, 256, 4, 4]               0\n",
            "          Dropout-94            [-1, 256, 4, 4]               0\n",
            "           Conv2d-95            [-1, 512, 4, 4]         131,072\n",
            "      BatchNorm2d-96            [-1, 512, 4, 4]           1,024\n",
            "             ReLU-97            [-1, 512, 4, 4]               0\n",
            "          Dropout-98            [-1, 512, 4, 4]               0\n",
            "       Bottleneck-99            [-1, 512, 4, 4]               0\n",
            "          Conv2d-100            [-1, 512, 4, 4]         262,144\n",
            "     BatchNorm2d-101            [-1, 512, 4, 4]           1,024\n",
            "            ReLU-102            [-1, 512, 4, 4]               0\n",
            "         Dropout-103            [-1, 512, 4, 4]               0\n",
            "          Conv2d-104            [-1, 512, 2, 2]          73,728\n",
            "     BatchNorm2d-105            [-1, 512, 2, 2]           1,024\n",
            "            ReLU-106            [-1, 512, 2, 2]               0\n",
            "         Dropout-107            [-1, 512, 2, 2]               0\n",
            "          Conv2d-108           [-1, 1024, 2, 2]         524,288\n",
            "     BatchNorm2d-109           [-1, 1024, 2, 2]           2,048\n",
            "          Conv2d-110           [-1, 1024, 2, 2]         524,288\n",
            "     BatchNorm2d-111           [-1, 1024, 2, 2]           2,048\n",
            "            ReLU-112           [-1, 1024, 2, 2]               0\n",
            "         Dropout-113           [-1, 1024, 2, 2]               0\n",
            "      Bottleneck-114           [-1, 1024, 2, 2]               0\n",
            "          Conv2d-115            [-1, 512, 2, 2]         524,288\n",
            "     BatchNorm2d-116            [-1, 512, 2, 2]           1,024\n",
            "            ReLU-117            [-1, 512, 2, 2]               0\n",
            "         Dropout-118            [-1, 512, 2, 2]               0\n",
            "          Conv2d-119            [-1, 512, 2, 2]          73,728\n",
            "     BatchNorm2d-120            [-1, 512, 2, 2]           1,024\n",
            "            ReLU-121            [-1, 512, 2, 2]               0\n",
            "         Dropout-122            [-1, 512, 2, 2]               0\n",
            "          Conv2d-123           [-1, 1024, 2, 2]         524,288\n",
            "     BatchNorm2d-124           [-1, 1024, 2, 2]           2,048\n",
            "            ReLU-125           [-1, 1024, 2, 2]               0\n",
            "         Dropout-126           [-1, 1024, 2, 2]               0\n",
            "      Bottleneck-127           [-1, 1024, 2, 2]               0\n",
            "          Conv2d-128            [-1, 512, 2, 2]         524,288\n",
            "     BatchNorm2d-129            [-1, 512, 2, 2]           1,024\n",
            "            ReLU-130            [-1, 512, 2, 2]               0\n",
            "         Dropout-131            [-1, 512, 2, 2]               0\n",
            "          Conv2d-132            [-1, 512, 2, 2]          73,728\n",
            "     BatchNorm2d-133            [-1, 512, 2, 2]           1,024\n",
            "            ReLU-134            [-1, 512, 2, 2]               0\n",
            "         Dropout-135            [-1, 512, 2, 2]               0\n",
            "          Conv2d-136           [-1, 1024, 2, 2]         524,288\n",
            "     BatchNorm2d-137           [-1, 1024, 2, 2]           2,048\n",
            "            ReLU-138           [-1, 1024, 2, 2]               0\n",
            "         Dropout-139           [-1, 1024, 2, 2]               0\n",
            "      Bottleneck-140           [-1, 1024, 2, 2]               0\n",
            "          Conv2d-141            [-1, 512, 2, 2]         524,288\n",
            "     BatchNorm2d-142            [-1, 512, 2, 2]           1,024\n",
            "            ReLU-143            [-1, 512, 2, 2]               0\n",
            "         Dropout-144            [-1, 512, 2, 2]               0\n",
            "          Conv2d-145            [-1, 512, 2, 2]          73,728\n",
            "     BatchNorm2d-146            [-1, 512, 2, 2]           1,024\n",
            "            ReLU-147            [-1, 512, 2, 2]               0\n",
            "         Dropout-148            [-1, 512, 2, 2]               0\n",
            "          Conv2d-149           [-1, 1024, 2, 2]         524,288\n",
            "     BatchNorm2d-150           [-1, 1024, 2, 2]           2,048\n",
            "            ReLU-151           [-1, 1024, 2, 2]               0\n",
            "         Dropout-152           [-1, 1024, 2, 2]               0\n",
            "      Bottleneck-153           [-1, 1024, 2, 2]               0\n",
            "          Conv2d-154            [-1, 512, 2, 2]         524,288\n",
            "     BatchNorm2d-155            [-1, 512, 2, 2]           1,024\n",
            "            ReLU-156            [-1, 512, 2, 2]               0\n",
            "         Dropout-157            [-1, 512, 2, 2]               0\n",
            "          Conv2d-158            [-1, 512, 2, 2]          73,728\n",
            "     BatchNorm2d-159            [-1, 512, 2, 2]           1,024\n",
            "            ReLU-160            [-1, 512, 2, 2]               0\n",
            "         Dropout-161            [-1, 512, 2, 2]               0\n",
            "          Conv2d-162           [-1, 1024, 2, 2]         524,288\n",
            "     BatchNorm2d-163           [-1, 1024, 2, 2]           2,048\n",
            "            ReLU-164           [-1, 1024, 2, 2]               0\n",
            "         Dropout-165           [-1, 1024, 2, 2]               0\n",
            "      Bottleneck-166           [-1, 1024, 2, 2]               0\n",
            "          Conv2d-167            [-1, 512, 2, 2]         524,288\n",
            "     BatchNorm2d-168            [-1, 512, 2, 2]           1,024\n",
            "            ReLU-169            [-1, 512, 2, 2]               0\n",
            "         Dropout-170            [-1, 512, 2, 2]               0\n",
            "          Conv2d-171            [-1, 512, 2, 2]          73,728\n",
            "     BatchNorm2d-172            [-1, 512, 2, 2]           1,024\n",
            "            ReLU-173            [-1, 512, 2, 2]               0\n",
            "         Dropout-174            [-1, 512, 2, 2]               0\n",
            "          Conv2d-175           [-1, 1024, 2, 2]         524,288\n",
            "     BatchNorm2d-176           [-1, 1024, 2, 2]           2,048\n",
            "            ReLU-177           [-1, 1024, 2, 2]               0\n",
            "         Dropout-178           [-1, 1024, 2, 2]               0\n",
            "      Bottleneck-179           [-1, 1024, 2, 2]               0\n",
            "          Conv2d-180           [-1, 1024, 2, 2]       1,048,576\n",
            "     BatchNorm2d-181           [-1, 1024, 2, 2]           2,048\n",
            "            ReLU-182           [-1, 1024, 2, 2]               0\n",
            "         Dropout-183           [-1, 1024, 2, 2]               0\n",
            "          Conv2d-184           [-1, 1024, 1, 1]         294,912\n",
            "     BatchNorm2d-185           [-1, 1024, 1, 1]           2,048\n",
            "            ReLU-186           [-1, 1024, 1, 1]               0\n",
            "         Dropout-187           [-1, 1024, 1, 1]               0\n",
            "          Conv2d-188           [-1, 2048, 1, 1]       2,097,152\n",
            "     BatchNorm2d-189           [-1, 2048, 1, 1]           4,096\n",
            "          Conv2d-190           [-1, 2048, 1, 1]       2,097,152\n",
            "     BatchNorm2d-191           [-1, 2048, 1, 1]           4,096\n",
            "            ReLU-192           [-1, 2048, 1, 1]               0\n",
            "         Dropout-193           [-1, 2048, 1, 1]               0\n",
            "      Bottleneck-194           [-1, 2048, 1, 1]               0\n",
            "          Conv2d-195           [-1, 1024, 1, 1]       2,097,152\n",
            "     BatchNorm2d-196           [-1, 1024, 1, 1]           2,048\n",
            "            ReLU-197           [-1, 1024, 1, 1]               0\n",
            "         Dropout-198           [-1, 1024, 1, 1]               0\n",
            "          Conv2d-199           [-1, 1024, 1, 1]         294,912\n",
            "     BatchNorm2d-200           [-1, 1024, 1, 1]           2,048\n",
            "            ReLU-201           [-1, 1024, 1, 1]               0\n",
            "         Dropout-202           [-1, 1024, 1, 1]               0\n",
            "          Conv2d-203           [-1, 2048, 1, 1]       2,097,152\n",
            "     BatchNorm2d-204           [-1, 2048, 1, 1]           4,096\n",
            "            ReLU-205           [-1, 2048, 1, 1]               0\n",
            "         Dropout-206           [-1, 2048, 1, 1]               0\n",
            "      Bottleneck-207           [-1, 2048, 1, 1]               0\n",
            "          Conv2d-208           [-1, 1024, 1, 1]       2,097,152\n",
            "     BatchNorm2d-209           [-1, 1024, 1, 1]           2,048\n",
            "            ReLU-210           [-1, 1024, 1, 1]               0\n",
            "         Dropout-211           [-1, 1024, 1, 1]               0\n",
            "          Conv2d-212           [-1, 1024, 1, 1]         294,912\n",
            "     BatchNorm2d-213           [-1, 1024, 1, 1]           2,048\n",
            "            ReLU-214           [-1, 1024, 1, 1]               0\n",
            "         Dropout-215           [-1, 1024, 1, 1]               0\n",
            "          Conv2d-216           [-1, 2048, 1, 1]       2,097,152\n",
            "     BatchNorm2d-217           [-1, 2048, 1, 1]           4,096\n",
            "            ReLU-218           [-1, 2048, 1, 1]               0\n",
            "         Dropout-219           [-1, 2048, 1, 1]               0\n",
            "      Bottleneck-220           [-1, 2048, 1, 1]               0\n",
            "AdaptiveAvgPool2d-221           [-1, 2048, 1, 1]               0\n",
            "          Linear-222                 [-1, 1000]       2,049,000\n",
            "          ResNet-223                 [-1, 1000]               0\n",
            "          Linear-224                   [-1, 26]          26,026\n",
            "================================================================\n",
            "Total params: 25,054,930\n",
            "Trainable params: 25,054,930\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.01\n",
            "Forward/backward pass size (MB): 8.23\n",
            "Params size (MB): 95.58\n",
            "Estimated Total Size (MB): 103.82\n",
            "----------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LXgjheSMLQNF"
      },
      "source": [
        "labels_df = pd.read_csv('/content/dirty_mnist_2nd_answer.csv')[:]\r\n",
        "imgs_dir = np.array(sorted(glob.glob('/content/dirty_mnist/*')))[:]\r\n",
        "labels = np.array(labels_df.values[:,1:])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 402
        },
        "id": "51c_sOGzLdYu",
        "outputId": "f59356d3-d998-4469-9c30-38a533a26d5b"
      },
      "source": [
        "labels_df"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>index</th>\n",
              "      <th>a</th>\n",
              "      <th>b</th>\n",
              "      <th>c</th>\n",
              "      <th>d</th>\n",
              "      <th>e</th>\n",
              "      <th>f</th>\n",
              "      <th>g</th>\n",
              "      <th>h</th>\n",
              "      <th>i</th>\n",
              "      <th>j</th>\n",
              "      <th>k</th>\n",
              "      <th>l</th>\n",
              "      <th>m</th>\n",
              "      <th>n</th>\n",
              "      <th>o</th>\n",
              "      <th>p</th>\n",
              "      <th>q</th>\n",
              "      <th>r</th>\n",
              "      <th>s</th>\n",
              "      <th>t</th>\n",
              "      <th>u</th>\n",
              "      <th>v</th>\n",
              "      <th>w</th>\n",
              "      <th>x</th>\n",
              "      <th>y</th>\n",
              "      <th>z</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49995</th>\n",
              "      <td>49995</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49996</th>\n",
              "      <td>49996</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49997</th>\n",
              "      <td>49997</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49998</th>\n",
              "      <td>49998</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49999</th>\n",
              "      <td>49999</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>50000 rows × 27 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       index  a  b  c  d  e  f  g  h  i  j  ...  p  q  r  s  t  u  v  w  x  y  z\n",
              "0          0  1  1  0  1  0  1  0  0  0  0  ...  1  0  1  1  0  1  0  0  1  1  1\n",
              "1          1  1  0  0  1  0  1  0  1  0  1  ...  1  0  1  0  1  0  0  0  0  1  1\n",
              "2          2  0  0  0  0  0  0  0  0  1  1  ...  1  0  0  1  1  1  0  1  1  1  0\n",
              "3          3  0  0  1  0  0  0  1  1  0  0  ...  1  1  0  1  1  0  1  1  0  1  0\n",
              "4          4  0  1  0  1  0  1  0  1  1  0  ...  1  0  1  0  0  0  1  0  1  0  0\n",
              "...      ... .. .. .. .. .. .. .. .. .. ..  ... .. .. .. .. .. .. .. .. .. .. ..\n",
              "49995  49995  0  1  1  0  0  0  0  1  0  0  ...  0  0  0  0  1  0  0  0  1  1  0\n",
              "49996  49996  0  1  0  1  0  1  1  1  0  1  ...  0  0  1  1  1  0  1  0  0  0  1\n",
              "49997  49997  0  1  0  0  1  1  1  1  0  0  ...  0  1  0  0  0  0  1  1  1  0  0\n",
              "49998  49998  0  1  1  1  0  0  1  1  0  1  ...  0  0  1  1  1  0  0  0  1  0  0\n",
              "49999  49999  1  0  1  0  0  0  0  0  0  1  ...  1  0  1  1  0  1  1  0  1  0  0\n",
              "\n",
              "[50000 rows x 27 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8kEMKf5WMyil"
      },
      "source": [
        "kf = KFold(n_splits=5, shuffle=True)\r\n",
        "folds=[]\r\n",
        "for train_idx, valid_idx in kf.split(labels_df):\r\n",
        "    folds.append((train_idx, valid_idx))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4lcdbFcTLOAk"
      },
      "source": [
        "def seed_everything(seed: int = 42):\r\n",
        "    random.seed(seed)\r\n",
        "    np.random.seed(seed)\r\n",
        "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\r\n",
        "    torch.manual_seed(seed)\r\n",
        "    torch.cuda.manual_seed(seed)  # type: ignore\r\n",
        "    torch.backends.cudnn.deterministic = True  # type: ignore\r\n",
        "    torch.backends.cudnn.benchmark = True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UUPKxUF-PNMM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fabd61f5-f3d5-4fb6-d600-d63fed372765"
      },
      "source": [
        "#seed_everything(42)\r\n",
        "for fold in range(1):\r\n",
        "#     model = nn.DataParallel(model)\r\n",
        "    train_idx = folds[fold][0]\r\n",
        "    valid_idx = folds[fold][1]\r\n",
        "\r\n",
        "    train_transform = T.Compose([                                \r\n",
        "        T.ToTensor(),\r\n",
        "        T.Normalize(\r\n",
        "        [0.485, 0.456, 0.406],\r\n",
        "        [0.229, 0.224, 0.225]\r\n",
        "    )\r\n",
        "        ])\r\n",
        "    valid_transform = T.Compose([                                 \r\n",
        "        T.ToTensor(),\r\n",
        "        T.Normalize(\r\n",
        "        [0.485, 0.456, 0.406],\r\n",
        "        [0.229, 0.224, 0.225]\r\n",
        "    )\r\n",
        "        ])\r\n",
        "\r\n",
        "\r\n",
        "    epochs=20\r\n",
        "    batch_size=32   # 자신의 VRAM에 맞게 조절해야 OOM을 피할 수 있습니다.\r\n",
        "    \r\n",
        "    \r\n",
        "    # Data Loader\r\n",
        "    train_dataset = MnistDataset_v1(imgs_dir=imgs_dir[train_idx], labels=labels[train_idx], transform=train_transform)\r\n",
        "    train_loader = DataLoader(dataset=train_dataset, batch_size=64, shuffle=True)\r\n",
        "\r\n",
        "    valid_dataset = MnistDataset_v1(imgs_dir=imgs_dir[valid_idx], labels=labels[valid_idx], transform=valid_transform)\r\n",
        "    valid_loader = DataLoader(dataset=valid_dataset, batch_size=32, shuffle=False)  \r\n",
        "    \r\n",
        "    \r\n",
        "    # optimizer\r\n",
        "    # polynomial optimizer를 사용합니다.\r\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr = 0.001)\r\n",
        "    scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones= [23,29], gamma=0.1)\r\n",
        "\r\n",
        "    criterion = torch.nn.BCELoss()\r\n",
        "    \r\n",
        "    \r\n",
        "    epoch_accuracy = []\r\n",
        "    valid_accuracy = []\r\n",
        "    valid_losses=[]\r\n",
        "    valid_best_accuracy=0\r\n",
        "\r\n",
        "    best_models=[]\r\n",
        "\r\n",
        "    for epoch in range(epochs):\r\n",
        "      with tqdm(train_loader,total=train_loader.__len__(),unit='batch') as train_bar:\r\n",
        "        model.train()\r\n",
        "        batch_accuracy_list = []\r\n",
        "        batch_loss_list = []\r\n",
        "        start=time.time()\r\n",
        "        for n, (X, y) in enumerate((train_bar)):\r\n",
        "            train_bar.set_description(f\"Train Epoch {epoch}\")\r\n",
        "            X = torch.tensor(X, device=device, dtype=torch.float32)\r\n",
        "            y = torch.tensor(y, device=device, dtype=torch.float32)\r\n",
        "            y_hat = model(X)\r\n",
        "            \r\n",
        "            \r\n",
        "            optimizer.zero_grad()\r\n",
        "            loss = criterion(y_hat, y)\r\n",
        "            loss.backward()\r\n",
        "            optimizer.step()\r\n",
        "\r\n",
        "\r\n",
        "            \r\n",
        "            y_hat  = y_hat.cpu().detach().numpy()\r\n",
        "            y_hat = y_hat>0.5\r\n",
        "            y = y.cpu().detach().numpy()\r\n",
        "\r\n",
        "            batch_accuracy = (y_hat == y).mean()\r\n",
        "            batch_accuracy_list.append(batch_accuracy)\r\n",
        "            batch_loss_list.append(loss.item())\r\n",
        "            train_acc = np.mean(batch_accuracy_list)\r\n",
        "            \r\n",
        "            train_bar.set_postfix(train_loss= loss.item(),train_acc = train_acc)\r\n",
        "\r\n",
        "        model.eval()\r\n",
        "        valid_batch_accuracy=[]\r\n",
        "        valid_batch_loss = []\r\n",
        "\r\n",
        "        with torch.no_grad():\r\n",
        "          with tqdm(valid_loader,total=valid_loader.__len__(),unit=\"batch\") as valid_bar:\r\n",
        "            for n_valid, (X_valid, y_valid) in enumerate((valid_bar)):\r\n",
        "                valid_bar.set_description(f\"Valid Epoch {epoch}\")\r\n",
        "                X_valid = torch.tensor(X_valid, device=device)#, dtype=torch.float32)\r\n",
        "                y_valid = torch.tensor(y_valid, device=device, dtype=torch.float32)\r\n",
        "                y_valid_hat = model(X_valid)\r\n",
        "                \r\n",
        "                valid_loss = criterion(y_valid_hat, y_valid).item()\r\n",
        "                \r\n",
        "                y_valid_hat = y_valid_hat.cpu().detach().numpy()>0.5\r\n",
        "                \r\n",
        "                \r\n",
        "                valid_batch_loss.append(valid_loss)\r\n",
        "                valid_batch_accuracy.append((y_valid_hat == y_valid.cpu().detach().numpy()).mean())\r\n",
        "                val_acc=np.mean(valid_batch_accuracy)\r\n",
        "                valid_bar.set_postfix(valid_loss = valid_loss,valid_acc = val_acc)\r\n",
        "                \r\n",
        "            valid_losses.append(np.mean(valid_batch_loss))\r\n",
        "            valid_accuracy.append(np.mean(valid_batch_accuracy))\r\n",
        "\r\n",
        "        scheduler.step()\r\n",
        "\r\n",
        "        if np.mean(valid_batch_accuracy) > 0.6:\r\n",
        "            path = \"/content/drive/MyDrive/Colab Notebooks/dacon/models4/\"\r\n",
        "            MODEL = \"RESNEXT\"\r\n",
        "            torch.save(model, f'{path}_{MODEL}_{valid_loss:2.4f}_epoch_{epoch}.pth')\r\n",
        "\r\n",
        "        if np.mean(valid_batch_accuracy)>valid_best_accuracy:\r\n",
        "            best_model=model\r\n",
        "            valid_best_accuracy = np.mean(valid_batch_accuracy)\r\n",
        "            path = \"/content/drive/MyDrive/Colab Notebooks/dacon/models4/\"\r\n",
        "            MODEL = \"RESNEXTT\"\r\n",
        "            torch.save(model, f'{path}_{MODEL}_{valid_loss:2.4f}_epoch_{epoch}.pth')\r\n",
        "\r\n",
        "\r\n",
        "    best_models.append(best_model)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train Epoch 0:   0%|          | 0/625 [00:00<?, ?batch/s]/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:58: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:59: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gtV7IZSlQp8g"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}