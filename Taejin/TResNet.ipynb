{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "train.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "954WxGTSkddB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6b7db8cd-6e85-443d-8cdc-71acb4be98f4"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q_lA5C3cXTq4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7978d060-e3bf-4d85-c6b3-86f2a0a91828"
      },
      "source": [
        "%cd /content/drive/MyDrive/dacon_cv/\n",
        "!ls"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/dacon_cv\n",
            "'제_2회 컴퓨터 비전 학습 경진대회 베이스라인 코드.ipynb'\n",
            " ASL\n",
            " converT2coco.ipynb\n",
            " dirty_mnist\n",
            " dirty_mnist_2nd_answer.csv\n",
            " dirty_mnist_2nd.zip\n",
            " dirty_mnist_answer.csv\n",
            " dirty_mnist.zip\n",
            " mnist_data\n",
            " sample_submission.csv\n",
            " test_dirty_mnist\n",
            " test_dirty_mnist_2nd.zip\n",
            " test.json\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eh6s6z70awqp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b7bfad4f-2dcd-43a5-f6b3-d5887d523b76"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fri Feb 19 18:04:35 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.39       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   42C    P8     9W /  70W |      0MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NzrC9J3OGrK2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "de3aa6a5-fb5d-4839-a6e8-34cf4a7c98a8"
      },
      "source": [
        "!rm -rf dirty_mnist/ \n",
        "!mkdir dirty_mnist/\n",
        "!ls"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "'제_2회 컴퓨터 비전 학습 경진대회 베이스라인 코드.ipynb'\n",
            " ASL\n",
            " converT2coco.ipynb\n",
            " dirty_mnist\n",
            " dirty_mnist_2nd_answer.csv\n",
            " dirty_mnist_2nd.zip\n",
            " dirty_mnist_answer.csv\n",
            " dirty_mnist.zip\n",
            " mnist_data\n",
            " sample_submission.csv\n",
            " test_dirty_mnist\n",
            " test_dirty_mnist_2nd.zip\n",
            " test.json\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IaMcfTqyL6M6",
        "outputId": "548a23f4-6fcc-4a60-e8fc-1ffd9dcd72ed"
      },
      "source": [
        "!unzip -q dirty_mnist_2nd.zip -d dirty_mnist/\n",
        "!ls"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "'제_2회 컴퓨터 비전 학습 경진대회 베이스라인 코드.ipynb'\n",
            " ASL\n",
            " converT2coco.ipynb\n",
            " dirty_mnist\n",
            " dirty_mnist_2nd_answer.csv\n",
            " dirty_mnist_2nd.zip\n",
            " dirty_mnist_answer.csv\n",
            " dirty_mnist.zip\n",
            " mnist_data\n",
            " sample_submission.csv\n",
            " test_dirty_mnist\n",
            " test_dirty_mnist_2nd.zip\n",
            " test.json\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6qddEGp0bgHs"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "import shutil\n",
        "import os\n",
        "import json"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N0mr5L08blTk"
      },
      "source": [
        "namelist = os.listdir('./dirty_mnist/')\n",
        "answer = pd.read_csv('dirty_mnist_answer.csv')\n",
        "train, val = train_test_split(namelist, test_size=0.2, random_state=23)\n",
        "train_num = [int(t[:5]) for t in train]\n",
        "val_num = [int(v[:5]) for v in val]\n",
        "train_ohe = [True if i in train_num else False for i in range(50000)]\n",
        "val_ohe = [True if i in val_num else False for i in range(50000)]\n",
        "train_ans = answer[train_ohe]\n",
        "val_ans = answer[val_ohe]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_PJ30bSobxn6",
        "outputId": "557351dc-e3f1-4643-d716-608f53a02126"
      },
      "source": [
        "%cd ./dirty_mnist/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/dacon_cv/dirty_mnist\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "afbyD31jb0Fc"
      },
      "source": [
        "os.makedirs('./dirty_mnist_train', exist_ok=True)\n",
        "os.makedirs('./dirty_mnist_val', exist_ok=True)\n",
        "for t in train:\n",
        "    shutil.move(t, './dirty_mnist_train')\n",
        "for v in val:\n",
        "    shutil.move(v, './dirty_mnist_val')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HOtC7LJLb01c"
      },
      "source": [
        "def csv_to_coco(df, json_name):\n",
        "    dic = {\n",
        "        \"info\": {\n",
        "            \"description\": \"DACON DIRTY MNIST DATASET\",\n",
        "            \"url\": \"https://oranz.tistory.com/\",\n",
        "            \"version\": \"1.0\",\n",
        "            \"year\": 2021,\n",
        "            \"contributor\": \"DACON\",\n",
        "            \"date_created\": \"2021/02/18\"\n",
        "        },\n",
        "        \"licenses\": [{\n",
        "            \"url\": \"http://creativecommons.org/licenses/by-nc-sa/2.0/\",\n",
        "            \"id\": 1,\n",
        "            \"name\": \"Attribution-NonCommercial-ShareAlike License\"\n",
        "        }],\n",
        "        \"images\": [],\n",
        "        \"annotations\": [],\n",
        "        \"categories\": []\n",
        "    }\n",
        "    \n",
        "    for i in range(1, 27):\n",
        "        dic[\"categories\"].append({\"supercategory\": \"alphabet\", \"id\": i, \"name\": chr(i + 96)})\n",
        "        \n",
        "    anno_id = 1\n",
        "    for r in range(len(df)):\n",
        "        image_id = int(df.iloc[r, 0]) # cast to int for JSON serialization\n",
        "        name = str(image_id)\n",
        "        name = '0' * (5 - len(name)) + name + '.png'\n",
        "        dic[\"images\"].append({\n",
        "            \"license\": 1,\n",
        "            \"file_name\": name,\n",
        "            \"coco_url\": \"\",\n",
        "            \"height\": 256,\n",
        "            \"width\": 256,\n",
        "            \"date_captured\": \"2020-05-19 23:03:57\",\n",
        "            \"flickr_url\": \"\",\n",
        "            \"id\": image_id\n",
        "        })\n",
        "        \n",
        "        for c in range(1, 27):\n",
        "            if df.iloc[r, c] == 1:\n",
        "                dic[\"annotations\"].append({\n",
        "                    \"segmentation\": [],\n",
        "                    \"area\": 232,\n",
        "                    \"iscrowd\": 0,\n",
        "                    \"image_id\": image_id,\n",
        "                    \"bbox\": [1, 5, 4, 9],\n",
        "                    \"category_id\": c,\n",
        "                    \"id\": anno_id\n",
        "                })\n",
        "                anno_id += 1\n",
        "    with open(json_name, \"w\") as json_file:\n",
        "        json.dump(dic, json_file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z8HXplx1b5me"
      },
      "source": [
        "csv_to_coco(val_ans, \"val.json\")\n",
        "csv_to_coco(train_ans, \"train.json\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5tODw8jcRq6G",
        "outputId": "76c9820d-54b2-44d6-d807-47cae924f339"
      },
      "source": [
        "!pip install inplace-abn\n",
        "!pip install randaugment"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting inplace-abn\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a4/27/c5791febcdd9af346b66dff19759898476f148177c02b02a72e07ca8aba0/inplace-abn-1.1.0.tar.gz (137kB)\n",
            "\r\u001b[K     |██▍                             | 10kB 5.8MB/s eta 0:00:01\r\u001b[K     |████▊                           | 20kB 8.7MB/s eta 0:00:01\r\u001b[K     |███████▏                        | 30kB 11.7MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 40kB 14.1MB/s eta 0:00:01\r\u001b[K     |████████████                    | 51kB 16.5MB/s eta 0:00:01\r\u001b[K     |██████████████▎                 | 61kB 8.0MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 71kB 8.8MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 81kB 9.2MB/s eta 0:00:01\r\u001b[K     |█████████████████████▌          | 92kB 10.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 102kB 10.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▎     | 112kB 10.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 122kB 10.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 133kB 10.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 143kB 10.6MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: inplace-abn\n",
            "  Building wheel for inplace-abn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for inplace-abn: filename=inplace_abn-1.1.0-cp36-cp36m-linux_x86_64.whl size=2752485 sha256=99c13f92e1e89721307f9907d3ffc502eaf2e9b815759ae08a56b20f8c2c1661\n",
            "  Stored in directory: /root/.cache/pip/wheels/90/e6/ce/baadcff0441c600caa5874d4d3322a7909e724fb7abab21a15\n",
            "Successfully built inplace-abn\n",
            "Installing collected packages: inplace-abn\n",
            "Successfully installed inplace-abn-1.1.0\n",
            "Collecting randaugment\n",
            "  Downloading https://files.pythonhosted.org/packages/fb/ea/e24549f459800dc3bed21cd4e9c0d49d5b8deed65214b2444bd3e5a49f30/randaugment-1.0.2-py3-none-any.whl\n",
            "Installing collected packages: randaugment\n",
            "Successfully installed randaugment-1.0.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k8epwFeaNfSX",
        "outputId": "9f725ffe-a7a9-49c5-812b-e7e70c2cd24d"
      },
      "source": [
        "cd ../ASL"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/dacon_cv/ASL\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e81_-1TERvYN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6bd29e28-8122-4fac-b81b-e0e1ff358e4c"
      },
      "source": [
        "import argparse\n",
        "import easydict\n",
        "import torch\n",
        "import torch.nn.parallel\n",
        "import torch.optim\n",
        "import torch.utils.data.distributed\n",
        "import torchvision.transforms as transforms\n",
        "import os\n",
        "from torch.optim import lr_scheduler\n",
        "from src.helper_functions.helper_functions import mAP, CocoDetection, CutoutPIL, ModelEma, add_weight_decay\n",
        "from src.models import create_model\n",
        "from src.loss_functions.losses import AsymmetricLoss\n",
        "from randaugment import RandAugment\n",
        "from torch.cuda.amp import GradScaler, autocast\n",
        "\n",
        "parser = argparse.ArgumentParser(description='PyTorch MS_COCO Training')\n",
        "parser.add_argument('data', metavar='DIR', help='path to dataset', default='../dirty_mnist')\n",
        "parser.add_argument('--lr', default=1e-4, type=float)\n",
        "parser.add_argument('--model-name', default='tresnet_m')\n",
        "parser.add_argument('--model-path', default='./tresnet_m.pth', type=str)\n",
        "parser.add_argument('--num-classes', default=26)\n",
        "parser.add_argument('-j', '--workers', default=4, type=int, metavar='N',\n",
        "                    help='number of data loading workers (default: 16)')\n",
        "parser.add_argument('--image-size', default=224, type=int,\n",
        "                    metavar='N', help='input image size (default: 448)')\n",
        "parser.add_argument('--thre', default=0.8, type=float,\n",
        "                    metavar='N', help='threshold value')\n",
        "parser.add_argument('-b', '--batch-size', default=256, type=int,\n",
        "                    metavar='N', help='mini-batch size (default: 16)')\n",
        "parser.add_argument('--print-freq', '-p', default=64, type=int,\n",
        "                    metavar='N', help='print frequency (default: 64)')\n",
        "\n",
        "\n",
        "def main():\n",
        "    args = easydict.EasyDict({ \"data\": \"../dirty_mnist\", \n",
        "                              \"lr\": 23e-5, \n",
        "                              \"model_name\": \"tresnet_m\", \n",
        "                              \"model_path\": \"./tresnet_m.pth\",\n",
        "                              \"num_classes\": 26, \n",
        "                              \"workers\": 4, \n",
        "                              \"image_size\": 224, \n",
        "                              \"thre\": 0.8,\n",
        "                              \"batch_size\": 256,\n",
        "                              \"print_freq\": 64\n",
        "                              })\n",
        "    args.do_bottleneck_head = False\n",
        "\n",
        "    # Setup model\n",
        "    print('creating model...')\n",
        "    model = create_model(args).cuda()\n",
        "    if args.model_path:  # make sure to load pretrained ImageNet model\n",
        "        state = torch.load(args.model_path, map_location='cpu')\n",
        "        filtered_dict = {k: v for k, v in state['model'].items() if\n",
        "                         (k in model.state_dict() and 'head.fc' not in k)}\n",
        "        model.load_state_dict(filtered_dict, strict=False)\n",
        "    print('done\\n')\n",
        "\n",
        "    # COCO Data loading\n",
        "    instances_path_val = os.path.join(args.data, 'val.json')\n",
        "    instances_path_train = os.path.join(args.data, 'train.json')\n",
        "    # data_path_val = args.data\n",
        "    # data_path_train = args.data\n",
        "    data_path_val   = f'{args.data}/dirty_mnist_val'    # args.data\n",
        "    data_path_train = f'{args.data}/dirty_mnist_train'  # args.data\n",
        "    val_dataset = CocoDetection(data_path_val,\n",
        "                                instances_path_val,\n",
        "                                transforms.Compose([\n",
        "                                    transforms.Resize((args.image_size, args.image_size)),\n",
        "                                    transforms.ToTensor(),\n",
        "                                    # normalize, # no need, toTensor does normalization\n",
        "                                ]))\n",
        "    train_dataset = CocoDetection(data_path_train,\n",
        "                                  instances_path_train,\n",
        "                                  transforms.Compose([\n",
        "                                      transforms.Resize((args.image_size, args.image_size)),\n",
        "                                      CutoutPIL(cutout_factor=0.5),\n",
        "                                      RandAugment(),\n",
        "                                      transforms.ToTensor(),\n",
        "                                      # normalize,\n",
        "                                  ]))\n",
        "    print(\"len(val_dataset)): \", len(val_dataset))\n",
        "    print(\"len(train_dataset)): \", len(train_dataset))\n",
        "\n",
        "    # Pytorch Data loader\n",
        "    train_loader = torch.utils.data.DataLoader(\n",
        "        train_dataset, batch_size=args.batch_size, shuffle=True,\n",
        "        num_workers=args.workers, pin_memory=True)\n",
        "\n",
        "    val_loader = torch.utils.data.DataLoader(\n",
        "        val_dataset, batch_size=args.batch_size, shuffle=False,\n",
        "        num_workers=args.workers, pin_memory=False)\n",
        "\n",
        "    # Actuall Training\n",
        "    train_multi_label_coco(model, train_loader, val_loader, args.lr)\n",
        "\n",
        "\n",
        "def train_multi_label_coco(model, train_loader, val_loader, lr):\n",
        "    ema = ModelEma(model, 0.9997)  # 0.9997^641=0.82\n",
        "\n",
        "    # set optimizer\n",
        "    Epochs = 80\n",
        "    Stop_epoch = 40\n",
        "    weight_decay = 1e-4\n",
        "    criterion = AsymmetricLoss(gamma_neg=2, gamma_pos=1, clip=0)\n",
        "    parameters = add_weight_decay(model, weight_decay)\n",
        "    optimizer = torch.optim.Adam(params=parameters, lr=lr, weight_decay=0)  # true wd, filter_bias_and_bn\n",
        "    steps_per_epoch = len(train_loader)\n",
        "    scheduler = lr_scheduler.OneCycleLR(optimizer, max_lr=lr, steps_per_epoch=steps_per_epoch, epochs=Epochs,\n",
        "                                        pct_start=0.2)\n",
        "\n",
        "    highest_mAP = 0\n",
        "    trainInfoList = []\n",
        "    scaler = GradScaler()\n",
        "    for epoch in range(Epochs):\n",
        "        for i, (inputData, target) in enumerate(train_loader):\n",
        "            inputData = inputData.cuda()\n",
        "            target = target.cuda()  # (batch,3,num_classes)\n",
        "            target = target.max(dim=1)[0]\n",
        "            with autocast():  # mixed precision\n",
        "                output = model(inputData).float()  # sigmoid will be done in loss !\n",
        "            loss = criterion(output, target)\n",
        "            model.zero_grad()\n",
        "\n",
        "            scaler.scale(loss).backward()\n",
        "            # loss.backward()\n",
        "\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "            # optimizer.step()\n",
        "\n",
        "            scheduler.step()\n",
        "\n",
        "            ema.update(model)\n",
        "            # store information\n",
        "            if i % 50 == 0:\n",
        "                trainInfoList.append([epoch, i, loss.item()])\n",
        "                print('Epoch [{}/{}], Step [{}/{}], LR {:.1e}, Loss: {:.1f}'\n",
        "                      .format(epoch, Epochs, str(i).zfill(3), str(steps_per_epoch).zfill(3),\n",
        "                              scheduler.get_last_lr()[0], \\\n",
        "                              loss.item()))\n",
        "\n",
        "        try:\n",
        "            torch.save(model.state_dict(), os.path.join(\n",
        "                'model_asl/', 'model-{}-{}.pth'.format(epoch + 1, i + 1)))\n",
        "        except:\n",
        "            print(\"model per epoch is not being saved.\")\n",
        "            pass\n",
        "\n",
        "        model.eval()\n",
        "        mAP_score = validate_multi(val_loader, model, ema)\n",
        "        model.train()\n",
        "        if mAP_score > highest_mAP:\n",
        "            highest_mAP = mAP_score\n",
        "            try:\n",
        "                torch.save(model.state_dict(), os.path.join(\n",
        "                    'model_asl/', 'model-highest.pth'))\n",
        "            except:\n",
        "                print(\"best model is not being saved.\")\n",
        "                pass\n",
        "        print('current_mAP = {:.2f}, highest_mAP = {:.2f}\\n'.format(mAP_score, highest_mAP))\n",
        "\n",
        "\n",
        "def validate_multi(val_loader, model, ema_model):\n",
        "    print(\"starting validation\")\n",
        "    Sig = torch.nn.Sigmoid()\n",
        "    preds_regular = []\n",
        "    preds_ema = []\n",
        "    targets = []\n",
        "    for i, (input, target) in enumerate(val_loader):\n",
        "        target = target\n",
        "        target = target.max(dim=1)[0]\n",
        "        # compute output\n",
        "        with torch.no_grad():\n",
        "            with autocast():\n",
        "                output_regular = Sig(model(input.cuda())).cpu()\n",
        "                output_ema = Sig(ema_model.module(input.cuda())).cpu()\n",
        "\n",
        "        # for mAP calculation\n",
        "        preds_regular.append(output_regular.cpu().detach())\n",
        "        preds_ema.append(output_ema.cpu().detach())\n",
        "        targets.append(target.cpu().detach())\n",
        "\n",
        "    mAP_score_regular = mAP(torch.cat(targets).numpy(), torch.cat(preds_regular).numpy())\n",
        "    mAP_score_ema = mAP(torch.cat(targets).numpy(), torch.cat(preds_ema).numpy())\n",
        "    print(\"mAP score regular {:.2f}, mAP score EMA {:.2f}\".format(mAP_score_regular, mAP_score_ema))\n",
        "    return max(mAP_score_regular, mAP_score_ema)\n",
        "\n",
        "\n",
        "main()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "creating model...\n",
            "done\n",
            "\n",
            "loading annotations into memory...\n",
            "Done (t=0.53s)\n",
            "creating index...\n",
            "index created!\n",
            "loading annotations into memory...\n",
            "Done (t=2.20s)\n",
            "creating index...\n",
            "index created!\n",
            "len(val_dataset)):  10000\n",
            "len(train_dataset)):  40000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [0/80], Step [000/157], LR 9.2e-06, Loss: 1742.2\n",
            "Epoch [0/80], Step [050/157], LR 9.4e-06, Loss: 1712.3\n",
            "Epoch [0/80], Step [100/157], LR 1.0e-05, Loss: 1692.2\n",
            "Epoch [0/80], Step [150/157], LR 1.1e-05, Loss: 1681.7\n",
            "starting validation\n",
            "mAP score regular 46.71, mAP score EMA 46.27\n",
            "current_mAP = 46.71, highest_mAP = 46.71\n",
            "\n",
            "Epoch [1/80], Step [000/157], LR 1.1e-05, Loss: 1681.0\n",
            "Epoch [1/80], Step [050/157], LR 1.3e-05, Loss: 1678.6\n",
            "Epoch [1/80], Step [100/157], LR 1.5e-05, Loss: 1674.1\n",
            "Epoch [1/80], Step [150/157], LR 1.7e-05, Loss: 1679.1\n",
            "starting validation\n",
            "mAP score regular 47.24, mAP score EMA 46.27\n",
            "current_mAP = 47.24, highest_mAP = 47.24\n",
            "\n",
            "Epoch [2/80], Step [000/157], LR 1.8e-05, Loss: 1674.5\n",
            "Epoch [2/80], Step [050/157], LR 2.1e-05, Loss: 1672.8\n",
            "Epoch [2/80], Step [100/157], LR 2.4e-05, Loss: 1675.7\n",
            "Epoch [2/80], Step [150/157], LR 2.7e-05, Loss: 1674.0\n",
            "starting validation\n",
            "mAP score regular 48.35, mAP score EMA 46.29\n",
            "current_mAP = 48.35, highest_mAP = 48.35\n",
            "\n",
            "Epoch [3/80], Step [000/157], LR 2.8e-05, Loss: 1678.6\n",
            "Epoch [3/80], Step [050/157], LR 3.2e-05, Loss: 1669.7\n",
            "Epoch [3/80], Step [100/157], LR 3.6e-05, Loss: 1667.8\n",
            "Epoch [3/80], Step [150/157], LR 4.1e-05, Loss: 1664.5\n",
            "starting validation\n",
            "mAP score regular 50.32, mAP score EMA 46.36\n",
            "current_mAP = 50.32, highest_mAP = 50.32\n",
            "\n",
            "Epoch [4/80], Step [000/157], LR 4.2e-05, Loss: 1664.2\n",
            "Epoch [4/80], Step [050/157], LR 4.7e-05, Loss: 1660.4\n",
            "Epoch [4/80], Step [100/157], LR 5.2e-05, Loss: 1661.9\n",
            "Epoch [4/80], Step [150/157], LR 5.8e-05, Loss: 1663.4\n",
            "starting validation\n",
            "mAP score regular 53.26, mAP score EMA 46.52\n",
            "current_mAP = 53.26, highest_mAP = 53.26\n",
            "\n",
            "Epoch [5/80], Step [000/157], LR 5.8e-05, Loss: 1653.0\n",
            "Epoch [5/80], Step [050/157], LR 6.4e-05, Loss: 1656.5\n",
            "Epoch [5/80], Step [100/157], LR 7.0e-05, Loss: 1656.2\n",
            "Epoch [5/80], Step [150/157], LR 7.7e-05, Loss: 1637.0\n",
            "starting validation\n",
            "mAP score regular 56.87, mAP score EMA 46.79\n",
            "current_mAP = 56.87, highest_mAP = 56.87\n",
            "\n",
            "Epoch [6/80], Step [000/157], LR 7.8e-05, Loss: 1641.0\n",
            "Epoch [6/80], Step [050/157], LR 8.4e-05, Loss: 1629.1\n",
            "Epoch [6/80], Step [100/157], LR 9.1e-05, Loss: 1627.4\n",
            "Epoch [6/80], Step [150/157], LR 9.7e-05, Loss: 1625.6\n",
            "starting validation\n",
            "mAP score regular 60.64, mAP score EMA 47.16\n",
            "current_mAP = 60.64, highest_mAP = 60.64\n",
            "\n",
            "Epoch [7/80], Step [000/157], LR 9.8e-05, Loss: 1607.3\n",
            "Epoch [7/80], Step [050/157], LR 1.1e-04, Loss: 1607.9\n",
            "Epoch [7/80], Step [100/157], LR 1.1e-04, Loss: 1607.4\n",
            "Epoch [7/80], Step [150/157], LR 1.2e-04, Loss: 1587.7\n",
            "starting validation\n",
            "mAP score regular 64.65, mAP score EMA 47.65\n",
            "current_mAP = 64.65, highest_mAP = 64.65\n",
            "\n",
            "Epoch [8/80], Step [000/157], LR 1.2e-04, Loss: 1566.3\n",
            "Epoch [8/80], Step [050/157], LR 1.3e-04, Loss: 1573.8\n",
            "Epoch [8/80], Step [100/157], LR 1.3e-04, Loss: 1561.7\n",
            "Epoch [8/80], Step [150/157], LR 1.4e-04, Loss: 1561.0\n",
            "starting validation\n",
            "mAP score regular 68.24, mAP score EMA 48.29\n",
            "current_mAP = 68.24, highest_mAP = 68.24\n",
            "\n",
            "Epoch [9/80], Step [000/157], LR 1.4e-04, Loss: 1531.2\n",
            "Epoch [9/80], Step [050/157], LR 1.5e-04, Loss: 1517.3\n",
            "Epoch [9/80], Step [100/157], LR 1.5e-04, Loss: 1540.9\n",
            "Epoch [9/80], Step [150/157], LR 1.6e-04, Loss: 1541.6\n",
            "starting validation\n",
            "mAP score regular 71.44, mAP score EMA 49.08\n",
            "current_mAP = 71.44, highest_mAP = 71.44\n",
            "\n",
            "Epoch [10/80], Step [000/157], LR 1.6e-04, Loss: 1508.4\n",
            "Epoch [10/80], Step [050/157], LR 1.7e-04, Loss: 1502.2\n",
            "Epoch [10/80], Step [100/157], LR 1.7e-04, Loss: 1495.9\n",
            "Epoch [10/80], Step [150/157], LR 1.8e-04, Loss: 1486.4\n",
            "starting validation\n",
            "mAP score regular 73.88, mAP score EMA 50.01\n",
            "current_mAP = 73.88, highest_mAP = 73.88\n",
            "\n",
            "Epoch [11/80], Step [000/157], LR 1.8e-04, Loss: 1460.1\n",
            "Epoch [11/80], Step [050/157], LR 1.9e-04, Loss: 1468.1\n",
            "Epoch [11/80], Step [100/157], LR 1.9e-04, Loss: 1458.5\n",
            "Epoch [11/80], Step [150/157], LR 2.0e-04, Loss: 1452.7\n",
            "starting validation\n",
            "mAP score regular 76.12, mAP score EMA 51.03\n",
            "current_mAP = 76.12, highest_mAP = 76.12\n",
            "\n",
            "Epoch [12/80], Step [000/157], LR 2.0e-04, Loss: 1428.8\n",
            "Epoch [12/80], Step [050/157], LR 2.0e-04, Loss: 1445.7\n",
            "Epoch [12/80], Step [100/157], LR 2.1e-04, Loss: 1422.1\n",
            "Epoch [12/80], Step [150/157], LR 2.1e-04, Loss: 1427.7\n",
            "starting validation\n",
            "mAP score regular 77.74, mAP score EMA 52.14\n",
            "current_mAP = 77.74, highest_mAP = 77.74\n",
            "\n",
            "Epoch [13/80], Step [000/157], LR 2.1e-04, Loss: 1397.8\n",
            "Epoch [13/80], Step [050/157], LR 2.2e-04, Loss: 1407.6\n",
            "Epoch [13/80], Step [100/157], LR 2.2e-04, Loss: 1377.9\n",
            "Epoch [13/80], Step [150/157], LR 2.2e-04, Loss: 1364.7\n",
            "starting validation\n",
            "mAP score regular 79.32, mAP score EMA 53.32\n",
            "current_mAP = 79.32, highest_mAP = 79.32\n",
            "\n",
            "Epoch [14/80], Step [000/157], LR 2.2e-04, Loss: 1356.1\n",
            "Epoch [14/80], Step [050/157], LR 2.2e-04, Loss: 1357.5\n",
            "Epoch [14/80], Step [100/157], LR 2.3e-04, Loss: 1349.0\n",
            "Epoch [14/80], Step [150/157], LR 2.3e-04, Loss: 1350.8\n",
            "starting validation\n",
            "mAP score regular 80.37, mAP score EMA 54.57\n",
            "current_mAP = 80.37, highest_mAP = 80.37\n",
            "\n",
            "Epoch [15/80], Step [000/157], LR 2.3e-04, Loss: 1311.6\n",
            "Epoch [15/80], Step [050/157], LR 2.3e-04, Loss: 1305.9\n",
            "Epoch [15/80], Step [100/157], LR 2.3e-04, Loss: 1310.0\n",
            "Epoch [15/80], Step [150/157], LR 2.3e-04, Loss: 1314.4\n",
            "starting validation\n",
            "mAP score regular 80.91, mAP score EMA 55.85\n",
            "current_mAP = 80.91, highest_mAP = 80.91\n",
            "\n",
            "Epoch [16/80], Step [000/157], LR 2.3e-04, Loss: 1285.0\n",
            "Epoch [16/80], Step [050/157], LR 2.3e-04, Loss: 1305.3\n",
            "Epoch [16/80], Step [100/157], LR 2.3e-04, Loss: 1270.4\n",
            "Epoch [16/80], Step [150/157], LR 2.3e-04, Loss: 1276.7\n",
            "starting validation\n",
            "mAP score regular 81.49, mAP score EMA 57.15\n",
            "current_mAP = 81.49, highest_mAP = 81.49\n",
            "\n",
            "Epoch [17/80], Step [000/157], LR 2.3e-04, Loss: 1245.5\n",
            "Epoch [17/80], Step [050/157], LR 2.3e-04, Loss: 1239.8\n",
            "Epoch [17/80], Step [100/157], LR 2.3e-04, Loss: 1256.3\n",
            "Epoch [17/80], Step [150/157], LR 2.3e-04, Loss: 1265.8\n",
            "starting validation\n",
            "mAP score regular 81.73, mAP score EMA 58.45\n",
            "current_mAP = 81.73, highest_mAP = 81.73\n",
            "\n",
            "Epoch [18/80], Step [000/157], LR 2.3e-04, Loss: 1232.1\n",
            "Epoch [18/80], Step [050/157], LR 2.3e-04, Loss: 1260.6\n",
            "Epoch [18/80], Step [100/157], LR 2.3e-04, Loss: 1223.6\n",
            "Epoch [18/80], Step [150/157], LR 2.3e-04, Loss: 1201.8\n",
            "starting validation\n",
            "mAP score regular 81.85, mAP score EMA 59.74\n",
            "current_mAP = 81.85, highest_mAP = 81.85\n",
            "\n",
            "Epoch [19/80], Step [000/157], LR 2.3e-04, Loss: 1163.8\n",
            "Epoch [19/80], Step [050/157], LR 2.3e-04, Loss: 1185.1\n",
            "Epoch [19/80], Step [100/157], LR 2.3e-04, Loss: 1212.7\n",
            "Epoch [19/80], Step [150/157], LR 2.3e-04, Loss: 1203.6\n",
            "starting validation\n",
            "mAP score regular 81.92, mAP score EMA 61.02\n",
            "current_mAP = 81.92, highest_mAP = 81.92\n",
            "\n",
            "Epoch [20/80], Step [000/157], LR 2.3e-04, Loss: 1191.7\n",
            "Epoch [20/80], Step [050/157], LR 2.3e-04, Loss: 1194.3\n",
            "Epoch [20/80], Step [100/157], LR 2.3e-04, Loss: 1163.6\n",
            "Epoch [20/80], Step [150/157], LR 2.3e-04, Loss: 1162.9\n",
            "starting validation\n",
            "mAP score regular 81.82, mAP score EMA 62.29\n",
            "current_mAP = 81.82, highest_mAP = 81.92\n",
            "\n",
            "Epoch [21/80], Step [000/157], LR 2.3e-04, Loss: 1149.1\n",
            "Epoch [21/80], Step [050/157], LR 2.3e-04, Loss: 1139.3\n",
            "Epoch [21/80], Step [100/157], LR 2.3e-04, Loss: 1116.7\n",
            "Epoch [21/80], Step [150/157], LR 2.3e-04, Loss: 1124.6\n",
            "starting validation\n",
            "mAP score regular 81.78, mAP score EMA 63.54\n",
            "current_mAP = 81.78, highest_mAP = 81.92\n",
            "\n",
            "Epoch [22/80], Step [000/157], LR 2.3e-04, Loss: 1077.4\n",
            "Epoch [22/80], Step [050/157], LR 2.2e-04, Loss: 1092.7\n",
            "Epoch [22/80], Step [100/157], LR 2.2e-04, Loss: 1094.8\n",
            "Epoch [22/80], Step [150/157], LR 2.2e-04, Loss: 1122.4\n",
            "starting validation\n",
            "mAP score regular 81.57, mAP score EMA 64.76\n",
            "current_mAP = 81.57, highest_mAP = 81.92\n",
            "\n",
            "Epoch [23/80], Step [000/157], LR 2.2e-04, Loss: 1069.7\n",
            "Epoch [23/80], Step [050/157], LR 2.2e-04, Loss: 1071.5\n",
            "Epoch [23/80], Step [100/157], LR 2.2e-04, Loss: 1084.4\n",
            "Epoch [23/80], Step [150/157], LR 2.2e-04, Loss: 1064.5\n",
            "starting validation\n",
            "mAP score regular 81.64, mAP score EMA 65.97\n",
            "current_mAP = 81.64, highest_mAP = 81.92\n",
            "\n",
            "Epoch [24/80], Step [000/157], LR 2.2e-04, Loss: 1024.9\n",
            "Epoch [24/80], Step [050/157], LR 2.2e-04, Loss: 1049.5\n",
            "Epoch [24/80], Step [100/157], LR 2.2e-04, Loss: 1061.8\n",
            "Epoch [24/80], Step [150/157], LR 2.2e-04, Loss: 1058.3\n",
            "starting validation\n",
            "mAP score regular 81.46, mAP score EMA 67.15\n",
            "current_mAP = 81.46, highest_mAP = 81.92\n",
            "\n",
            "Epoch [25/80], Step [000/157], LR 2.2e-04, Loss: 997.3\n",
            "Epoch [25/80], Step [050/157], LR 2.2e-04, Loss: 1008.7\n",
            "Epoch [25/80], Step [100/157], LR 2.2e-04, Loss: 997.0\n",
            "Epoch [25/80], Step [150/157], LR 2.2e-04, Loss: 1000.8\n",
            "starting validation\n",
            "mAP score regular 81.08, mAP score EMA 68.27\n",
            "current_mAP = 81.08, highest_mAP = 81.92\n",
            "\n",
            "Epoch [26/80], Step [000/157], LR 2.2e-04, Loss: 976.6\n",
            "Epoch [26/80], Step [050/157], LR 2.2e-04, Loss: 956.1\n",
            "Epoch [26/80], Step [100/157], LR 2.1e-04, Loss: 981.4\n",
            "Epoch [26/80], Step [150/157], LR 2.1e-04, Loss: 982.7\n",
            "starting validation\n",
            "mAP score regular 81.08, mAP score EMA 69.37\n",
            "current_mAP = 81.08, highest_mAP = 81.92\n",
            "\n",
            "Epoch [27/80], Step [000/157], LR 2.1e-04, Loss: 946.8\n",
            "Epoch [27/80], Step [050/157], LR 2.1e-04, Loss: 899.0\n",
            "Epoch [27/80], Step [100/157], LR 2.1e-04, Loss: 925.4\n",
            "Epoch [27/80], Step [150/157], LR 2.1e-04, Loss: 957.6\n",
            "starting validation\n",
            "mAP score regular 81.02, mAP score EMA 70.41\n",
            "current_mAP = 81.02, highest_mAP = 81.92\n",
            "\n",
            "Epoch [28/80], Step [000/157], LR 2.1e-04, Loss: 948.0\n",
            "Epoch [28/80], Step [050/157], LR 2.1e-04, Loss: 919.6\n",
            "Epoch [28/80], Step [100/157], LR 2.1e-04, Loss: 943.7\n",
            "Epoch [28/80], Step [150/157], LR 2.1e-04, Loss: 929.9\n",
            "starting validation\n",
            "mAP score regular 80.98, mAP score EMA 71.40\n",
            "current_mAP = 80.98, highest_mAP = 81.92\n",
            "\n",
            "Epoch [29/80], Step [000/157], LR 2.1e-04, Loss: 854.6\n",
            "Epoch [29/80], Step [050/157], LR 2.1e-04, Loss: 903.2\n",
            "Epoch [29/80], Step [100/157], LR 2.1e-04, Loss: 912.1\n",
            "Epoch [29/80], Step [150/157], LR 2.0e-04, Loss: 907.1\n",
            "starting validation\n",
            "mAP score regular 80.65, mAP score EMA 72.34\n",
            "current_mAP = 80.65, highest_mAP = 81.92\n",
            "\n",
            "Epoch [30/80], Step [000/157], LR 2.0e-04, Loss: 899.1\n",
            "Epoch [30/80], Step [050/157], LR 2.0e-04, Loss: 866.8\n",
            "Epoch [30/80], Step [100/157], LR 2.0e-04, Loss: 863.3\n",
            "Epoch [30/80], Step [150/157], LR 2.0e-04, Loss: 868.5\n",
            "starting validation\n",
            "mAP score regular 80.17, mAP score EMA 73.21\n",
            "current_mAP = 80.17, highest_mAP = 81.92\n",
            "\n",
            "Epoch [31/80], Step [000/157], LR 2.0e-04, Loss: 821.5\n",
            "Epoch [31/80], Step [050/157], LR 2.0e-04, Loss: 856.7\n",
            "Epoch [31/80], Step [100/157], LR 2.0e-04, Loss: 913.8\n",
            "Epoch [31/80], Step [150/157], LR 2.0e-04, Loss: 858.3\n",
            "starting validation\n",
            "mAP score regular 80.33, mAP score EMA 74.02\n",
            "current_mAP = 80.33, highest_mAP = 81.92\n",
            "\n",
            "Epoch [32/80], Step [000/157], LR 2.0e-04, Loss: 821.6\n",
            "Epoch [32/80], Step [050/157], LR 1.9e-04, Loss: 789.4\n",
            "Epoch [32/80], Step [100/157], LR 1.9e-04, Loss: 822.7\n",
            "Epoch [32/80], Step [150/157], LR 1.9e-04, Loss: 802.3\n",
            "starting validation\n",
            "mAP score regular 80.09, mAP score EMA 74.76\n",
            "current_mAP = 80.09, highest_mAP = 81.92\n",
            "\n",
            "Epoch [33/80], Step [000/157], LR 1.9e-04, Loss: 762.3\n",
            "Epoch [33/80], Step [050/157], LR 1.9e-04, Loss: 834.6\n",
            "Epoch [33/80], Step [100/157], LR 1.9e-04, Loss: 784.3\n",
            "Epoch [33/80], Step [150/157], LR 1.9e-04, Loss: 791.9\n",
            "starting validation\n",
            "mAP score regular 80.21, mAP score EMA 75.45\n",
            "current_mAP = 80.21, highest_mAP = 81.92\n",
            "\n",
            "Epoch [34/80], Step [000/157], LR 1.9e-04, Loss: 742.4\n",
            "Epoch [34/80], Step [050/157], LR 1.9e-04, Loss: 749.6\n",
            "Epoch [34/80], Step [100/157], LR 1.9e-04, Loss: 803.7\n",
            "Epoch [34/80], Step [150/157], LR 1.8e-04, Loss: 741.5\n",
            "starting validation\n",
            "mAP score regular 79.97, mAP score EMA 76.08\n",
            "current_mAP = 79.97, highest_mAP = 81.92\n",
            "\n",
            "Epoch [35/80], Step [000/157], LR 1.8e-04, Loss: 732.3\n",
            "Epoch [35/80], Step [050/157], LR 1.8e-04, Loss: 797.1\n",
            "Epoch [35/80], Step [100/157], LR 1.8e-04, Loss: 785.6\n",
            "Epoch [35/80], Step [150/157], LR 1.8e-04, Loss: 789.9\n",
            "starting validation\n",
            "mAP score regular 80.01, mAP score EMA 76.66\n",
            "current_mAP = 80.01, highest_mAP = 81.92\n",
            "\n",
            "Epoch [36/80], Step [000/157], LR 1.8e-04, Loss: 749.3\n",
            "Epoch [36/80], Step [050/157], LR 1.8e-04, Loss: 770.9\n",
            "Epoch [36/80], Step [100/157], LR 1.8e-04, Loss: 689.0\n",
            "Epoch [36/80], Step [150/157], LR 1.7e-04, Loss: 744.5\n",
            "starting validation\n",
            "mAP score regular 79.67, mAP score EMA 77.19\n",
            "current_mAP = 79.67, highest_mAP = 81.92\n",
            "\n",
            "Epoch [37/80], Step [000/157], LR 1.7e-04, Loss: 675.4\n",
            "Epoch [37/80], Step [050/157], LR 1.7e-04, Loss: 741.2\n",
            "Epoch [37/80], Step [100/157], LR 1.7e-04, Loss: 737.7\n",
            "Epoch [37/80], Step [150/157], LR 1.7e-04, Loss: 696.8\n",
            "starting validation\n",
            "mAP score regular 79.57, mAP score EMA 77.68\n",
            "current_mAP = 79.57, highest_mAP = 81.92\n",
            "\n",
            "Epoch [38/80], Step [000/157], LR 1.7e-04, Loss: 775.4\n",
            "Epoch [38/80], Step [050/157], LR 1.7e-04, Loss: 719.9\n",
            "Epoch [38/80], Step [100/157], LR 1.7e-04, Loss: 700.3\n",
            "Epoch [38/80], Step [150/157], LR 1.6e-04, Loss: 704.4\n",
            "starting validation\n",
            "mAP score regular 79.67, mAP score EMA 78.12\n",
            "current_mAP = 79.67, highest_mAP = 81.92\n",
            "\n",
            "Epoch [39/80], Step [000/157], LR 1.6e-04, Loss: 687.3\n",
            "Epoch [39/80], Step [050/157], LR 1.6e-04, Loss: 723.2\n",
            "Epoch [39/80], Step [100/157], LR 1.6e-04, Loss: 688.3\n",
            "Epoch [39/80], Step [150/157], LR 1.6e-04, Loss: 714.4\n",
            "starting validation\n",
            "mAP score regular 79.32, mAP score EMA 78.52\n",
            "current_mAP = 79.32, highest_mAP = 81.92\n",
            "\n",
            "Epoch [40/80], Step [000/157], LR 1.6e-04, Loss: 648.1\n",
            "Epoch [40/80], Step [050/157], LR 1.6e-04, Loss: 697.5\n",
            "Epoch [40/80], Step [100/157], LR 1.6e-04, Loss: 733.9\n",
            "Epoch [40/80], Step [150/157], LR 1.5e-04, Loss: 630.3\n",
            "starting validation\n",
            "mAP score regular 79.40, mAP score EMA 78.88\n",
            "current_mAP = 79.40, highest_mAP = 81.92\n",
            "\n",
            "Epoch [41/80], Step [000/157], LR 1.5e-04, Loss: 651.2\n",
            "Epoch [41/80], Step [050/157], LR 1.5e-04, Loss: 619.0\n",
            "Epoch [41/80], Step [100/157], LR 1.5e-04, Loss: 689.8\n",
            "Epoch [41/80], Step [150/157], LR 1.5e-04, Loss: 671.8\n",
            "starting validation\n",
            "mAP score regular 79.53, mAP score EMA 79.20\n",
            "current_mAP = 79.53, highest_mAP = 81.92\n",
            "\n",
            "Epoch [42/80], Step [000/157], LR 1.5e-04, Loss: 634.2\n",
            "Epoch [42/80], Step [050/157], LR 1.5e-04, Loss: 654.7\n",
            "Epoch [42/80], Step [100/157], LR 1.4e-04, Loss: 661.8\n",
            "Epoch [42/80], Step [150/157], LR 1.4e-04, Loss: 655.6\n",
            "starting validation\n",
            "mAP score regular 79.16, mAP score EMA 79.50\n",
            "current_mAP = 79.50, highest_mAP = 81.92\n",
            "\n",
            "Epoch [43/80], Step [000/157], LR 1.4e-04, Loss: 664.3\n",
            "Epoch [43/80], Step [050/157], LR 1.4e-04, Loss: 629.1\n",
            "Epoch [43/80], Step [100/157], LR 1.4e-04, Loss: 614.3\n",
            "Epoch [43/80], Step [150/157], LR 1.4e-04, Loss: 631.0\n",
            "starting validation\n",
            "mAP score regular 79.35, mAP score EMA 79.76\n",
            "current_mAP = 79.76, highest_mAP = 81.92\n",
            "\n",
            "Epoch [44/80], Step [000/157], LR 1.4e-04, Loss: 622.7\n",
            "Epoch [44/80], Step [050/157], LR 1.4e-04, Loss: 572.3\n",
            "Epoch [44/80], Step [100/157], LR 1.3e-04, Loss: 601.1\n",
            "Epoch [44/80], Step [150/157], LR 1.3e-04, Loss: 613.2\n",
            "starting validation\n",
            "mAP score regular 79.27, mAP score EMA 79.99\n",
            "current_mAP = 79.99, highest_mAP = 81.92\n",
            "\n",
            "Epoch [45/80], Step [000/157], LR 1.3e-04, Loss: 639.3\n",
            "Epoch [45/80], Step [050/157], LR 1.3e-04, Loss: 596.7\n",
            "Epoch [45/80], Step [100/157], LR 1.3e-04, Loss: 616.4\n",
            "Epoch [45/80], Step [150/157], LR 1.3e-04, Loss: 562.0\n",
            "starting validation\n",
            "mAP score regular 79.30, mAP score EMA 80.20\n",
            "current_mAP = 80.20, highest_mAP = 81.92\n",
            "\n",
            "Epoch [46/80], Step [000/157], LR 1.3e-04, Loss: 610.4\n",
            "Epoch [46/80], Step [050/157], LR 1.2e-04, Loss: 613.0\n",
            "Epoch [46/80], Step [100/157], LR 1.2e-04, Loss: 615.5\n",
            "Epoch [46/80], Step [150/157], LR 1.2e-04, Loss: 623.0\n",
            "starting validation\n",
            "mAP score regular 78.85, mAP score EMA 80.38\n",
            "current_mAP = 80.38, highest_mAP = 81.92\n",
            "\n",
            "Epoch [47/80], Step [000/157], LR 1.2e-04, Loss: 582.4\n",
            "Epoch [47/80], Step [050/157], LR 1.2e-04, Loss: 595.4\n",
            "Epoch [47/80], Step [100/157], LR 1.2e-04, Loss: 641.0\n",
            "Epoch [47/80], Step [150/157], LR 1.2e-04, Loss: 632.4\n",
            "starting validation\n",
            "mAP score regular 79.05, mAP score EMA 80.55\n",
            "current_mAP = 80.55, highest_mAP = 81.92\n",
            "\n",
            "Epoch [48/80], Step [000/157], LR 1.1e-04, Loss: 617.4\n",
            "Epoch [48/80], Step [050/157], LR 1.1e-04, Loss: 604.6\n",
            "Epoch [48/80], Step [100/157], LR 1.1e-04, Loss: 572.0\n",
            "Epoch [48/80], Step [150/157], LR 1.1e-04, Loss: 570.3\n",
            "starting validation\n",
            "mAP score regular 78.95, mAP score EMA 80.69\n",
            "current_mAP = 80.69, highest_mAP = 81.92\n",
            "\n",
            "Epoch [49/80], Step [000/157], LR 1.1e-04, Loss: 591.2\n",
            "Epoch [49/80], Step [050/157], LR 1.1e-04, Loss: 610.0\n",
            "Epoch [49/80], Step [100/157], LR 1.1e-04, Loss: 547.0\n",
            "Epoch [49/80], Step [150/157], LR 1.0e-04, Loss: 548.3\n",
            "starting validation\n",
            "mAP score regular 78.94, mAP score EMA 80.81\n",
            "current_mAP = 80.81, highest_mAP = 81.92\n",
            "\n",
            "Epoch [50/80], Step [000/157], LR 1.0e-04, Loss: 574.6\n",
            "Epoch [50/80], Step [050/157], LR 1.0e-04, Loss: 576.9\n",
            "Epoch [50/80], Step [100/157], LR 1.0e-04, Loss: 613.8\n",
            "Epoch [50/80], Step [150/157], LR 9.8e-05, Loss: 531.2\n",
            "starting validation\n",
            "mAP score regular 78.77, mAP score EMA 80.91\n",
            "current_mAP = 80.91, highest_mAP = 81.92\n",
            "\n",
            "Epoch [51/80], Step [000/157], LR 9.8e-05, Loss: 545.6\n",
            "Epoch [51/80], Step [050/157], LR 9.6e-05, Loss: 526.7\n",
            "Epoch [51/80], Step [100/157], LR 9.5e-05, Loss: 528.3\n",
            "Epoch [51/80], Step [150/157], LR 9.3e-05, Loss: 500.2\n",
            "starting validation\n",
            "mAP score regular 78.81, mAP score EMA 81.00\n",
            "current_mAP = 81.00, highest_mAP = 81.92\n",
            "\n",
            "Epoch [52/80], Step [000/157], LR 9.2e-05, Loss: 477.2\n",
            "Epoch [52/80], Step [050/157], LR 9.1e-05, Loss: 558.5\n",
            "Epoch [52/80], Step [100/157], LR 8.9e-05, Loss: 556.3\n",
            "Epoch [52/80], Step [150/157], LR 8.7e-05, Loss: 570.7\n",
            "starting validation\n",
            "mAP score regular 78.64, mAP score EMA 81.08\n",
            "current_mAP = 81.08, highest_mAP = 81.92\n",
            "\n",
            "Epoch [53/80], Step [000/157], LR 8.7e-05, Loss: 557.2\n",
            "Epoch [53/80], Step [050/157], LR 8.5e-05, Loss: 526.1\n",
            "Epoch [53/80], Step [100/157], LR 8.4e-05, Loss: 519.3\n",
            "Epoch [53/80], Step [150/157], LR 8.2e-05, Loss: 544.0\n",
            "starting validation\n",
            "mAP score regular 78.64, mAP score EMA 81.14\n",
            "current_mAP = 81.14, highest_mAP = 81.92\n",
            "\n",
            "Epoch [54/80], Step [000/157], LR 8.2e-05, Loss: 562.3\n",
            "Epoch [54/80], Step [050/157], LR 8.0e-05, Loss: 528.4\n",
            "Epoch [54/80], Step [100/157], LR 7.8e-05, Loss: 505.7\n",
            "Epoch [54/80], Step [150/157], LR 7.6e-05, Loss: 512.2\n",
            "starting validation\n",
            "mAP score regular 78.51, mAP score EMA 81.18\n",
            "current_mAP = 81.18, highest_mAP = 81.92\n",
            "\n",
            "Epoch [55/80], Step [000/157], LR 7.6e-05, Loss: 503.0\n",
            "Epoch [55/80], Step [050/157], LR 7.5e-05, Loss: 559.8\n",
            "Epoch [55/80], Step [100/157], LR 7.3e-05, Loss: 519.0\n",
            "Epoch [55/80], Step [150/157], LR 7.1e-05, Loss: 472.2\n",
            "starting validation\n",
            "mAP score regular 78.52, mAP score EMA 81.20\n",
            "current_mAP = 81.20, highest_mAP = 81.92\n",
            "\n",
            "Epoch [56/80], Step [000/157], LR 7.1e-05, Loss: 517.2\n",
            "Epoch [56/80], Step [050/157], LR 6.9e-05, Loss: 535.5\n",
            "Epoch [56/80], Step [100/157], LR 6.8e-05, Loss: 479.1\n",
            "Epoch [56/80], Step [150/157], LR 6.6e-05, Loss: 503.3\n",
            "starting validation\n",
            "mAP score regular 78.53, mAP score EMA 81.23\n",
            "current_mAP = 81.23, highest_mAP = 81.92\n",
            "\n",
            "Epoch [57/80], Step [000/157], LR 6.6e-05, Loss: 490.5\n",
            "Epoch [57/80], Step [050/157], LR 6.4e-05, Loss: 427.6\n",
            "Epoch [57/80], Step [100/157], LR 6.3e-05, Loss: 522.6\n",
            "Epoch [57/80], Step [150/157], LR 6.1e-05, Loss: 499.2\n",
            "starting validation\n",
            "mAP score regular 78.53, mAP score EMA 81.24\n",
            "current_mAP = 81.24, highest_mAP = 81.92\n",
            "\n",
            "Epoch [58/80], Step [000/157], LR 6.1e-05, Loss: 499.7\n",
            "Epoch [58/80], Step [050/157], LR 5.9e-05, Loss: 490.8\n",
            "Epoch [58/80], Step [100/157], LR 5.8e-05, Loss: 557.9\n",
            "Epoch [58/80], Step [150/157], LR 5.6e-05, Loss: 506.6\n",
            "starting validation\n",
            "mAP score regular 78.32, mAP score EMA 81.24\n",
            "current_mAP = 81.24, highest_mAP = 81.92\n",
            "\n",
            "Epoch [59/80], Step [000/157], LR 5.6e-05, Loss: 496.7\n",
            "Epoch [59/80], Step [050/157], LR 5.4e-05, Loss: 516.1\n",
            "Epoch [59/80], Step [100/157], LR 5.3e-05, Loss: 465.7\n",
            "Epoch [59/80], Step [150/157], LR 5.1e-05, Loss: 472.8\n",
            "starting validation\n",
            "mAP score regular 78.54, mAP score EMA 81.23\n",
            "current_mAP = 81.23, highest_mAP = 81.92\n",
            "\n",
            "Epoch [60/80], Step [000/157], LR 5.1e-05, Loss: 461.3\n",
            "Epoch [60/80], Step [050/157], LR 5.0e-05, Loss: 515.1\n",
            "Epoch [60/80], Step [100/157], LR 4.8e-05, Loss: 534.8\n",
            "Epoch [60/80], Step [150/157], LR 4.7e-05, Loss: 432.2\n",
            "starting validation\n",
            "mAP score regular 78.42, mAP score EMA 81.22\n",
            "current_mAP = 81.22, highest_mAP = 81.92\n",
            "\n",
            "Epoch [61/80], Step [000/157], LR 4.6e-05, Loss: 496.6\n",
            "Epoch [61/80], Step [050/157], LR 4.5e-05, Loss: 443.2\n",
            "Epoch [61/80], Step [100/157], LR 4.4e-05, Loss: 488.4\n",
            "Epoch [61/80], Step [150/157], LR 4.2e-05, Loss: 503.7\n",
            "starting validation\n",
            "mAP score regular 78.37, mAP score EMA 81.21\n",
            "current_mAP = 81.21, highest_mAP = 81.92\n",
            "\n",
            "Epoch [62/80], Step [000/157], LR 4.2e-05, Loss: 485.5\n",
            "Epoch [62/80], Step [050/157], LR 4.1e-05, Loss: 472.7\n",
            "Epoch [62/80], Step [100/157], LR 3.9e-05, Loss: 489.4\n",
            "Epoch [62/80], Step [150/157], LR 3.8e-05, Loss: 496.4\n",
            "starting validation\n",
            "mAP score regular 78.35, mAP score EMA 81.18\n",
            "current_mAP = 81.18, highest_mAP = 81.92\n",
            "\n",
            "Epoch [63/80], Step [000/157], LR 3.8e-05, Loss: 509.9\n",
            "Epoch [63/80], Step [050/157], LR 3.6e-05, Loss: 509.0\n",
            "Epoch [63/80], Step [100/157], LR 3.5e-05, Loss: 513.4\n",
            "Epoch [63/80], Step [150/157], LR 3.4e-05, Loss: 498.1\n",
            "starting validation\n",
            "mAP score regular 78.28, mAP score EMA 81.14\n",
            "current_mAP = 81.14, highest_mAP = 81.92\n",
            "\n",
            "Epoch [64/80], Step [000/157], LR 3.4e-05, Loss: 484.7\n",
            "Epoch [64/80], Step [050/157], LR 3.2e-05, Loss: 484.5\n",
            "Epoch [64/80], Step [100/157], LR 3.1e-05, Loss: 477.8\n",
            "Epoch [64/80], Step [150/157], LR 3.0e-05, Loss: 463.2\n",
            "starting validation\n",
            "mAP score regular 78.34, mAP score EMA 81.12\n",
            "current_mAP = 81.12, highest_mAP = 81.92\n",
            "\n",
            "Epoch [65/80], Step [000/157], LR 3.0e-05, Loss: 465.4\n",
            "Epoch [65/80], Step [050/157], LR 2.9e-05, Loss: 465.1\n",
            "Epoch [65/80], Step [100/157], LR 2.7e-05, Loss: 448.9\n",
            "Epoch [65/80], Step [150/157], LR 2.6e-05, Loss: 484.8\n",
            "starting validation\n",
            "mAP score regular 78.33, mAP score EMA 81.07\n",
            "current_mAP = 81.07, highest_mAP = 81.92\n",
            "\n",
            "Epoch [66/80], Step [000/157], LR 2.6e-05, Loss: 479.9\n",
            "Epoch [66/80], Step [050/157], LR 2.5e-05, Loss: 458.6\n",
            "Epoch [66/80], Step [100/157], LR 2.4e-05, Loss: 428.9\n",
            "Epoch [66/80], Step [150/157], LR 2.3e-05, Loss: 511.3\n",
            "starting validation\n",
            "mAP score regular 78.41, mAP score EMA 81.03\n",
            "current_mAP = 81.03, highest_mAP = 81.92\n",
            "\n",
            "Epoch [67/80], Step [000/157], LR 2.3e-05, Loss: 498.6\n",
            "Epoch [67/80], Step [050/157], LR 2.2e-05, Loss: 493.4\n",
            "Epoch [67/80], Step [100/157], LR 2.0e-05, Loss: 425.5\n",
            "Epoch [67/80], Step [150/157], LR 1.9e-05, Loss: 430.4\n",
            "starting validation\n",
            "mAP score regular 78.29, mAP score EMA 80.97\n",
            "current_mAP = 80.97, highest_mAP = 81.92\n",
            "\n",
            "Epoch [68/80], Step [000/157], LR 1.9e-05, Loss: 444.1\n",
            "Epoch [68/80], Step [050/157], LR 1.8e-05, Loss: 491.2\n",
            "Epoch [68/80], Step [100/157], LR 1.7e-05, Loss: 439.9\n",
            "Epoch [68/80], Step [150/157], LR 1.6e-05, Loss: 387.6\n",
            "starting validation\n",
            "mAP score regular 78.40, mAP score EMA 80.93\n",
            "current_mAP = 80.93, highest_mAP = 81.92\n",
            "\n",
            "Epoch [69/80], Step [000/157], LR 1.6e-05, Loss: 417.4\n",
            "Epoch [69/80], Step [050/157], LR 1.5e-05, Loss: 503.6\n",
            "Epoch [69/80], Step [100/157], LR 1.5e-05, Loss: 425.3\n",
            "Epoch [69/80], Step [150/157], LR 1.4e-05, Loss: 437.9\n",
            "starting validation\n",
            "mAP score regular 78.28, mAP score EMA 80.88\n",
            "current_mAP = 80.88, highest_mAP = 81.92\n",
            "\n",
            "Epoch [70/80], Step [000/157], LR 1.4e-05, Loss: 449.2\n",
            "Epoch [70/80], Step [050/157], LR 1.3e-05, Loss: 458.8\n",
            "Epoch [70/80], Step [100/157], LR 1.2e-05, Loss: 483.7\n",
            "Epoch [70/80], Step [150/157], LR 1.1e-05, Loss: 442.8\n",
            "starting validation\n",
            "mAP score regular 78.24, mAP score EMA 80.81\n",
            "current_mAP = 80.81, highest_mAP = 81.92\n",
            "\n",
            "Epoch [71/80], Step [000/157], LR 1.1e-05, Loss: 488.1\n",
            "Epoch [71/80], Step [050/157], LR 1.0e-05, Loss: 479.0\n",
            "Epoch [71/80], Step [100/157], LR 9.5e-06, Loss: 451.7\n",
            "Epoch [71/80], Step [150/157], LR 8.8e-06, Loss: 423.8\n",
            "starting validation\n",
            "mAP score regular 78.28, mAP score EMA 80.76\n",
            "current_mAP = 80.76, highest_mAP = 81.92\n",
            "\n",
            "Epoch [72/80], Step [000/157], LR 8.7e-06, Loss: 522.6\n",
            "Epoch [72/80], Step [050/157], LR 8.1e-06, Loss: 477.3\n",
            "Epoch [72/80], Step [100/157], LR 7.4e-06, Loss: 469.0\n",
            "Epoch [72/80], Step [150/157], LR 6.8e-06, Loss: 506.8\n",
            "starting validation\n",
            "mAP score regular 78.30, mAP score EMA 80.70\n",
            "current_mAP = 80.70, highest_mAP = 81.92\n",
            "\n",
            "Epoch [73/80], Step [000/157], LR 6.7e-06, Loss: 457.7\n",
            "Epoch [73/80], Step [050/157], LR 6.1e-06, Loss: 441.5\n",
            "Epoch [73/80], Step [100/157], LR 5.5e-06, Loss: 455.0\n",
            "Epoch [73/80], Step [150/157], LR 5.0e-06, Loss: 412.7\n",
            "starting validation\n",
            "mAP score regular 78.26, mAP score EMA 80.63\n",
            "current_mAP = 80.63, highest_mAP = 81.92\n",
            "\n",
            "Epoch [74/80], Step [000/157], LR 4.9e-06, Loss: 434.0\n",
            "Epoch [74/80], Step [050/157], LR 4.4e-06, Loss: 481.7\n",
            "Epoch [74/80], Step [100/157], LR 3.9e-06, Loss: 419.3\n",
            "Epoch [74/80], Step [150/157], LR 3.5e-06, Loss: 424.5\n",
            "starting validation\n",
            "mAP score regular 78.26, mAP score EMA 80.58\n",
            "current_mAP = 80.58, highest_mAP = 81.92\n",
            "\n",
            "Epoch [75/80], Step [000/157], LR 3.4e-06, Loss: 457.4\n",
            "Epoch [75/80], Step [050/157], LR 3.0e-06, Loss: 442.0\n",
            "Epoch [75/80], Step [100/157], LR 2.6e-06, Loss: 434.8\n",
            "Epoch [75/80], Step [150/157], LR 2.2e-06, Loss: 463.4\n",
            "starting validation\n",
            "mAP score regular 78.29, mAP score EMA 80.51\n",
            "current_mAP = 80.51, highest_mAP = 81.92\n",
            "\n",
            "Epoch [76/80], Step [000/157], LR 2.2e-06, Loss: 427.6\n",
            "Epoch [76/80], Step [050/157], LR 1.9e-06, Loss: 466.5\n",
            "Epoch [76/80], Step [100/157], LR 1.6e-06, Loss: 453.5\n",
            "Epoch [76/80], Step [150/157], LR 1.3e-06, Loss: 487.0\n",
            "starting validation\n",
            "mAP score regular 78.26, mAP score EMA 80.44\n",
            "current_mAP = 80.44, highest_mAP = 81.92\n",
            "\n",
            "Epoch [77/80], Step [000/157], LR 1.2e-06, Loss: 430.8\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VyupVaLHt8L1",
        "outputId": "130af79c-ea51-4707-db73-9f7d9cbc38e7"
      },
      "source": [
        "import torch\n",
        "from src.helper_functions.helper_functions import mAP, CocoDetection, CutoutPIL, ModelEma, add_weight_decay\n",
        "from src.loss_functions.losses import AsymmetricLoss, AsymmetricLossOptimized\n",
        "from src.models import create_model\n",
        "import argparse\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "# parsing args\n",
        "args = easydict.EasyDict({ \"model_path\": \"./models/model-highest.pth\",\n",
        "                          \"model_name\": \"tresnet_m\",\n",
        "                          \"workers\": 4, \n",
        "                          \"dataset_type\": \"MS-COCO\", \n",
        "                          \"th\": 0.8,\n",
        "                          \"num_classes\": 26\n",
        "                          })\n",
        "instances_path_test = '/content/drive/My Drive/dacon_cv/test.json'\n",
        "data_path_test = '/content/drive/My Drive/dacon_cv/test_dirty_mnist'\n",
        "test_dataset = CocoDetection(data_path_test,\n",
        "                            instances_path_test,\n",
        "                            transforms.Compose([\n",
        "                                transforms.Resize((224, 224)),\n",
        "                                transforms.ToTensor(),\n",
        "                            ]))\n",
        "\n",
        "test_data_loader = torch.utils.data.DataLoader(\n",
        "    test_dataset, batch_size=128, shuffle=False,\n",
        "    num_workers=4, pin_memory=True, drop_last=False)\n",
        "\n",
        "predictions_list = []\n",
        "# 배치 단위로 추론\n",
        "prediction_df = pd.read_csv(\"/content/drive/My Drive/dacon_cv/sample_submission.csv\")\n",
        "prediction_array = np.zeros([prediction_df.shape[0],\n",
        "                            prediction_df.shape[1]- 1])\n",
        "\n",
        "\n",
        "                    \n",
        "print(\"test dset: \", test_dataset)\n",
        "\n",
        "# setup model\n",
        "print('creating and loading the model...')\n",
        "state = torch.load(args.model_path, map_location='cpu')\n",
        "model = create_model(args).cuda()\n",
        "model.load_state_dict(state, strict=True)\n",
        "for idx, sample in enumerate(test_data_loader):\n",
        "    with torch.no_grad():\n",
        "        # 추론\n",
        "        model.eval()\n",
        "        images = sample[0].cuda()\n",
        "        probs  = model(images)\n",
        "        probs = probs.cpu().detach().numpy()\n",
        "        preds = (probs > 0.7)\n",
        "\n",
        "        # 예측 결과를 \n",
        "        # prediction_array에 입력\n",
        "        batch_index = 128 * idx\n",
        "        prediction_array[batch_index: batch_index + images.shape[0],:] = preds.astype(int)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loading annotations into memory...\n",
            "Done (t=0.03s)\n",
            "creating index...\n",
            "index created!\n",
            "test dset:  Dataset CocoDetection\n",
            "    Number of datapoints: 5000\n",
            "    Root location: /content/drive/My Drive/dacon_cv/test_dirty_mnist\n",
            "creating and loading the model...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "qAfZbq4s4FPS",
        "outputId": "fdf67076-ca1c-4e82-d07d-7513015305ad"
      },
      "source": [
        "sample_submission = pd.read_csv(\"../sample_submission.csv\")\n",
        "sample_submission.iloc[:,1:] = prediction_array\n",
        "sample_submission.to_csv(\"prediction.csv\", index = False)\n",
        "sample_submission"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>index</th>\n",
              "      <th>a</th>\n",
              "      <th>b</th>\n",
              "      <th>c</th>\n",
              "      <th>d</th>\n",
              "      <th>e</th>\n",
              "      <th>f</th>\n",
              "      <th>g</th>\n",
              "      <th>h</th>\n",
              "      <th>i</th>\n",
              "      <th>j</th>\n",
              "      <th>k</th>\n",
              "      <th>l</th>\n",
              "      <th>m</th>\n",
              "      <th>n</th>\n",
              "      <th>o</th>\n",
              "      <th>p</th>\n",
              "      <th>q</th>\n",
              "      <th>r</th>\n",
              "      <th>s</th>\n",
              "      <th>t</th>\n",
              "      <th>u</th>\n",
              "      <th>v</th>\n",
              "      <th>w</th>\n",
              "      <th>x</th>\n",
              "      <th>y</th>\n",
              "      <th>z</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>50000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>50001</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>50002</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>50003</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>50004</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4995</th>\n",
              "      <td>54995</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4996</th>\n",
              "      <td>54996</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4997</th>\n",
              "      <td>54997</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4998</th>\n",
              "      <td>54998</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4999</th>\n",
              "      <td>54999</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5000 rows × 27 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      index    a    b    c    d    e    f  ...    t    u    v    w    x    y    z\n",
              "0     50000  0.0  0.0  1.0  0.0  0.0  1.0  ...  0.0  1.0  0.0  0.0  1.0  0.0  0.0\n",
              "1     50001  0.0  1.0  0.0  1.0  1.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  1.0  0.0\n",
              "2     50002  0.0  0.0  1.0  0.0  1.0  1.0  ...  0.0  0.0  0.0  1.0  0.0  0.0  1.0\n",
              "3     50003  1.0  1.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0  1.0  0.0  0.0\n",
              "4     50004  0.0  0.0  1.0  0.0  1.0  1.0  ...  0.0  0.0  1.0  1.0  0.0  0.0  0.0\n",
              "...     ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...\n",
              "4995  54995  0.0  0.0  1.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  1.0  0.0  0.0  0.0\n",
              "4996  54996  1.0  0.0  1.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  1.0\n",
              "4997  54997  1.0  0.0  0.0  1.0  0.0  1.0  ...  0.0  1.0  0.0  0.0  0.0  0.0  1.0\n",
              "4998  54998  0.0  0.0  1.0  0.0  0.0  1.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  1.0\n",
              "4999  54999  1.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
              "\n",
              "[5000 rows x 27 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 120
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A63wZEevqBXv",
        "outputId": "55afcc86-4f1e-44c0-826b-a49fb95d5294"
      },
      "source": [
        "import os\n",
        "from copy import deepcopy\n",
        "import random\n",
        "import time\n",
        "from copy import deepcopy\n",
        "\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from torchvision import datasets as datasets\n",
        "import torch\n",
        "from PIL import ImageDraw\n",
        "from pycocotools.coco import COCO\n",
        "\n",
        "class CocoDetection(datasets.coco.CocoDetection):\n",
        "    def __init__(self, root, annFile, transform=None, target_transform=None):\n",
        "        self.root = root\n",
        "        self.coco = COCO(annFile)\n",
        "        self.ids = list(self.coco.imgToAnns.keys())\n",
        "        print(self.ids)\n",
        "        self.transform = transform\n",
        "        self.target_transform = target_transform\n",
        "        self.cat2cat = dict()\n",
        "        for cat in self.coco.cats.keys():\n",
        "            self.cat2cat[cat] = len(self.cat2cat)\n",
        "        # print(self.cat2cat)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        coco = self.coco\n",
        "        img_id = self.ids[index]\n",
        "        ann_ids = coco.getAnnIds(imgIds=img_id)\n",
        "        target = coco.loadAnns(ann_ids)\n",
        "\n",
        "        output = torch.zeros((3, 26), dtype=torch.long)\n",
        "        for obj in target:\n",
        "            if obj['area'] < 32 * 32:\n",
        "                output[0][self.cat2cat[obj['category_id']]] = 1\n",
        "            elif obj['area'] < 96 * 96:\n",
        "                output[1][self.cat2cat[obj['category_id']]] = 1\n",
        "            else:\n",
        "                output[2][self.cat2cat[obj['category_id']]] = 1\n",
        "        target = output\n",
        "\n",
        "        path = coco.loadImgs(img_id)[0]['file_name']\n",
        "        img = Image.open(os.path.join(self.root, path)).convert('RGB')\n",
        "        if self.transform is not None:\n",
        "            img = self.transform(img)\n",
        "\n",
        "        if self.target_transform is not None:\n",
        "            target = self.target_transform(target)\n",
        "        return img, target\n",
        "\n",
        "test_dataset = CocoDetection(data_path_test,\n",
        "                            instances_path_test,\n",
        "                            transforms.Compose([\n",
        "                                transforms.Resize((224, 224)),\n",
        "                                transforms.ToTensor(),\n",
        "                            ]))\n",
        "test_dataset"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loading annotations into memory...\n",
            "Done (t=0.02s)\n",
            "creating index...\n",
            "index created!\n",
            "[50000, 50001, 50002, 50003, 50004, 50005, 50006, 50007, 50008, 50009, 50010, 50011, 50012, 50013, 50014, 50015, 50016, 50017, 50018, 50019, 50020, 50021, 50022, 50023, 50024, 50025, 50026, 50027, 50028, 50029, 50030, 50031, 50032, 50033, 50034, 50035, 50036, 50037, 50038, 50039, 50040, 50041, 50042, 50043, 50044, 50045, 50046, 50047, 50048, 50049, 50050, 50051, 50052, 50053, 50054, 50055, 50056, 50057, 50058, 50059, 50060, 50061, 50062, 50063, 50064, 50065, 50066, 50067, 50068, 50069, 50070, 50071, 50072, 50073, 50074, 50075, 50076, 50077, 50078, 50079, 50080, 50081, 50082, 50083, 50084, 50085, 50086, 50087, 50088, 50089, 50090, 50091, 50092, 50093, 50094, 50095, 50096, 50097, 50098, 50099, 50100, 50101, 50102, 50103, 50104, 50105, 50106, 50107, 50108, 50109, 50110, 50111, 50112, 50113, 50114, 50115, 50116, 50117, 50118, 50119, 50120, 50121, 50122, 50123, 50124, 50125, 50126, 50127, 50128, 50129, 50130, 50131, 50132, 50133, 50134, 50135, 50136, 50137, 50138, 50139, 50140, 50141, 50142, 50143, 50144, 50145, 50146, 50147, 50148, 50149, 50150, 50151, 50152, 50153, 50154, 50155, 50156, 50157, 50158, 50159, 50160, 50161, 50162, 50163, 50164, 50165, 50166, 50167, 50168, 50169, 50170, 50171, 50172, 50173, 50174, 50175, 50176, 50177, 50178, 50179, 50180, 50181, 50182, 50183, 50184, 50185, 50186, 50187, 50188, 50189, 50190, 50191, 50192, 50193, 50194, 50195, 50196, 50197, 50198, 50199, 50200, 50201, 50202, 50203, 50204, 50205, 50206, 50207, 50208, 50209, 50210, 50211, 50212, 50213, 50214, 50215, 50216, 50217, 50218, 50219, 50220, 50221, 50222, 50223, 50224, 50225, 50226, 50227, 50228, 50229, 50230, 50231, 50232, 50233, 50234, 50235, 50236, 50237, 50238, 50239, 50240, 50241, 50242, 50243, 50244, 50245, 50246, 50247, 50248, 50249, 50250, 50251, 50252, 50253, 50254, 50255, 50256, 50257, 50258, 50259, 50260, 50261, 50262, 50263, 50264, 50265, 50266, 50267, 50268, 50269, 50270, 50271, 50272, 50273, 50274, 50275, 50276, 50277, 50278, 50279, 50280, 50281, 50282, 50283, 50284, 50285, 50286, 50287, 50288, 50289, 50290, 50291, 50292, 50293, 50294, 50295, 50296, 50297, 50298, 50299, 50300, 50301, 50302, 50303, 50304, 50305, 50306, 50307, 50308, 50309, 50310, 50311, 50312, 50313, 50314, 50315, 50316, 50317, 50318, 50319, 50320, 50321, 50322, 50323, 50324, 50325, 50326, 50327, 50328, 50329, 50330, 50331, 50332, 50333, 50334, 50335, 50336, 50337, 50338, 50339, 50340, 50341, 50342, 50343, 50344, 50345, 50346, 50347, 50348, 50349, 50350, 50351, 50352, 50353, 50354, 50355, 50356, 50357, 50358, 50359, 50360, 50361, 50362, 50363, 50364, 50365, 50366, 50367, 50368, 50369, 50370, 50371, 50372, 50373, 50374, 50375, 50376, 50377, 50378, 50379, 50380, 50381, 50382, 50383, 50384, 50385, 50386, 50387, 50388, 50389, 50390, 50391, 50392, 50393, 50394, 50395, 50396, 50397, 50398, 50399, 50400, 50401, 50402, 50403, 50404, 50405, 50406, 50407, 50408, 50409, 50410, 50411, 50412, 50413, 50414, 50415, 50416, 50417, 50418, 50419, 50420, 50421, 50422, 50423, 50424, 50425, 50426, 50427, 50428, 50429, 50430, 50431, 50432, 50433, 50434, 50435, 50436, 50437, 50438, 50439, 50440, 50441, 50442, 50443, 50444, 50445, 50446, 50447, 50448, 50449, 50450, 50451, 50452, 50453, 50454, 50455, 50456, 50457, 50458, 50459, 50460, 50461, 50462, 50463, 50464, 50465, 50466, 50467, 50468, 50469, 50470, 50471, 50472, 50473, 50474, 50475, 50476, 50477, 50478, 50479, 50480, 50481, 50482, 50483, 50484, 50485, 50486, 50487, 50488, 50489, 50490, 50491, 50492, 50493, 50494, 50495, 50496, 50497, 50498, 50499, 50500, 50501, 50502, 50503, 50504, 50505, 50506, 50507, 50508, 50509, 50510, 50511, 50512, 50513, 50514, 50515, 50516, 50517, 50518, 50519, 50520, 50521, 50522, 50523, 50524, 50525, 50526, 50527, 50528, 50529, 50530, 50531, 50532, 50533, 50534, 50535, 50536, 50537, 50538, 50539, 50540, 50541, 50542, 50543, 50544, 50545, 50546, 50547, 50548, 50549, 50550, 50551, 50552, 50553, 50554, 50555, 50556, 50557, 50558, 50559, 50560, 50561, 50562, 50563, 50564, 50565, 50566, 50567, 50568, 50569, 50570, 50571, 50572, 50573, 50574, 50575, 50576, 50577, 50578, 50579, 50580, 50581, 50582, 50583, 50584, 50585, 50586, 50587, 50588, 50589, 50590, 50591, 50592, 50593, 50594, 50595, 50596, 50597, 50598, 50599, 50600, 50601, 50602, 50603, 50604, 50605, 50606, 50607, 50608, 50609, 50610, 50611, 50612, 50613, 50614, 50615, 50616, 50617, 50618, 50619, 50620, 50621, 50622, 50623, 50624, 50625, 50626, 50627, 50628, 50629, 50630, 50631, 50632, 50633, 50634, 50635, 50636, 50637, 50638, 50639, 50640, 50641, 50642, 50643, 50644, 50645, 50646, 50647, 50648, 50649, 50650, 50651, 50652, 50653, 50654, 50655, 50656, 50657, 50658, 50659, 50660, 50661, 50662, 50663, 50664, 50665, 50666, 50667, 50668, 50669, 50670, 50671, 50672, 50673, 50674, 50675, 50676, 50677, 50678, 50679, 50680, 50681, 50682, 50683, 50684, 50685, 50686, 50687, 50688, 50689, 50690, 50691, 50692, 50693, 50694, 50695, 50696, 50697, 50698, 50699, 50700, 50701, 50702, 50703, 50704, 50705, 50706, 50707, 50708, 50709, 50710, 50711, 50712, 50713, 50714, 50715, 50716, 50717, 50718, 50719, 50720, 50721, 50722, 50723, 50724, 50725, 50726, 50727, 50728, 50729, 50730, 50731, 50732, 50733, 50734, 50735, 50736, 50737, 50738, 50739, 50740, 50741, 50742, 50743, 50744, 50745, 50746, 50747, 50748, 50749, 50750, 50751, 50752, 50753, 50754, 50755, 50756, 50757, 50758, 50759, 50760, 50761, 50762, 50763, 50764, 50765, 50766, 50767, 50768, 50769, 50770, 50771, 50772, 50773, 50774, 50775, 50776, 50777, 50778, 50779, 50780, 50781, 50782, 50783, 50784, 50785, 50786, 50787, 50788, 50789, 50790, 50791, 50792, 50793, 50794, 50795, 50796, 50797, 50798, 50799, 50800, 50801, 50802, 50803, 50804, 50805, 50806, 50807, 50808, 50809, 50810, 50811, 50812, 50813, 50814, 50815, 50816, 50817, 50818, 50819, 50820, 50821, 50822, 50823, 50824, 50825, 50826, 50827, 50828, 50829, 50830, 50831, 50832, 50833, 50834, 50835, 50836, 50837, 50838, 50839, 50840, 50841, 50842, 50843, 50844, 50845, 50846, 50847, 50848, 50849, 50850, 50851, 50852, 50853, 50854, 50855, 50856, 50857, 50858, 50859, 50860, 50861, 50862, 50863, 50864, 50865, 50866, 50867, 50868, 50869, 50870, 50871, 50872, 50873, 50874, 50875, 50876, 50877, 50878, 50879, 50880, 50881, 50882, 50883, 50884, 50885, 50886, 50887, 50888, 50889, 50890, 50891, 50892, 50893, 50894, 50895, 50896, 50897, 50898, 50899, 50900, 50901, 50902, 50903, 50904, 50905, 50906, 50907, 50908, 50909, 50910, 50911, 50912, 50913, 50914, 50915, 50916, 50917, 50918, 50919, 50920, 50921, 50922, 50923, 50924, 50925, 50926, 50927, 50928, 50929, 50930, 50931, 50932, 50933, 50934, 50935, 50936, 50937, 50938, 50939, 50940, 50941, 50942, 50943, 50944, 50945, 50946, 50947, 50948, 50949, 50950, 50951, 50952, 50953, 50954, 50955, 50956, 50957, 50958, 50959, 50960, 50961, 50962, 50963, 50964, 50965, 50966, 50967, 50968, 50969, 50970, 50971, 50972, 50973, 50974, 50975, 50976, 50977, 50978, 50979, 50980, 50981, 50982, 50983, 50984, 50985, 50986, 50987, 50988, 50989, 50990, 50991, 50992, 50993, 50994, 50995, 50996, 50997, 50998, 50999, 51000, 51001, 51002, 51003, 51004, 51005, 51006, 51007, 51008, 51009, 51010, 51011, 51012, 51013, 51014, 51015, 51016, 51017, 51018, 51019, 51020, 51021, 51022, 51023, 51024, 51025, 51026, 51027, 51028, 51029, 51030, 51031, 51032, 51033, 51034, 51035, 51036, 51037, 51038, 51039, 51040, 51041, 51042, 51043, 51044, 51045, 51046, 51047, 51048, 51049, 51050, 51051, 51052, 51053, 51054, 51055, 51056, 51057, 51058, 51059, 51060, 51061, 51062, 51063, 51064, 51065, 51066, 51067, 51068, 51069, 51070, 51071, 51072, 51073, 51074, 51075, 51076, 51077, 51078, 51079, 51080, 51081, 51082, 51083, 51084, 51085, 51086, 51087, 51088, 51089, 51090, 51091, 51092, 51093, 51094, 51095, 51096, 51097, 51098, 51099, 51100, 51101, 51102, 51103, 51104, 51105, 51106, 51107, 51108, 51109, 51110, 51111, 51112, 51113, 51114, 51115, 51116, 51117, 51118, 51119, 51120, 51121, 51122, 51123, 51124, 51125, 51126, 51127, 51128, 51129, 51130, 51131, 51132, 51133, 51134, 51135, 51136, 51137, 51138, 51139, 51140, 51141, 51142, 51143, 51144, 51145, 51146, 51147, 51148, 51149, 51150, 51151, 51152, 51153, 51154, 51155, 51156, 51157, 51158, 51159, 51160, 51161, 51162, 51163, 51164, 51165, 51166, 51167, 51168, 51169, 51170, 51171, 51172, 51173, 51174, 51175, 51176, 51177, 51178, 51179, 51180, 51181, 51182, 51183, 51184, 51185, 51186, 51187, 51188, 51189, 51190, 51191, 51192, 51193, 51194, 51195, 51196, 51197, 51198, 51199, 51200, 51201, 51202, 51203, 51204, 51205, 51206, 51207, 51208, 51209, 51210, 51211, 51212, 51213, 51214, 51215, 51216, 51217, 51218, 51219, 51220, 51221, 51222, 51223, 51224, 51225, 51226, 51227, 51228, 51229, 51230, 51231, 51232, 51233, 51234, 51235, 51236, 51237, 51238, 51239, 51240, 51241, 51242, 51243, 51244, 51245, 51246, 51247, 51248, 51249, 51250, 51251, 51252, 51253, 51254, 51255, 51256, 51257, 51258, 51259, 51260, 51261, 51262, 51263, 51264, 51265, 51266, 51267, 51268, 51269, 51270, 51271, 51272, 51273, 51274, 51275, 51276, 51277, 51278, 51279, 51280, 51281, 51282, 51283, 51284, 51285, 51286, 51287, 51288, 51289, 51290, 51291, 51292, 51293, 51294, 51295, 51296, 51297, 51298, 51299, 51300, 51301, 51302, 51303, 51304, 51305, 51306, 51307, 51308, 51309, 51310, 51311, 51312, 51313, 51314, 51315, 51316, 51317, 51318, 51319, 51320, 51321, 51322, 51323, 51324, 51325, 51326, 51327, 51328, 51329, 51330, 51331, 51332, 51333, 51334, 51335, 51336, 51337, 51338, 51339, 51340, 51341, 51342, 51343, 51344, 51345, 51346, 51347, 51348, 51349, 51350, 51351, 51352, 51353, 51354, 51355, 51356, 51357, 51358, 51359, 51360, 51361, 51362, 51363, 51364, 51365, 51366, 51367, 51368, 51369, 51370, 51371, 51372, 51373, 51374, 51375, 51376, 51377, 51378, 51379, 51380, 51381, 51382, 51383, 51384, 51385, 51386, 51387, 51388, 51389, 51390, 51391, 51392, 51393, 51394, 51395, 51396, 51397, 51398, 51399, 51400, 51401, 51402, 51403, 51404, 51405, 51406, 51407, 51408, 51409, 51410, 51411, 51412, 51413, 51414, 51415, 51416, 51417, 51418, 51419, 51420, 51421, 51422, 51423, 51424, 51425, 51426, 51427, 51428, 51429, 51430, 51431, 51432, 51433, 51434, 51435, 51436, 51437, 51438, 51439, 51440, 51441, 51442, 51443, 51444, 51445, 51446, 51447, 51448, 51449, 51450, 51451, 51452, 51453, 51454, 51455, 51456, 51457, 51458, 51459, 51460, 51461, 51462, 51463, 51464, 51465, 51466, 51467, 51468, 51469, 51470, 51471, 51472, 51473, 51474, 51475, 51476, 51477, 51478, 51479, 51480, 51481, 51482, 51483, 51484, 51485, 51486, 51487, 51488, 51489, 51490, 51491, 51492, 51493, 51494, 51495, 51496, 51497, 51498, 51499, 51500, 51501, 51502, 51503, 51504, 51505, 51506, 51507, 51508, 51509, 51510, 51511, 51512, 51513, 51514, 51515, 51516, 51517, 51518, 51519, 51520, 51521, 51522, 51523, 51524, 51525, 51526, 51527, 51528, 51529, 51530, 51531, 51532, 51533, 51534, 51535, 51536, 51537, 51538, 51539, 51540, 51541, 51542, 51543, 51544, 51545, 51546, 51547, 51548, 51549, 51550, 51551, 51552, 51553, 51554, 51555, 51556, 51557, 51558, 51559, 51560, 51561, 51562, 51563, 51564, 51565, 51566, 51567, 51568, 51569, 51570, 51571, 51572, 51573, 51574, 51575, 51576, 51577, 51578, 51579, 51580, 51581, 51582, 51583, 51584, 51585, 51586, 51587, 51588, 51589, 51590, 51591, 51592, 51593, 51594, 51595, 51596, 51597, 51598, 51599, 51600, 51601, 51602, 51603, 51604, 51605, 51606, 51607, 51608, 51609, 51610, 51611, 51612, 51613, 51614, 51615, 51616, 51617, 51618, 51619, 51620, 51621, 51622, 51623, 51624, 51625, 51626, 51627, 51628, 51629, 51630, 51631, 51632, 51633, 51634, 51635, 51636, 51637, 51638, 51639, 51640, 51641, 51642, 51643, 51644, 51645, 51646, 51647, 51648, 51649, 51650, 51651, 51652, 51653, 51654, 51655, 51656, 51657, 51658, 51659, 51660, 51661, 51662, 51663, 51664, 51665, 51666, 51667, 51668, 51669, 51670, 51671, 51672, 51673, 51674, 51675, 51676, 51677, 51678, 51679, 51680, 51681, 51682, 51683, 51684, 51685, 51686, 51687, 51688, 51689, 51690, 51691, 51692, 51693, 51694, 51695, 51696, 51697, 51698, 51699, 51700, 51701, 51702, 51703, 51704, 51705, 51706, 51707, 51708, 51709, 51710, 51711, 51712, 51713, 51714, 51715, 51716, 51717, 51718, 51719, 51720, 51721, 51722, 51723, 51724, 51725, 51726, 51727, 51728, 51729, 51730, 51731, 51732, 51733, 51734, 51735, 51736, 51737, 51738, 51739, 51740, 51741, 51742, 51743, 51744, 51745, 51746, 51747, 51748, 51749, 51750, 51751, 51752, 51753, 51754, 51755, 51756, 51757, 51758, 51759, 51760, 51761, 51762, 51763, 51764, 51765, 51766, 51767, 51768, 51769, 51770, 51771, 51772, 51773, 51774, 51775, 51776, 51777, 51778, 51779, 51780, 51781, 51782, 51783, 51784, 51785, 51786, 51787, 51788, 51789, 51790, 51791, 51792, 51793, 51794, 51795, 51796, 51797, 51798, 51799, 51800, 51801, 51802, 51803, 51804, 51805, 51806, 51807, 51808, 51809, 51810, 51811, 51812, 51813, 51814, 51815, 51816, 51817, 51818, 51819, 51820, 51821, 51822, 51823, 51824, 51825, 51826, 51827, 51828, 51829, 51830, 51831, 51832, 51833, 51834, 51835, 51836, 51837, 51838, 51839, 51840, 51841, 51842, 51843, 51844, 51845, 51846, 51847, 51848, 51849, 51850, 51851, 51852, 51853, 51854, 51855, 51856, 51857, 51858, 51859, 51860, 51861, 51862, 51863, 51864, 51865, 51866, 51867, 51868, 51869, 51870, 51871, 51872, 51873, 51874, 51875, 51876, 51877, 51878, 51879, 51880, 51881, 51882, 51883, 51884, 51885, 51886, 51887, 51888, 51889, 51890, 51891, 51892, 51893, 51894, 51895, 51896, 51897, 51898, 51899, 51900, 51901, 51902, 51903, 51904, 51905, 51906, 51907, 51908, 51909, 51910, 51911, 51912, 51913, 51914, 51915, 51916, 51917, 51918, 51919, 51920, 51921, 51922, 51923, 51924, 51925, 51926, 51927, 51928, 51929, 51930, 51931, 51932, 51933, 51934, 51935, 51936, 51937, 51938, 51939, 51940, 51941, 51942, 51943, 51944, 51945, 51946, 51947, 51948, 51949, 51950, 51951, 51952, 51953, 51954, 51955, 51956, 51957, 51958, 51959, 51960, 51961, 51962, 51963, 51964, 51965, 51966, 51967, 51968, 51969, 51970, 51971, 51972, 51973, 51974, 51975, 51976, 51977, 51978, 51979, 51980, 51981, 51982, 51983, 51984, 51985, 51986, 51987, 51988, 51989, 51990, 51991, 51992, 51993, 51994, 51995, 51996, 51997, 51998, 51999, 52000, 52001, 52002, 52003, 52004, 52005, 52006, 52007, 52008, 52009, 52010, 52011, 52012, 52013, 52014, 52015, 52016, 52017, 52018, 52019, 52020, 52021, 52022, 52023, 52024, 52025, 52026, 52027, 52028, 52029, 52030, 52031, 52032, 52033, 52034, 52035, 52036, 52037, 52038, 52039, 52040, 52041, 52042, 52043, 52044, 52045, 52046, 52047, 52048, 52049, 52050, 52051, 52052, 52053, 52054, 52055, 52056, 52057, 52058, 52059, 52060, 52061, 52062, 52063, 52064, 52065, 52066, 52067, 52068, 52069, 52070, 52071, 52072, 52073, 52074, 52075, 52076, 52077, 52078, 52079, 52080, 52081, 52082, 52083, 52084, 52085, 52086, 52087, 52088, 52089, 52090, 52091, 52092, 52093, 52094, 52095, 52096, 52097, 52098, 52099, 52100, 52101, 52102, 52103, 52104, 52105, 52106, 52107, 52108, 52109, 52110, 52111, 52112, 52113, 52114, 52115, 52116, 52117, 52118, 52119, 52120, 52121, 52122, 52123, 52124, 52125, 52126, 52127, 52128, 52129, 52130, 52131, 52132, 52133, 52134, 52135, 52136, 52137, 52138, 52139, 52140, 52141, 52142, 52143, 52144, 52145, 52146, 52147, 52148, 52149, 52150, 52151, 52152, 52153, 52154, 52155, 52156, 52157, 52158, 52159, 52160, 52161, 52162, 52163, 52164, 52165, 52166, 52167, 52168, 52169, 52170, 52171, 52172, 52173, 52174, 52175, 52176, 52177, 52178, 52179, 52180, 52181, 52182, 52183, 52184, 52185, 52186, 52187, 52188, 52189, 52190, 52191, 52192, 52193, 52194, 52195, 52196, 52197, 52198, 52199, 52200, 52201, 52202, 52203, 52204, 52205, 52206, 52207, 52208, 52209, 52210, 52211, 52212, 52213, 52214, 52215, 52216, 52217, 52218, 52219, 52220, 52221, 52222, 52223, 52224, 52225, 52226, 52227, 52228, 52229, 52230, 52231, 52232, 52233, 52234, 52235, 52236, 52237, 52238, 52239, 52240, 52241, 52242, 52243, 52244, 52245, 52246, 52247, 52248, 52249, 52250, 52251, 52252, 52253, 52254, 52255, 52256, 52257, 52258, 52259, 52260, 52261, 52262, 52263, 52264, 52265, 52266, 52267, 52268, 52269, 52270, 52271, 52272, 52273, 52274, 52275, 52276, 52277, 52278, 52279, 52280, 52281, 52282, 52283, 52284, 52285, 52286, 52287, 52288, 52289, 52290, 52291, 52292, 52293, 52294, 52295, 52296, 52297, 52298, 52299, 52300, 52301, 52302, 52303, 52304, 52305, 52306, 52307, 52308, 52309, 52310, 52311, 52312, 52313, 52314, 52315, 52316, 52317, 52318, 52319, 52320, 52321, 52322, 52323, 52324, 52325, 52326, 52327, 52328, 52329, 52330, 52331, 52332, 52333, 52334, 52335, 52336, 52337, 52338, 52339, 52340, 52341, 52342, 52343, 52344, 52345, 52346, 52347, 52348, 52349, 52350, 52351, 52352, 52353, 52354, 52355, 52356, 52357, 52358, 52359, 52360, 52361, 52362, 52363, 52364, 52365, 52366, 52367, 52368, 52369, 52370, 52371, 52372, 52373, 52374, 52375, 52376, 52377, 52378, 52379, 52380, 52381, 52382, 52383, 52384, 52385, 52386, 52387, 52388, 52389, 52390, 52391, 52392, 52393, 52394, 52395, 52396, 52397, 52398, 52399, 52400, 52401, 52402, 52403, 52404, 52405, 52406, 52407, 52408, 52409, 52410, 52411, 52412, 52413, 52414, 52415, 52416, 52417, 52418, 52419, 52420, 52421, 52422, 52423, 52424, 52425, 52426, 52427, 52428, 52429, 52430, 52431, 52432, 52433, 52434, 52435, 52436, 52437, 52438, 52439, 52440, 52441, 52442, 52443, 52444, 52445, 52446, 52447, 52448, 52449, 52450, 52451, 52452, 52453, 52454, 52455, 52456, 52457, 52458, 52459, 52460, 52461, 52462, 52463, 52464, 52465, 52466, 52467, 52468, 52469, 52470, 52471, 52472, 52473, 52474, 52475, 52476, 52477, 52478, 52479, 52480, 52481, 52482, 52483, 52484, 52485, 52486, 52487, 52488, 52489, 52490, 52491, 52492, 52493, 52494, 52495, 52496, 52497, 52498, 52499, 52500, 52501, 52502, 52503, 52504, 52505, 52506, 52507, 52508, 52509, 52510, 52511, 52512, 52513, 52514, 52515, 52516, 52517, 52518, 52519, 52520, 52521, 52522, 52523, 52524, 52525, 52526, 52527, 52528, 52529, 52530, 52531, 52532, 52533, 52534, 52535, 52536, 52537, 52538, 52539, 52540, 52541, 52542, 52543, 52544, 52545, 52546, 52547, 52548, 52549, 52550, 52551, 52552, 52553, 52554, 52555, 52556, 52557, 52558, 52559, 52560, 52561, 52562, 52563, 52564, 52565, 52566, 52567, 52568, 52569, 52570, 52571, 52572, 52573, 52574, 52575, 52576, 52577, 52578, 52579, 52580, 52581, 52582, 52583, 52584, 52585, 52586, 52587, 52588, 52589, 52590, 52591, 52592, 52593, 52594, 52595, 52596, 52597, 52598, 52599, 52600, 52601, 52602, 52603, 52604, 52605, 52606, 52607, 52608, 52609, 52610, 52611, 52612, 52613, 52614, 52615, 52616, 52617, 52618, 52619, 52620, 52621, 52622, 52623, 52624, 52625, 52626, 52627, 52628, 52629, 52630, 52631, 52632, 52633, 52634, 52635, 52636, 52637, 52638, 52639, 52640, 52641, 52642, 52643, 52644, 52645, 52646, 52647, 52648, 52649, 52650, 52651, 52652, 52653, 52654, 52655, 52656, 52657, 52658, 52659, 52660, 52661, 52662, 52663, 52664, 52665, 52666, 52667, 52668, 52669, 52670, 52671, 52672, 52673, 52674, 52675, 52676, 52677, 52678, 52679, 52680, 52681, 52682, 52683, 52684, 52685, 52686, 52687, 52688, 52689, 52690, 52691, 52692, 52693, 52694, 52695, 52696, 52697, 52698, 52699, 52700, 52701, 52702, 52703, 52704, 52705, 52706, 52707, 52708, 52709, 52710, 52711, 52712, 52713, 52714, 52715, 52716, 52717, 52718, 52719, 52720, 52721, 52722, 52723, 52724, 52725, 52726, 52727, 52728, 52729, 52730, 52731, 52732, 52733, 52734, 52735, 52736, 52737, 52738, 52739, 52740, 52741, 52742, 52743, 52744, 52745, 52746, 52747, 52748, 52749, 52750, 52751, 52752, 52753, 52754, 52755, 52756, 52757, 52758, 52759, 52760, 52761, 52762, 52763, 52764, 52765, 52766, 52767, 52768, 52769, 52770, 52771, 52772, 52773, 52774, 52775, 52776, 52777, 52778, 52779, 52780, 52781, 52782, 52783, 52784, 52785, 52786, 52787, 52788, 52789, 52790, 52791, 52792, 52793, 52794, 52795, 52796, 52797, 52798, 52799, 52800, 52801, 52802, 52803, 52804, 52805, 52806, 52807, 52808, 52809, 52810, 52811, 52812, 52813, 52814, 52815, 52816, 52817, 52818, 52819, 52820, 52821, 52822, 52823, 52824, 52825, 52826, 52827, 52828, 52829, 52830, 52831, 52832, 52833, 52834, 52835, 52836, 52837, 52838, 52839, 52840, 52841, 52842, 52843, 52844, 52845, 52846, 52847, 52848, 52849, 52850, 52851, 52852, 52853, 52854, 52855, 52856, 52857, 52858, 52859, 52860, 52861, 52862, 52863, 52864, 52865, 52866, 52867, 52868, 52869, 52870, 52871, 52872, 52873, 52874, 52875, 52876, 52877, 52878, 52879, 52880, 52881, 52882, 52883, 52884, 52885, 52886, 52887, 52888, 52889, 52890, 52891, 52892, 52893, 52894, 52895, 52896, 52897, 52898, 52899, 52900, 52901, 52902, 52903, 52904, 52905, 52906, 52907, 52908, 52909, 52910, 52911, 52912, 52913, 52914, 52915, 52916, 52917, 52918, 52919, 52920, 52921, 52922, 52923, 52924, 52925, 52926, 52927, 52928, 52929, 52930, 52931, 52932, 52933, 52934, 52935, 52936, 52937, 52938, 52939, 52940, 52941, 52942, 52943, 52944, 52945, 52946, 52947, 52948, 52949, 52950, 52951, 52952, 52953, 52954, 52955, 52956, 52957, 52958, 52959, 52960, 52961, 52962, 52963, 52964, 52965, 52966, 52967, 52968, 52969, 52970, 52971, 52972, 52973, 52974, 52975, 52976, 52977, 52978, 52979, 52980, 52981, 52982, 52983, 52984, 52985, 52986, 52987, 52988, 52989, 52990, 52991, 52992, 52993, 52994, 52995, 52996, 52997, 52998, 52999, 53000, 53001, 53002, 53003, 53004, 53005, 53006, 53007, 53008, 53009, 53010, 53011, 53012, 53013, 53014, 53015, 53016, 53017, 53018, 53019, 53020, 53021, 53022, 53023, 53024, 53025, 53026, 53027, 53028, 53029, 53030, 53031, 53032, 53033, 53034, 53035, 53036, 53037, 53038, 53039, 53040, 53041, 53042, 53043, 53044, 53045, 53046, 53047, 53048, 53049, 53050, 53051, 53052, 53053, 53054, 53055, 53056, 53057, 53058, 53059, 53060, 53061, 53062, 53063, 53064, 53065, 53066, 53067, 53068, 53069, 53070, 53071, 53072, 53073, 53074, 53075, 53076, 53077, 53078, 53079, 53080, 53081, 53082, 53083, 53084, 53085, 53086, 53087, 53088, 53089, 53090, 53091, 53092, 53093, 53094, 53095, 53096, 53097, 53098, 53099, 53100, 53101, 53102, 53103, 53104, 53105, 53106, 53107, 53108, 53109, 53110, 53111, 53112, 53113, 53114, 53115, 53116, 53117, 53118, 53119, 53120, 53121, 53122, 53123, 53124, 53125, 53126, 53127, 53128, 53129, 53130, 53131, 53132, 53133, 53134, 53135, 53136, 53137, 53138, 53139, 53140, 53141, 53142, 53143, 53144, 53145, 53146, 53147, 53148, 53149, 53150, 53151, 53152, 53153, 53154, 53155, 53156, 53157, 53158, 53159, 53160, 53161, 53162, 53163, 53164, 53165, 53166, 53167, 53168, 53169, 53170, 53171, 53172, 53173, 53174, 53175, 53176, 53177, 53178, 53179, 53180, 53181, 53182, 53183, 53184, 53185, 53186, 53187, 53188, 53189, 53190, 53191, 53192, 53193, 53194, 53195, 53196, 53197, 53198, 53199, 53200, 53201, 53202, 53203, 53204, 53205, 53206, 53207, 53208, 53209, 53210, 53211, 53212, 53213, 53214, 53215, 53216, 53217, 53218, 53219, 53220, 53221, 53222, 53223, 53224, 53225, 53226, 53227, 53228, 53229, 53230, 53231, 53232, 53233, 53234, 53235, 53236, 53237, 53238, 53239, 53240, 53241, 53242, 53243, 53244, 53245, 53246, 53247, 53248, 53249, 53250, 53251, 53252, 53253, 53254, 53255, 53256, 53257, 53258, 53259, 53260, 53261, 53262, 53263, 53264, 53265, 53266, 53267, 53268, 53269, 53270, 53271, 53272, 53273, 53274, 53275, 53276, 53277, 53278, 53279, 53280, 53281, 53282, 53283, 53284, 53285, 53286, 53287, 53288, 53289, 53290, 53291, 53292, 53293, 53294, 53295, 53296, 53297, 53298, 53299, 53300, 53301, 53302, 53303, 53304, 53305, 53306, 53307, 53308, 53309, 53310, 53311, 53312, 53313, 53314, 53315, 53316, 53317, 53318, 53319, 53320, 53321, 53322, 53323, 53324, 53325, 53326, 53327, 53328, 53329, 53330, 53331, 53332, 53333, 53334, 53335, 53336, 53337, 53338, 53339, 53340, 53341, 53342, 53343, 53344, 53345, 53346, 53347, 53348, 53349, 53350, 53351, 53352, 53353, 53354, 53355, 53356, 53357, 53358, 53359, 53360, 53361, 53362, 53363, 53364, 53365, 53366, 53367, 53368, 53369, 53370, 53371, 53372, 53373, 53374, 53375, 53376, 53377, 53378, 53379, 53380, 53381, 53382, 53383, 53384, 53385, 53386, 53387, 53388, 53389, 53390, 53391, 53392, 53393, 53394, 53395, 53396, 53397, 53398, 53399, 53400, 53401, 53402, 53403, 53404, 53405, 53406, 53407, 53408, 53409, 53410, 53411, 53412, 53413, 53414, 53415, 53416, 53417, 53418, 53419, 53420, 53421, 53422, 53423, 53424, 53425, 53426, 53427, 53428, 53429, 53430, 53431, 53432, 53433, 53434, 53435, 53436, 53437, 53438, 53439, 53440, 53441, 53442, 53443, 53444, 53445, 53446, 53447, 53448, 53449, 53450, 53451, 53452, 53453, 53454, 53455, 53456, 53457, 53458, 53459, 53460, 53461, 53462, 53463, 53464, 53465, 53466, 53467, 53468, 53469, 53470, 53471, 53472, 53473, 53474, 53475, 53476, 53477, 53478, 53479, 53480, 53481, 53482, 53483, 53484, 53485, 53486, 53487, 53488, 53489, 53490, 53491, 53492, 53493, 53494, 53495, 53496, 53497, 53498, 53499, 53500, 53501, 53502, 53503, 53504, 53505, 53506, 53507, 53508, 53509, 53510, 53511, 53512, 53513, 53514, 53515, 53516, 53517, 53518, 53519, 53520, 53521, 53522, 53523, 53524, 53525, 53526, 53527, 53528, 53529, 53530, 53531, 53532, 53533, 53534, 53535, 53536, 53537, 53538, 53539, 53540, 53541, 53542, 53543, 53544, 53545, 53546, 53547, 53548, 53549, 53550, 53551, 53552, 53553, 53554, 53555, 53556, 53557, 53558, 53559, 53560, 53561, 53562, 53563, 53564, 53565, 53566, 53567, 53568, 53569, 53570, 53571, 53572, 53573, 53574, 53575, 53576, 53577, 53578, 53579, 53580, 53581, 53582, 53583, 53584, 53585, 53586, 53587, 53588, 53589, 53590, 53591, 53592, 53593, 53594, 53595, 53596, 53597, 53598, 53599, 53600, 53601, 53602, 53603, 53604, 53605, 53606, 53607, 53608, 53609, 53610, 53611, 53612, 53613, 53614, 53615, 53616, 53617, 53618, 53619, 53620, 53621, 53622, 53623, 53624, 53625, 53626, 53627, 53628, 53629, 53630, 53631, 53632, 53633, 53634, 53635, 53636, 53637, 53638, 53639, 53640, 53641, 53642, 53643, 53644, 53645, 53646, 53647, 53648, 53649, 53650, 53651, 53652, 53653, 53654, 53655, 53656, 53657, 53658, 53659, 53660, 53661, 53662, 53663, 53664, 53665, 53666, 53667, 53668, 53669, 53670, 53671, 53672, 53673, 53674, 53675, 53676, 53677, 53678, 53679, 53680, 53681, 53682, 53683, 53684, 53685, 53686, 53687, 53688, 53689, 53690, 53691, 53692, 53693, 53694, 53695, 53696, 53697, 53698, 53699, 53700, 53701, 53702, 53703, 53704, 53705, 53706, 53707, 53708, 53709, 53710, 53711, 53712, 53713, 53714, 53715, 53716, 53717, 53718, 53719, 53720, 53721, 53722, 53723, 53724, 53725, 53726, 53727, 53728, 53729, 53730, 53731, 53732, 53733, 53734, 53735, 53736, 53737, 53738, 53739, 53740, 53741, 53742, 53743, 53744, 53745, 53746, 53747, 53748, 53749, 53750, 53751, 53752, 53753, 53754, 53755, 53756, 53757, 53758, 53759, 53760, 53761, 53762, 53763, 53764, 53765, 53766, 53767, 53768, 53769, 53770, 53771, 53772, 53773, 53774, 53775, 53776, 53777, 53778, 53779, 53780, 53781, 53782, 53783, 53784, 53785, 53786, 53787, 53788, 53789, 53790, 53791, 53792, 53793, 53794, 53795, 53796, 53797, 53798, 53799, 53800, 53801, 53802, 53803, 53804, 53805, 53806, 53807, 53808, 53809, 53810, 53811, 53812, 53813, 53814, 53815, 53816, 53817, 53818, 53819, 53820, 53821, 53822, 53823, 53824, 53825, 53826, 53827, 53828, 53829, 53830, 53831, 53832, 53833, 53834, 53835, 53836, 53837, 53838, 53839, 53840, 53841, 53842, 53843, 53844, 53845, 53846, 53847, 53848, 53849, 53850, 53851, 53852, 53853, 53854, 53855, 53856, 53857, 53858, 53859, 53860, 53861, 53862, 53863, 53864, 53865, 53866, 53867, 53868, 53869, 53870, 53871, 53872, 53873, 53874, 53875, 53876, 53877, 53878, 53879, 53880, 53881, 53882, 53883, 53884, 53885, 53886, 53887, 53888, 53889, 53890, 53891, 53892, 53893, 53894, 53895, 53896, 53897, 53898, 53899, 53900, 53901, 53902, 53903, 53904, 53905, 53906, 53907, 53908, 53909, 53910, 53911, 53912, 53913, 53914, 53915, 53916, 53917, 53918, 53919, 53920, 53921, 53922, 53923, 53924, 53925, 53926, 53927, 53928, 53929, 53930, 53931, 53932, 53933, 53934, 53935, 53936, 53937, 53938, 53939, 53940, 53941, 53942, 53943, 53944, 53945, 53946, 53947, 53948, 53949, 53950, 53951, 53952, 53953, 53954, 53955, 53956, 53957, 53958, 53959, 53960, 53961, 53962, 53963, 53964, 53965, 53966, 53967, 53968, 53969, 53970, 53971, 53972, 53973, 53974, 53975, 53976, 53977, 53978, 53979, 53980, 53981, 53982, 53983, 53984, 53985, 53986, 53987, 53988, 53989, 53990, 53991, 53992, 53993, 53994, 53995, 53996, 53997, 53998, 53999, 54000, 54001, 54002, 54003, 54004, 54005, 54006, 54007, 54008, 54009, 54010, 54011, 54012, 54013, 54014, 54015, 54016, 54017, 54018, 54019, 54020, 54021, 54022, 54023, 54024, 54025, 54026, 54027, 54028, 54029, 54030, 54031, 54032, 54033, 54034, 54035, 54036, 54037, 54038, 54039, 54040, 54041, 54042, 54043, 54044, 54045, 54046, 54047, 54048, 54049, 54050, 54051, 54052, 54053, 54054, 54055, 54056, 54057, 54058, 54059, 54060, 54061, 54062, 54063, 54064, 54065, 54066, 54067, 54068, 54069, 54070, 54071, 54072, 54073, 54074, 54075, 54076, 54077, 54078, 54079, 54080, 54081, 54082, 54083, 54084, 54085, 54086, 54087, 54088, 54089, 54090, 54091, 54092, 54093, 54094, 54095, 54096, 54097, 54098, 54099, 54100, 54101, 54102, 54103, 54104, 54105, 54106, 54107, 54108, 54109, 54110, 54111, 54112, 54113, 54114, 54115, 54116, 54117, 54118, 54119, 54120, 54121, 54122, 54123, 54124, 54125, 54126, 54127, 54128, 54129, 54130, 54131, 54132, 54133, 54134, 54135, 54136, 54137, 54138, 54139, 54140, 54141, 54142, 54143, 54144, 54145, 54146, 54147, 54148, 54149, 54150, 54151, 54152, 54153, 54154, 54155, 54156, 54157, 54158, 54159, 54160, 54161, 54162, 54163, 54164, 54165, 54166, 54167, 54168, 54169, 54170, 54171, 54172, 54173, 54174, 54175, 54176, 54177, 54178, 54179, 54180, 54181, 54182, 54183, 54184, 54185, 54186, 54187, 54188, 54189, 54190, 54191, 54192, 54193, 54194, 54195, 54196, 54197, 54198, 54199, 54200, 54201, 54202, 54203, 54204, 54205, 54206, 54207, 54208, 54209, 54210, 54211, 54212, 54213, 54214, 54215, 54216, 54217, 54218, 54219, 54220, 54221, 54222, 54223, 54224, 54225, 54226, 54227, 54228, 54229, 54230, 54231, 54232, 54233, 54234, 54235, 54236, 54237, 54238, 54239, 54240, 54241, 54242, 54243, 54244, 54245, 54246, 54247, 54248, 54249, 54250, 54251, 54252, 54253, 54254, 54255, 54256, 54257, 54258, 54259, 54260, 54261, 54262, 54263, 54264, 54265, 54266, 54267, 54268, 54269, 54270, 54271, 54272, 54273, 54274, 54275, 54276, 54277, 54278, 54279, 54280, 54281, 54282, 54283, 54284, 54285, 54286, 54287, 54288, 54289, 54290, 54291, 54292, 54293, 54294, 54295, 54296, 54297, 54298, 54299, 54300, 54301, 54302, 54303, 54304, 54305, 54306, 54307, 54308, 54309, 54310, 54311, 54312, 54313, 54314, 54315, 54316, 54317, 54318, 54319, 54320, 54321, 54322, 54323, 54324, 54325, 54326, 54327, 54328, 54329, 54330, 54331, 54332, 54333, 54334, 54335, 54336, 54337, 54338, 54339, 54340, 54341, 54342, 54343, 54344, 54345, 54346, 54347, 54348, 54349, 54350, 54351, 54352, 54353, 54354, 54355, 54356, 54357, 54358, 54359, 54360, 54361, 54362, 54363, 54364, 54365, 54366, 54367, 54368, 54369, 54370, 54371, 54372, 54373, 54374, 54375, 54376, 54377, 54378, 54379, 54380, 54381, 54382, 54383, 54384, 54385, 54386, 54387, 54388, 54389, 54390, 54391, 54392, 54393, 54394, 54395, 54396, 54397, 54398, 54399, 54400, 54401, 54402, 54403, 54404, 54405, 54406, 54407, 54408, 54409, 54410, 54411, 54412, 54413, 54414, 54415, 54416, 54417, 54418, 54419, 54420, 54421, 54422, 54423, 54424, 54425, 54426, 54427, 54428, 54429, 54430, 54431, 54432, 54433, 54434, 54435, 54436, 54437, 54438, 54439, 54440, 54441, 54442, 54443, 54444, 54445, 54446, 54447, 54448, 54449, 54450, 54451, 54452, 54453, 54454, 54455, 54456, 54457, 54458, 54459, 54460, 54461, 54462, 54463, 54464, 54465, 54466, 54467, 54468, 54469, 54470, 54471, 54472, 54473, 54474, 54475, 54476, 54477, 54478, 54479, 54480, 54481, 54482, 54483, 54484, 54485, 54486, 54487, 54488, 54489, 54490, 54491, 54492, 54493, 54494, 54495, 54496, 54497, 54498, 54499, 54500, 54501, 54502, 54503, 54504, 54505, 54506, 54507, 54508, 54509, 54510, 54511, 54512, 54513, 54514, 54515, 54516, 54517, 54518, 54519, 54520, 54521, 54522, 54523, 54524, 54525, 54526, 54527, 54528, 54529, 54530, 54531, 54532, 54533, 54534, 54535, 54536, 54537, 54538, 54539, 54540, 54541, 54542, 54543, 54544, 54545, 54546, 54547, 54548, 54549, 54550, 54551, 54552, 54553, 54554, 54555, 54556, 54557, 54558, 54559, 54560, 54561, 54562, 54563, 54564, 54565, 54566, 54567, 54568, 54569, 54570, 54571, 54572, 54573, 54574, 54575, 54576, 54577, 54578, 54579, 54580, 54581, 54582, 54583, 54584, 54585, 54586, 54587, 54588, 54589, 54590, 54591, 54592, 54593, 54594, 54595, 54596, 54597, 54598, 54599, 54600, 54601, 54602, 54603, 54604, 54605, 54606, 54607, 54608, 54609, 54610, 54611, 54612, 54613, 54614, 54615, 54616, 54617, 54618, 54619, 54620, 54621, 54622, 54623, 54624, 54625, 54626, 54627, 54628, 54629, 54630, 54631, 54632, 54633, 54634, 54635, 54636, 54637, 54638, 54639, 54640, 54641, 54642, 54643, 54644, 54645, 54646, 54647, 54648, 54649, 54650, 54651, 54652, 54653, 54654, 54655, 54656, 54657, 54658, 54659, 54660, 54661, 54662, 54663, 54664, 54665, 54666, 54667, 54668, 54669, 54670, 54671, 54672, 54673, 54674, 54675, 54676, 54677, 54678, 54679, 54680, 54681, 54682, 54683, 54684, 54685, 54686, 54687, 54688, 54689, 54690, 54691, 54692, 54693, 54694, 54695, 54696, 54697, 54698, 54699, 54700, 54701, 54702, 54703, 54704, 54705, 54706, 54707, 54708, 54709, 54710, 54711, 54712, 54713, 54714, 54715, 54716, 54717, 54718, 54719, 54720, 54721, 54722, 54723, 54724, 54725, 54726, 54727, 54728, 54729, 54730, 54731, 54732, 54733, 54734, 54735, 54736, 54737, 54738, 54739, 54740, 54741, 54742, 54743, 54744, 54745, 54746, 54747, 54748, 54749, 54750, 54751, 54752, 54753, 54754, 54755, 54756, 54757, 54758, 54759, 54760, 54761, 54762, 54763, 54764, 54765, 54766, 54767, 54768, 54769, 54770, 54771, 54772, 54773, 54774, 54775, 54776, 54777, 54778, 54779, 54780, 54781, 54782, 54783, 54784, 54785, 54786, 54787, 54788, 54789, 54790, 54791, 54792, 54793, 54794, 54795, 54796, 54797, 54798, 54799, 54800, 54801, 54802, 54803, 54804, 54805, 54806, 54807, 54808, 54809, 54810, 54811, 54812, 54813, 54814, 54815, 54816, 54817, 54818, 54819, 54820, 54821, 54822, 54823, 54824, 54825, 54826, 54827, 54828, 54829, 54830, 54831, 54832, 54833, 54834, 54835, 54836, 54837, 54838, 54839, 54840, 54841, 54842, 54843, 54844, 54845, 54846, 54847, 54848, 54849, 54850, 54851, 54852, 54853, 54854, 54855, 54856, 54857, 54858, 54859, 54860, 54861, 54862, 54863, 54864, 54865, 54866, 54867, 54868, 54869, 54870, 54871, 54872, 54873, 54874, 54875, 54876, 54877, 54878, 54879, 54880, 54881, 54882, 54883, 54884, 54885, 54886, 54887, 54888, 54889, 54890, 54891, 54892, 54893, 54894, 54895, 54896, 54897, 54898, 54899, 54900, 54901, 54902, 54903, 54904, 54905, 54906, 54907, 54908, 54909, 54910, 54911, 54912, 54913, 54914, 54915, 54916, 54917, 54918, 54919, 54920, 54921, 54922, 54923, 54924, 54925, 54926, 54927, 54928, 54929, 54930, 54931, 54932, 54933, 54934, 54935, 54936, 54937, 54938, 54939, 54940, 54941, 54942, 54943, 54944, 54945, 54946, 54947, 54948, 54949, 54950, 54951, 54952, 54953, 54954, 54955, 54956, 54957, 54958, 54959, 54960, 54961, 54962, 54963, 54964, 54965, 54966, 54967, 54968, 54969, 54970, 54971, 54972, 54973, 54974, 54975, 54976, 54977, 54978, 54979, 54980, 54981, 54982, 54983, 54984, 54985, 54986, 54987, 54988, 54989, 54990, 54991, 54992, 54993, 54994, 54995, 54996, 54997, 54998, 54999]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset CocoDetection\n",
              "    Number of datapoints: 5000\n",
              "    Root location: /content/drive/My Drive/dacon_cv/test_dirty_mnist"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Nwp4_DKqd7m",
        "outputId": "0f5f4734-97ae-4d52-825d-5beed5215daf"
      },
      "source": [
        "%cd ../\n",
        "!ls"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/dacon_cv\n",
            "'제_2회 컴퓨터 비전 학습 경진대회 베이스라인 코드.ipynb'\n",
            " ASL\n",
            " converT2coco.ipynb\n",
            " data.zip\n",
            " dirty_mnist\n",
            " dirty_mnist_2nd_answer.csv\n",
            " dirty_mnist_2nd.zip\n",
            " dirty_mnist_answer.csv\n",
            " dirty_mnist.zip\n",
            " mnist_data\n",
            " sample_submission.csv\n",
            " test_dirty_mnist\n",
            " test_dirty_mnist_2nd.zip\n",
            " test.json\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iwIgdUbcu7ED"
      },
      "source": [
        "from random import randint\n",
        "\n",
        "def csv_to_coco(df, json_name):\n",
        "    dic = {\n",
        "        \"info\": {\n",
        "            \"description\": \"DACON DIRTY MNIST DATASET\",\n",
        "            \"url\": \"https://oranz.tistory.com/\",\n",
        "            \"version\": \"1.0\",\n",
        "            \"year\": 2021,\n",
        "            \"contributor\": \"DACON\",\n",
        "            \"date_created\": \"2021/02/18\"\n",
        "        },\n",
        "        \"licenses\": [{\n",
        "            \"url\": \"http://creativecommons.org/licenses/by-nc-sa/2.0/\",\n",
        "            \"id\": 1,\n",
        "            \"name\": \"Attribution-NonCommercial-ShareAlike License\"\n",
        "        }],\n",
        "        \"images\": [],\n",
        "        \"annotations\": [],\n",
        "        \"categories\": []\n",
        "    }\n",
        "    \n",
        "    for i in range(1, 27):\n",
        "        dic[\"categories\"].append({\"supercategory\": \"alphabet\", \"id\": i, \"name\": chr(i + 96)})\n",
        "        \n",
        "    anno_id = 1\n",
        "    for r in range(len(df)):\n",
        "        image_id = int(df.iloc[r, 0]) # cast to int for JSON serialization\n",
        "        name = str(image_id)\n",
        "        name = '0' * (5 - len(name)) + name + '.png'\n",
        "        dic[\"images\"].append({\n",
        "            \"license\": 1,\n",
        "            \"file_name\": name,\n",
        "            \"coco_url\": \"\",\n",
        "            \"height\": 256,\n",
        "            \"width\": 256,\n",
        "            \"date_captured\": \"2020-05-19 23:03:57\",\n",
        "            \"flickr_url\": \"\",\n",
        "            \"id\": image_id\n",
        "        })\n",
        "        \n",
        "        dic[\"annotations\"].append({\n",
        "            \"segmentation\": [],\n",
        "            \"area\": 232,\n",
        "            \"iscrowd\": 0,\n",
        "            \"image_id\": image_id,\n",
        "            \"bbox\": [1, 5, 4, 9],\n",
        "            \"category_id\": randint(1, 26),\n",
        "            \"id\": anno_id\n",
        "        })\n",
        "        anno_id += 1\n",
        "    with open(json_name, \"w\") as json_file:\n",
        "        json.dump(dic, json_file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2p-4MpmNwH2n"
      },
      "source": [
        "csv_to_coco(pd.read_csv('sample_submission.csv'), 'test.json')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CWdpEscswN57",
        "outputId": "3ed53ba6-a913-4a91-d7d0-e0bc56d50225"
      },
      "source": [
        "%cd ASL"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/dacon_cv/ASL\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eL0egld5wojh"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}