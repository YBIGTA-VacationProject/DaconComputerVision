{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "dacon__se_resnet_18 (일단 실행)+alpha.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "UGH0wdO6SAdF",
        "y6_d3o1xYhbF",
        "1Wa4YRMFYj_R",
        "mqz-utSWYnPG",
        "qVxFC4uLYre-",
        "xHFbiaB8Ywv1",
        "LnGoJOJtO6Go",
        "brDnIyvDY3K5",
        "OSvHdnCPjqwV"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "99211bc1dc08465e9475b3df03e42a26": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_6a8b72c204d64a9aba11363d1e0b6713",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_0181676b436c4a028ccf4aeaa14b7687",
              "IPY_MODEL_67b38fa9008a48798b7eb042f6ab5fe5"
            ]
          }
        },
        "6a8b72c204d64a9aba11363d1e0b6713": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "0181676b436c4a028ccf4aeaa14b7687": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_27c1be520e0c43f69ed0c8dd14e3848a",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 9218294,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 9218294,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_f07f551fbd954ddcb867f63d8974fad4"
          }
        },
        "67b38fa9008a48798b7eb042f6ab5fe5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_a69a6c648ee3470aa7d11a6251e1bf15",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 8.79M/8.79M [00:03&lt;00:00, 2.48MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_707a90ccee714b3d8b418aa2d4c53ca7"
          }
        },
        "27c1be520e0c43f69ed0c8dd14e3848a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "f07f551fbd954ddcb867f63d8974fad4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "a69a6c648ee3470aa7d11a6251e1bf15": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "707a90ccee714b3d8b418aa2d4c53ca7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UGH0wdO6SAdF"
      },
      "source": [
        "# 모델 선언\n",
        "model = se_resnet50() \n",
        "model.to(device)# gpu에 모델 할당 \n",
        "\n",
        "from torchsummary import summary \n",
        "\n",
        "summary(model, (3,256,256)) \n",
        "Total params: 28,071,976 \n",
        "\n",
        "model = se_resnet101() \n",
        "Total params: 49,292,328 \n",
        "\n",
        "model = se_resnet152() \n",
        "Total params: 66,770,984 \n",
        "\n",
        "model = se_resnet50() \n",
        "model.to(device)# gpu에 모델 할당\n",
        "\n",
        "from torchsummary import summary  \n",
        "\n",
        "summary(model, (1,256,256))\n",
        "Total params: 28,065,704"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y6_d3o1xYhbF"
      },
      "source": [
        "# import lib"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iXajYlx6Iudc"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "from tqdm import tqdm\n",
        "import imutils\n",
        "import zipfile\n",
        "import os\n",
        "from PIL import Image\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision.models as models\n",
        "import torchvision.transforms as T\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") # 디바이스 설정"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Wa4YRMFYj_R"
      },
      "source": [
        "# colab and data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9K4lZIYKLsfw",
        "outputId": "2ff3ee12-fc36-42f9-d63e-1eba2906a2e0"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hnMTh3h8LsT0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5131fb9b-bc04-4e4b-c220-df6ab2c81e91"
      },
      "source": [
        "from google.colab import output\n",
        "# !cp 파일1 파일2 # 파일1을 파일2로 복사 붙여넣기\n",
        "!cp -r \"/content/drive/MyDrive/21-1/data\" \"data\""
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cp: cannot stat '/content/drive/MyDrive/21-1/data/test_dirty_mnist_2nd': No such file or directory\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IrKMYlqRstNC"
      },
      "source": [
        "os.chdir('/content/data')"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6O0MJay0LsIS"
      },
      "source": [
        "from google.colab import output\n",
        "# 현재 디렉터리에 dirty_mnist라는 폴더 생성\n",
        "!mkdir \"./dirty_mnist_2nd\"\n",
        "#dirty_mnist.zip라는 zip파일을 dirty_mnist라는 폴더에 압축 풀기\n",
        "!unzip \"dirty_mnist_2nd.zip\" -d \"./dirty_mnist_2nd/\"\n",
        "# 현재 디렉터리에 test_dirty_mnist라는 폴더 생성\n",
        "!mkdir \"./test_dirty_mnist_2nd\"\n",
        "#test_dirty_mnist.zip라는 zip파일을 test_dirty_mnist라는 폴더에 압축 풀기\n",
        "!unzip \"test_dirty_mnist_2nd.zip\" -d \"./test_dirty_mnist_2nd/\"\n",
        "# 출력 결과 지우기\n",
        "output.clear()"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mqz-utSWYnPG"
      },
      "source": [
        "# 처음"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Je2ySK1kMOM2"
      },
      "source": [
        "dirty_mnist_answer = pd.read_csv(\"dirty_mnist_2nd_answer.csv\")\n",
        "# dirty_mnist라는 디렉터리 속에 들어있는 파일들의 이름을 \n",
        "# namelist라는 변수에 저장\n",
        "namelist = os.listdir('./dirty_mnist_2nd/')\n",
        "\n",
        "# unmpy를 tensor로 변환하는 ToTensor 정의\n",
        "class ToTensor(object):\n",
        "    \"\"\"numpy array를 tensor(torch)로 변환합니다.\"\"\"\n",
        "    def __call__(self, sample):\n",
        "        image, label = sample['image'], sample['label']\n",
        "        # swap color axis because\n",
        "        # numpy image: H x W x C\n",
        "        # torch image: C X H X W\n",
        "        image = image.transpose((2, 0, 1))\n",
        "        return {'image': torch.FloatTensor(image),\n",
        "                'label': torch.FloatTensor(label)}\n",
        "# to_tensor 선언\n",
        "to_tensor = T.Compose([\n",
        "                        ToTensor()\n",
        "                    ])\n",
        "\n",
        "class DatasetMNIST(torch.utils.data.Dataset):\n",
        "    def __init__(self,\n",
        "                 dir_path,\n",
        "                 meta_df,\n",
        "                 transforms=to_tensor,#미리 선언한 to_tensor를 transforms로 받음\n",
        "                 augmentations=None):\n",
        "        \n",
        "        self.dir_path = dir_path # 데이터의 이미지가 저장된 디렉터리 경로\n",
        "        self.meta_df = meta_df # 데이터의 인덱스와 정답지가 들어있는 DataFrame\n",
        "\n",
        "        self.transforms = transforms# Transform\n",
        "        self.augmentations = augmentations # Augmentation\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.meta_df)\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        # 폴더 경로 + 이미지 이름 + .png => 파일의 경로\n",
        "        # 참고) \"12\".zfill(5) => 000012\n",
        "        #       \"146\".zfill(5) => 000145\n",
        "        # cv2.IMREAD_GRAYSCALE : png파일을 채널이 1개인 GRAYSCALE로 읽음\n",
        "        \n",
        "        image = cv2.imread(self.dir_path +\\\n",
        "                           str(self.meta_df.iloc[index,0]).zfill(5) + '.png',\n",
        "                           cv2.IMREAD_GRAYSCALE)\n",
        "\n",
        "        # 0 ~ 255의 값을 갖고 크기가 (256,256)인 numpy array를\n",
        "        # 0 ~ 1 사이의 실수를 갖고 크기가 (256,256,1)인 numpy array로 변환\n",
        "        image = (image/255).astype('float')[..., np.newaxis]\n",
        "\n",
        "        # 정답 numpy array생성(존재하면 1 없으면 0)\n",
        "        label = self.meta_df.iloc[index, 1:].values.astype('float')\n",
        "        sample = {'image': image, 'label': label}\n",
        "\n",
        "        # transform 적용\n",
        "        # numpy to tensor\n",
        "        if self.transforms:\n",
        "            sample = self.transforms(sample)\n",
        "\n",
        "        # sample 반환\n",
        "        return sample"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BtPFpZxjKaHl"
      },
      "source": [
        "# 이어서"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "US1cFwxBKk0x"
      },
      "source": [
        "import torch.nn as nn\n",
        "from torch.hub import load_state_dict_from_url\n",
        "from torchvision.models import ResNet\n",
        "\n",
        "class SELayer(nn.Module):\n",
        "    def __init__(self, channel, reduction=16):\n",
        "        super(SELayer, self).__init__()\n",
        "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(channel, channel // reduction, bias=False),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(channel // reduction, channel, bias=False),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, c, _, _ = x.size()\n",
        "        y = self.avg_pool(x).view(b, c)\n",
        "        y = self.fc(y).view(b, c, 1, 1)\n",
        "        return x * y.expand_as(x)\n",
        "\n",
        "def conv3x3(in_planes, out_planes, stride=1):\n",
        "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "\n",
        "\n",
        "class SEBasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, inplanes, planes, stride=1, downsample=None, groups=1,\n",
        "                 base_width=64, dilation=1, norm_layer=None,\n",
        "                 *, reduction=16):\n",
        "        super(SEBasicBlock, self).__init__()\n",
        "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.conv2 = conv3x3(planes, planes, 1)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "        self.se = SELayer(planes, reduction)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "        out = self.se(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            residual = self.downsample(x)\n",
        "\n",
        "        out += residual\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "def se_resnet18(num_classes=1_000):\n",
        "    model = ResNet(SEBasicBlock, [2, 2, 2, 2], num_classes=num_classes)\n",
        "    model.avgpool = nn.AdaptiveAvgPool2d(1)\n",
        "    return model\n",
        "\n",
        "class se_resnet_18(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(se_resnet_18, self).__init__()\n",
        "        self.conv2d = nn.Conv2d(1, 3, 3, stride=1)\n",
        "        self.se_resnet18 =se_resnet18()\n",
        "        self.FC = nn.Linear(1000, 26)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv2d(x))\n",
        "        x = F.relu(self.se_resnet18(x))\n",
        "        x = torch.sigmoid(self.FC(x))\n",
        "        return x"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pYwY2Uy1KkT3"
      },
      "source": [
        "# cross validation을 적용하기 위해 KFold 생성\n",
        "from sklearn.model_selection import KFold\n",
        "kfold = KFold(n_splits=5, shuffle=True, random_state=0)\n",
        "\n",
        "# dirty_mnist_answer에서 train_idx와 val_idx를 생성\n",
        "best_models = [] # 폴드별로 가장 validation acc가 높은 모델 저장\n",
        "for fold_index, (trn_idx, val_idx) in enumerate(kfold.split(dirty_mnist_answer),1):\n",
        "    print(f'[fold: {fold_index}]')\n",
        "    # cuda cache 초기화\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    #train fold, validation fold 분할\n",
        "    train_answer = dirty_mnist_answer.iloc[trn_idx]\n",
        "    test_answer  = dirty_mnist_answer.iloc[val_idx]\n",
        "\n",
        "    #Dataset 정의\n",
        "    train_dataset = DatasetMNIST(\"dirty_mnist_2nd/\", train_answer)\n",
        "    valid_dataset = DatasetMNIST(\"dirty_mnist_2nd/\", test_answer)\n",
        "\n",
        "    #DataLoader 정의\n",
        "    train_data_loader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size = 128,\n",
        "        shuffle = False,\n",
        "        num_workers = 3\n",
        "    )\n",
        "\n",
        "    valid_data_loader = DataLoader(\n",
        "        valid_dataset,\n",
        "        batch_size = 32,\n",
        "        shuffle = False,\n",
        "        num_workers = 3\n",
        "    )\n",
        "\n",
        "    # 모델 선언\n",
        "    #model = torch.hub.load('moskomule/senet.pytorch','se_resnet20',num_classes=10)\n",
        "    #model = SENet18()\n",
        "    model = se_resnet_18()\n",
        "    #model = hub_model\n",
        "    #model = CifarSEResNet(CifarSEBasicBlock, 3)\n",
        "    #model = shufflenet_v2_x1_0()\n",
        "    model.to(device)# gpu에 모델 할당\n",
        "\n",
        "    # 훈련 옵션 설정\n",
        "    optimizer = torch.optim.Adam(model.parameters(),\n",
        "                                lr = 0.001)\n",
        "    lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n",
        "                                                step_size = 5,\n",
        "                                                gamma = 0.75)\n",
        "    criterion = torch.nn.BCELoss()\n",
        "\n",
        "\n",
        "    # 훈련 시작\n",
        "    valid_acc_max = 0\n",
        "    for epoch in range(10):\n",
        "        # 1개 epoch 훈련\n",
        "        train_acc_list = []\n",
        "\n",
        "        with tqdm(train_data_loader,#train_data_loader를 iterative하게 반환\n",
        "                total=train_data_loader.__len__(), # train_data_loader의 크기\n",
        "                unit=\"batch\") as train_bar:# 한번 반환하는 smaple의 단위는 \"batch\"\n",
        "            for sample in train_bar:\n",
        "                train_bar.set_description(f\"Train Epoch {epoch}\")\n",
        "\n",
        "                # 갱신할 변수들에 대한 모든 변화도를 0으로 초기화\n",
        "                # 참고)https://tutorials.pytorch.kr/beginner/pytorch_with_examples.html\n",
        "                optimizer.zero_grad()\n",
        "                images, labels = sample['image'], sample['label']\n",
        "                # tensor를 gpu에 올리기 \n",
        "                images = images.to(device)\n",
        "                labels = labels.to(device)\n",
        "               \n",
        "                # 모델의 dropoupt, batchnormalization를 train 모드로 설정\n",
        "                model.train()\n",
        "                # .forward()에서 중간 노드의 gradient를 계산\n",
        "                with torch.set_grad_enabled(True):\n",
        "                    # 모델 예측\n",
        "                    probs = model(images)\n",
        "                    print(probs)\n",
        "                    # loss 계산\n",
        "                    loss = criterion(probs, labels)\n",
        "                    # 중간 노드의 gradient로\n",
        "                    # backpropagation을 적용하여\n",
        "                    # gradient 계산\n",
        "                    loss.backward()\n",
        "                    # weight 갱신\n",
        "                    optimizer.step()\n",
        "\n",
        "                    # train accuracy 계산\n",
        "                    probs  = probs.cpu().detach().numpy()\n",
        "                    labels = labels.cpu().detach().numpy()\n",
        "                    preds = probs > 0.5\n",
        "                    batch_acc = (labels == preds).mean()\n",
        "                    train_acc_list.append(batch_acc)\n",
        "                    train_acc = np.mean(train_acc_list)\n",
        "\n",
        "                # 현재 progress bar에 현재 미니배치의 loss 결과 출력\n",
        "                train_bar.set_postfix(train_loss= loss.item(),\n",
        "                                      train_acc = train_acc)\n",
        "                \n",
        "\n",
        "        # 1개 epoch학습 후 Validation 점수 계산\n",
        "        valid_acc_list = []\n",
        "        with tqdm(valid_data_loader,\n",
        "                total=valid_data_loader.__len__(),\n",
        "                unit=\"batch\") as valid_bar:\n",
        "            for sample in valid_bar:\n",
        "                valid_bar.set_description(f\"Valid Epoch {epoch}\")\n",
        "                optimizer.zero_grad()\n",
        "                images, labels = sample['image'], sample['label']\n",
        "                images = images.to(device)\n",
        "                labels = labels.to(device)\n",
        "\n",
        "                # 모델의 dropoupt, batchnormalization를 eval모드로 설정\n",
        "                model.eval()\n",
        "                # .forward()에서 중간 노드의 gradient를 계산\n",
        "                with torch.no_grad():\n",
        "                    # validation loss만을 계산\n",
        "                    probs  = model(images)\n",
        "                    valid_loss = criterion(probs, labels)\n",
        "\n",
        "                    # train accuracy 계산\n",
        "                    probs  = probs.cpu().detach().numpy()\n",
        "                    labels = labels.cpu().detach().numpy()\n",
        "                    preds = probs > 0.5\n",
        "                    batch_acc = (labels == preds).mean()\n",
        "                    valid_acc_list.append(batch_acc)\n",
        "\n",
        "                valid_acc = np.mean(valid_acc_list)\n",
        "                valid_bar.set_postfix(valid_loss = valid_loss.item(),\n",
        "                                      valid_acc = valid_acc)\n",
        "            \n",
        "        # Learning rate 조절\n",
        "        lr_scheduler.step()\n",
        "\n",
        "        # 모델 저장\n",
        "        if valid_acc_max < valid_acc:\n",
        "            valid_acc_max = valid_acc\n",
        "            best_model = model\n",
        "            MODEL = \"se_resnet_18\"\n",
        "            # 모델을 저장할 구글 드라이브 경로\n",
        "            path = \"/content/drive/MyDrive/21-1/models_3/\"\n",
        "            torch.save(best_model, f'{path}{fold_index}_{MODEL}_{valid_loss.item():2.4f}_epoch_{epoch}.pth')\n",
        "\n",
        "    # 폴드별로 가장 좋은 모델 저장\n",
        "    best_models.append(best_model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ePnIp-9c6A_W",
        "outputId": "6177f157-965e-4ad8-c70d-067e79574cd9"
      },
      "source": [
        "# cross validation을 적용하기 위해 KFold 생성\n",
        "from sklearn.model_selection import KFold\n",
        "kfold = KFold(n_splits=5, shuffle=True, random_state=0)\n",
        "\n",
        "# dirty_mnist_answer에서 train_idx와 val_idx를 생성\n",
        "best_models = [] # 폴드별로 가장 validation acc가 높은 모델 저장\n",
        "for fold_index, (trn_idx, val_idx) in enumerate(kfold.split(dirty_mnist_answer),1):\n",
        "    print(f'[fold: {fold_index}]')\n",
        "    # cuda cache 초기화\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    #train fold, validation fold 분할\n",
        "    train_answer = dirty_mnist_answer.iloc[trn_idx]\n",
        "    test_answer  = dirty_mnist_answer.iloc[val_idx]\n",
        "\n",
        "    #Dataset 정의\n",
        "    train_dataset = DatasetMNIST(\"dirty_mnist_2nd/\", train_answer)\n",
        "    valid_dataset = DatasetMNIST(\"dirty_mnist_2nd/\", test_answer)\n",
        "\n",
        "    #DataLoader 정의\n",
        "    train_data_loader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size = 128,\n",
        "        shuffle = False,\n",
        "        num_workers = 3\n",
        "    )\n",
        "\n",
        "    valid_data_loader = DataLoader(\n",
        "        valid_dataset,\n",
        "        batch_size = 32,\n",
        "        shuffle = False,\n",
        "        num_workers = 3\n",
        "    )\n",
        "\n",
        "    # 모델 선언\n",
        "    #model = torch.hub.load('moskomule/senet.pytorch','se_resnet20',num_classes=10)\n",
        "    #model = SENet18()\n",
        "    model = se_resnet_18()\n",
        "    #model = hub_model\n",
        "    #model = CifarSEResNet(CifarSEBasicBlock, 3)\n",
        "    #model = shufflenet_v2_x1_0()\n",
        "    model.to(device)# gpu에 모델 할당\n",
        "\n",
        "    # 훈련 옵션 설정\n",
        "    optimizer = torch.optim.Adam(model.parameters(),\n",
        "                                lr = 0.001)\n",
        "    lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n",
        "                                                step_size = 5,\n",
        "                                                gamma = 0.75)\n",
        "    criterion = torch.nn.BCELoss()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[fold: 1]\n",
            "[fold: 2]\n",
            "[fold: 3]\n",
            "[fold: 4]\n",
            "[fold: 5]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wZAiNf83LlRr",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 266
        },
        "outputId": "ba5bde7d-75ef-444c-eee7-9d3f695a1181"
      },
      "source": [
        "# gpu에 올라가 있는 tensor -> cpu로 이동 -> numpy array로 변환\n",
        "sample_images = images.cpu().detach().numpy()\n",
        "sample_prob = probs\n",
        "sample_labels = labels\n",
        "\n",
        "idx = 1\n",
        "plt.imshow(sample_images[idx][0])\n",
        "plt.title(\"sample input image\")\n",
        "plt.show()\n",
        "\n",
        "print('예측값 : ',dirty_mnist_answer.columns[1:][sample_prob[idx] > 0.5])\n",
        "print('정답값 : ', dirty_mnist_answer.columns[1:][sample_labels[idx] > 0.5])"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-4119dfaadc0d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# gpu에 올라가 있는 tensor -> cpu로 이동 -> numpy array로 변환\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0msample_images\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0msample_prob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprobs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0msample_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'images' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DOjeefr6Ll2K"
      },
      "source": [
        "#test Dataset 정의\n",
        "sample_submission = pd.read_csv(\"sample_submission.csv\")\n",
        "test_dataset = DatasetMNIST(\"test_dirty_mnist_2nd/\", sample_submission)\n",
        "batch_size = 128\n",
        "test_data_loader = DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size = batch_size,\n",
        "    shuffle = False,\n",
        "    num_workers = 3,\n",
        "    drop_last = False\n",
        ")"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t7CpopNk7ljD"
      },
      "source": [
        "predictions_list = []\n",
        "# 배치 단위로 추론\n",
        "prediction_df = pd.read_csv(\"sample_submission.csv\")\n",
        "\n",
        "# 5개의 fold마다 가장 좋은 모델을 이용하여 예측\n",
        "model = torch.load('/content/drive/MyDrive/21-1/models_1/1_se_resnet_18_0.6523_epoch_5.pth')\n",
        "best_models.append(model)\n",
        "model = torch.load('/content/drive/MyDrive/21-1/models_1/2_se_resnet_18_0.6780_epoch_6.pth')\n",
        "best_models.append(model)\n",
        "model = torch.load('/content/drive/MyDrive/21-1/models_1/3_se_resnet_18_0.6826_epoch_6.pth')\n",
        "best_models.append(model)\n",
        "model = torch.load('/content/drive/MyDrive/21-1/models_1/4_se_resnet_18_0.6721_epoch_3.pth')\n",
        "best_models.append(model)\n",
        "model = torch.load('/content/drive/MyDrive/21-1/models_1/5_se_resnet_18_0.6662_epoch_4.pth')\n",
        "best_models.append(model)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M9kS9rcJ72We"
      },
      "source": [
        "best_models"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XmOVy0dDLxow",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 409
        },
        "outputId": "29e49048-6be6-40ad-ac47-0b09e4499471"
      },
      "source": [
        "predictions_list = []\n",
        "# 배치 단위로 추론\n",
        "prediction_df = pd.read_csv(\"sample_submission.csv\")\n",
        "\n",
        "# 5개의 fold마다 가장 좋은 모델을 이용하여 예측\n",
        "\n",
        "for model in best_models:\n",
        "    # 0으로 채워진 array 생성\n",
        "    prediction_array = np.zeros([prediction_df.shape[0],\n",
        "                                 prediction_df.shape[1] -1])\n",
        "    for idx, sample in enumerate(test_data_loader):\n",
        "        print(idx)\n",
        "        with torch.no_grad():\n",
        "            # 추론\n",
        "            model.eval()\n",
        "            images = sample['image']\n",
        "            images = images.to(device)\n",
        "            probs  = model(images)\n",
        "            probs = probs.cpu().detach().numpy()\n",
        "            preds = (probs > 0.5)\n",
        "\n",
        "            # 예측 결과를 \n",
        "            # prediction_array에 입력\n",
        "            batch_index = batch_size * idx\n",
        "            prediction_array[batch_index: batch_index + images.shape[0],:]\\\n",
        "                         = preds.astype(int)\n",
        "                         \n",
        "    # 채널을 하나 추가하여 list에 append\n",
        "    predictions_list.append(prediction_array[...,np.newaxis])"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-cb94addd75b4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m     prediction_array = np.zeros([prediction_df.shape[0],\n\u001b[1;32m     10\u001b[0m                                  prediction_df.shape[1] -1])\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_data_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    433\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 435\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    436\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1066\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1067\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shutdown\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1068\u001b[0;31m             \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1069\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1070\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1032\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1033\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1034\u001b[0;31m                 \u001b[0msuccess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1035\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1036\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    870\u001b[0m         \u001b[0;31m#   (bool: whether successfully get data, any: data if successful else None)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 872\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    873\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    874\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/multiprocessing/queues.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    102\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mblock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m                     \u001b[0mtimeout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeadline\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmonotonic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m                         \u001b[0;32mraise\u001b[0m \u001b[0mEmpty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/multiprocessing/connection.py\u001b[0m in \u001b[0;36mpoll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    255\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_readable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 257\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__enter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/multiprocessing/connection.py\u001b[0m in \u001b[0;36m_poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    412\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    413\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 414\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    415\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    416\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/multiprocessing/connection.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(object_list, timeout)\u001b[0m\n\u001b[1;32m    909\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    910\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 911\u001b[0;31m                 \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    912\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    913\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfileobj\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevents\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/selectors.py\u001b[0m in \u001b[0;36mselect\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    374\u001b[0m             \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    375\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 376\u001b[0;31m                 \u001b[0mfd_event_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    377\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mInterruptedError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A2rEgrSE6lug",
        "outputId": "f315c412-237b-4001-9f03-38aabdd86fe1"
      },
      "source": [
        "predictions_list"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DJUchr7jLz6Q",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "outputId": "8fda5270-820c-4e74-a5b6-5ccf1c4bf56a"
      },
      "source": [
        "# axis = 2를 기준으로 평균\n",
        "predictions_array = np.concatenate(predictions_list, axis = 2)\n",
        "predictions_mean = predictions_array.mean(axis = 2)\n",
        "\n",
        "# 평균 값이 0.5보다 클 경우 1 작으면 0\n",
        "predictions_mean = (predictions_mean > 0.5) * 1\n",
        "predictions_mean"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-824e9c353c91>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# axis = 2를 기준으로 평균\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mpredictions_array\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mpredictions_mean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredictions_array\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# 평균 값이 0.5보다 클 경우 1 작으면 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mconcatenate\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: need at least one array to concatenate"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JTrxjjyPL1mM"
      },
      "source": [
        "sample_submission = pd.read_csv(\"sample_submission.csv\")\n",
        "sample_submission.iloc[:,1:] = predictions_mean\n",
        "sample_submission.to_csv(\"se_resnet18_prediction.csv\", index = False)\n",
        "sample_submission"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ObYE8eYUMC7V"
      },
      "source": [
        "from google.colab import files\n",
        "files.download('se_resnet18_prediction.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qVxFC4uLYre-"
      },
      "source": [
        "# MultiLabelResnet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SJH-aCUXI17G"
      },
      "source": [
        "# nn.Module을 상속 받아 MultiLabelResnet를 정의\n",
        "class MultiLabelResnet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MultiLabelResnet, self).__init__()\n",
        "        self.conv2d = nn.Conv2d(1, 3, 3, stride=1)\n",
        "        self.resnet = models.resnet18() \n",
        "        self.FC = nn.Linear(1000, 26)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # resnet의 입력은 [3, N, N]으로\n",
        "        # 3개의 채널을 갖기 때문에\n",
        "        # resnet 입력 전에 conv2d를 한 층 추가\n",
        "        x = F.relu(self.conv2d(x))\n",
        "\n",
        "        # resnet18을 추가\n",
        "        x = F.relu(self.resnet(x))\n",
        "\n",
        "        # 마지막 출력에 nn.Linear를 추가\n",
        "        # multilabel을 예측해야 하기 때문에\n",
        "        # softmax가 아닌 sigmoid를 적용\n",
        "        x = torch.sigmoid(self.FC(x))\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LWvEweMWI3Zr",
        "outputId": "e383c1e7-f556-4784-b859-4b4ec18247a8"
      },
      "source": [
        "# 모델 선언\n",
        "model = MultiLabelResnet()\n",
        "model.to(device)# gpu에 모델 할당\n",
        "from torchsummary import summary\n",
        "summary(model, (1,256,256))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1          [-1, 3, 254, 254]              30\n",
            "            Conv2d-2         [-1, 64, 127, 127]           9,408\n",
            "       BatchNorm2d-3         [-1, 64, 127, 127]             128\n",
            "              ReLU-4         [-1, 64, 127, 127]               0\n",
            "         MaxPool2d-5           [-1, 64, 64, 64]               0\n",
            "            Conv2d-6           [-1, 64, 64, 64]          36,864\n",
            "       BatchNorm2d-7           [-1, 64, 64, 64]             128\n",
            "              ReLU-8           [-1, 64, 64, 64]               0\n",
            "            Conv2d-9           [-1, 64, 64, 64]          36,864\n",
            "      BatchNorm2d-10           [-1, 64, 64, 64]             128\n",
            "             ReLU-11           [-1, 64, 64, 64]               0\n",
            "       BasicBlock-12           [-1, 64, 64, 64]               0\n",
            "           Conv2d-13           [-1, 64, 64, 64]          36,864\n",
            "      BatchNorm2d-14           [-1, 64, 64, 64]             128\n",
            "             ReLU-15           [-1, 64, 64, 64]               0\n",
            "           Conv2d-16           [-1, 64, 64, 64]          36,864\n",
            "      BatchNorm2d-17           [-1, 64, 64, 64]             128\n",
            "             ReLU-18           [-1, 64, 64, 64]               0\n",
            "       BasicBlock-19           [-1, 64, 64, 64]               0\n",
            "           Conv2d-20          [-1, 128, 32, 32]          73,728\n",
            "      BatchNorm2d-21          [-1, 128, 32, 32]             256\n",
            "             ReLU-22          [-1, 128, 32, 32]               0\n",
            "           Conv2d-23          [-1, 128, 32, 32]         147,456\n",
            "      BatchNorm2d-24          [-1, 128, 32, 32]             256\n",
            "           Conv2d-25          [-1, 128, 32, 32]           8,192\n",
            "      BatchNorm2d-26          [-1, 128, 32, 32]             256\n",
            "             ReLU-27          [-1, 128, 32, 32]               0\n",
            "       BasicBlock-28          [-1, 128, 32, 32]               0\n",
            "           Conv2d-29          [-1, 128, 32, 32]         147,456\n",
            "      BatchNorm2d-30          [-1, 128, 32, 32]             256\n",
            "             ReLU-31          [-1, 128, 32, 32]               0\n",
            "           Conv2d-32          [-1, 128, 32, 32]         147,456\n",
            "      BatchNorm2d-33          [-1, 128, 32, 32]             256\n",
            "             ReLU-34          [-1, 128, 32, 32]               0\n",
            "       BasicBlock-35          [-1, 128, 32, 32]               0\n",
            "           Conv2d-36          [-1, 256, 16, 16]         294,912\n",
            "      BatchNorm2d-37          [-1, 256, 16, 16]             512\n",
            "             ReLU-38          [-1, 256, 16, 16]               0\n",
            "           Conv2d-39          [-1, 256, 16, 16]         589,824\n",
            "      BatchNorm2d-40          [-1, 256, 16, 16]             512\n",
            "           Conv2d-41          [-1, 256, 16, 16]          32,768\n",
            "      BatchNorm2d-42          [-1, 256, 16, 16]             512\n",
            "             ReLU-43          [-1, 256, 16, 16]               0\n",
            "       BasicBlock-44          [-1, 256, 16, 16]               0\n",
            "           Conv2d-45          [-1, 256, 16, 16]         589,824\n",
            "      BatchNorm2d-46          [-1, 256, 16, 16]             512\n",
            "             ReLU-47          [-1, 256, 16, 16]               0\n",
            "           Conv2d-48          [-1, 256, 16, 16]         589,824\n",
            "      BatchNorm2d-49          [-1, 256, 16, 16]             512\n",
            "             ReLU-50          [-1, 256, 16, 16]               0\n",
            "       BasicBlock-51          [-1, 256, 16, 16]               0\n",
            "           Conv2d-52            [-1, 512, 8, 8]       1,179,648\n",
            "      BatchNorm2d-53            [-1, 512, 8, 8]           1,024\n",
            "             ReLU-54            [-1, 512, 8, 8]               0\n",
            "           Conv2d-55            [-1, 512, 8, 8]       2,359,296\n",
            "      BatchNorm2d-56            [-1, 512, 8, 8]           1,024\n",
            "           Conv2d-57            [-1, 512, 8, 8]         131,072\n",
            "      BatchNorm2d-58            [-1, 512, 8, 8]           1,024\n",
            "             ReLU-59            [-1, 512, 8, 8]               0\n",
            "       BasicBlock-60            [-1, 512, 8, 8]               0\n",
            "           Conv2d-61            [-1, 512, 8, 8]       2,359,296\n",
            "      BatchNorm2d-62            [-1, 512, 8, 8]           1,024\n",
            "             ReLU-63            [-1, 512, 8, 8]               0\n",
            "           Conv2d-64            [-1, 512, 8, 8]       2,359,296\n",
            "      BatchNorm2d-65            [-1, 512, 8, 8]           1,024\n",
            "             ReLU-66            [-1, 512, 8, 8]               0\n",
            "       BasicBlock-67            [-1, 512, 8, 8]               0\n",
            "AdaptiveAvgPool2d-68            [-1, 512, 1, 1]               0\n",
            "           Linear-69                 [-1, 1000]         513,000\n",
            "           ResNet-70                 [-1, 1000]               0\n",
            "           Linear-71                   [-1, 26]          26,026\n",
            "================================================================\n",
            "Total params: 11,715,568\n",
            "Trainable params: 11,715,568\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.25\n",
            "Forward/backward pass size (MB): 83.12\n",
            "Params size (MB): 44.69\n",
            "Estimated Total Size (MB): 128.06\n",
            "----------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xHFbiaB8Ywv1"
      },
      "source": [
        "# SE-resnet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dk8X7-0ihZj-"
      },
      "source": [
        "1. [깃헙 블로그 : SENet(Squeeze and excitation networks)](https://jayhey.github.io/deep%20learning/2018/07/18/SENet/)\n",
        "2. [깃헙 블로그 : (SENet) Squeeze-and-Excitation Networks 번역 및 추가 설명과 Keras 구현](https://sike6054.github.io/blog/paper/seventh-post/)\n",
        "3. [깃헙 블로그 : Squeeze-and-Excitation Networks](https://wwiiiii.tistory.com/entry/SqueezeandExcitation-Networks)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2dyNefg4i-9y"
      },
      "source": [
        "[se_resnet](https://github.com/StickCui/PyTorch-SE-ResNet/blob/master/model/model.py)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LaC8e6jbI4Jn"
      },
      "source": [
        "import math\n",
        "class Bottleneck(nn.Module):\n",
        "    expansion = 4\n",
        "\n",
        "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
        "        super(Bottleneck, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride,\n",
        "                               padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "        self.conv3 = nn.Conv2d(planes, planes * 4, kernel_size=1, bias=False)\n",
        "        self.bn3 = nn.BatchNorm2d(planes * 4)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        # SE\n",
        "        self.global_pool = nn.AdaptiveAvgPool2d(1)\n",
        "        self.conv_down = nn.Conv2d(\n",
        "            planes * 4, planes // 4, kernel_size=1, bias=False)\n",
        "        self.conv_up = nn.Conv2d(\n",
        "            planes // 4, planes * 4, kernel_size=1, bias=False)\n",
        "        self.sig = nn.Sigmoid()\n",
        "        # Downsample\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv3(out)\n",
        "        out = self.bn3(out)\n",
        "\n",
        "        out1 = self.global_pool(out)\n",
        "        out1 = self.conv_down(out1)\n",
        "        out1 = self.relu(out1)\n",
        "        out1 = self.conv_up(out1)\n",
        "        out1 = self.sig(out1)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            residual = self.downsample(x)\n",
        "\n",
        "        res = out1 * out + residual\n",
        "        res = self.relu(res)\n",
        "\n",
        "        return res\n",
        "\n",
        "\n",
        "class SEResNet(nn.Module):\n",
        "\n",
        "    def __init__(self, block, layers, num_classes=1000):\n",
        "        self.inplanes = 128\n",
        "        super(SEResNet, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 128, kernel_size=256, stride=2, padding=3,\n",
        "                               bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(128)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "        self.layer1 = self._make_layer(block, 128, layers[0])\n",
        "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n",
        "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n",
        "        self.avgpool = nn.AvgPool2d(7)\n",
        "        self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
        "\n",
        "        #self.inplanes = 64\n",
        "        #super(SEResNet, self).__init__()\n",
        "        #self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3,\n",
        "        #                       bias=False)\n",
        "        #self.bn1 = nn.BatchNorm2d(64)\n",
        "        #self.relu = nn.ReLU(inplace=True)\n",
        "        #self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "        #self.layer1 = self._make_layer(block, 64, layers[0])\n",
        "        #self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n",
        "        #self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n",
        "        #self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n",
        "        #self.avgpool = nn.AvgPool2d(7)\n",
        "        #self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
        "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                m.weight.data.fill_(1)\n",
        "                m.bias.data.zero_()\n",
        "\n",
        "    def _make_layer(self, block, planes, blocks, stride=1):\n",
        "        downsample = None\n",
        "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
        "            downsample = nn.Sequential(\n",
        "                nn.Conv2d(self.inplanes, planes * block.expansion,\n",
        "                          kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(planes * block.expansion),\n",
        "            )\n",
        "\n",
        "        layers = []\n",
        "        layers.append(block(self.inplanes, planes, stride, downsample))\n",
        "        self.inplanes = planes * block.expansion\n",
        "        for i in range(1, blocks):\n",
        "            layers.append(block(self.inplanes, planes))\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "\n",
        "        x = self.avgpool(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "class Selayer(nn.Module):\n",
        "\n",
        "    def __init__(self, inplanes):\n",
        "        super(Selayer, self).__init__()\n",
        "        self.global_avgpool = nn.AdaptiveAvgPool2d(1)\n",
        "        self.conv1 = nn.Conv2d(inplanes, inplanes / 16, kernel_size=1, stride=1)\n",
        "        self.conv2 = nn.Conv2d(inplanes / 16, inplanes, kernel_size=1, stride=1)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        out = self.global_avgpool(x)\n",
        "\n",
        "        out = self.conv1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.sigmoid(out)\n",
        "\n",
        "        return x * out\n",
        "\n",
        "\n",
        "class BottleneckX(nn.Module):\n",
        "    expansion = 4\n",
        "\n",
        "    def __init__(self, inplanes, planes, cardinality, stride=1, downsample=None):\n",
        "        super(BottleneckX, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(inplanes, planes * 2, kernel_size=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes * 2)\n",
        "\n",
        "        self.conv2 = nn.Conv2d(planes * 2, planes * 2, kernel_size=3, stride=stride,\n",
        "                               padding=1, groups=cardinality, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes * 2)\n",
        "\n",
        "        self.conv3 = nn.Conv2d(planes * 2, planes * 4, kernel_size=1, bias=False)\n",
        "        self.bn3 = nn.BatchNorm2d(planes * 4)\n",
        "\n",
        "        self.selayer = Selayer(planes * 4)\n",
        "\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv3(out)\n",
        "        out = self.bn3(out)\n",
        "\n",
        "        out = self.selayer(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            residual = self.downsample(x)\n",
        "\n",
        "        out += residual\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YUfeKT8JJH9y"
      },
      "source": [
        "class SEResNet(nn.Module):\n",
        "\n",
        "    def __init__(self, block, layers, num_classes=1000):\n",
        "        self.inplanes = 64\n",
        "        super(SEResNet, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3,\n",
        "                               bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
        "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n",
        "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n",
        "        self.avgpool = nn.AvgPool2d(7)\n",
        "        self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
        "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                m.weight.data.fill_(1)\n",
        "                m.bias.data.zero_()\n",
        "\n",
        "    def _make_layer(self, block, planes, blocks, stride=1):\n",
        "        downsample = None\n",
        "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
        "            downsample = nn.Sequential(\n",
        "                nn.Conv2d(self.inplanes, planes * block.expansion,\n",
        "                          kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(planes * block.expansion),\n",
        "            )\n",
        "\n",
        "        layers = []\n",
        "        layers.append(block(self.inplanes, planes, stride, downsample))\n",
        "        self.inplanes = planes * block.expansion\n",
        "        for i in range(1, blocks):\n",
        "            layers.append(block(self.inplanes, planes))\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "\n",
        "        x = self.avgpool(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "def se_resnet50(**kwargs):\n",
        "    \"\"\"Constructs a SE-ResNet-50 model.\n",
        "    Args:\n",
        "        num_classes = 1000 (default)\n",
        "    \"\"\"\n",
        "    model = SEResNet(Bottleneck, [3, 4, 6, 3], **kwargs)\n",
        "    return model\n",
        "\n",
        "\n",
        "def se_resnet101(**kwargs):\n",
        "    \"\"\"Constructs a SE-ResNet-101 model.\n",
        "    Args:\n",
        "        num_classes = 1000 (default)\n",
        "    \"\"\"\n",
        "    model = SEResNet(Bottleneck, [3, 4, 23, 3], **kwargs)\n",
        "    return model\n",
        "\n",
        "\n",
        "def se_resnet152(**kwargs):\n",
        "    \"\"\"Constructs a SE-ResNet-152 model.\n",
        "    Args:\n",
        "        num_classes = 1000 (default)\n",
        "    \"\"\"\n",
        "    model = SEResNet(Bottleneck, [3, 8, 36, 3], **kwargs)\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VvCMq6scJH3z"
      },
      "source": [
        "# 모델 선언\n",
        "model = se_resnet50()\n",
        "model.to(device)# gpu에 모델 할당\n",
        "from torchsummary import summary\n",
        "summary(model, (3,256,256))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_OPImm-OJHwq"
      },
      "source": [
        "class SEResNet(nn.Module):\n",
        "\n",
        "    def __init__(self, block, layers, num_classes=1000):\n",
        "        self.inplanes = 64\n",
        "        super(SEResNet, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3,\n",
        "                               bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
        "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n",
        "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n",
        "        self.avgpool = nn.AvgPool2d(7)\n",
        "        self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
        "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                m.weight.data.fill_(1)\n",
        "                m.bias.data.zero_()\n",
        "\n",
        "    def _make_layer(self, block, planes, blocks, stride=1):\n",
        "        downsample = None\n",
        "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
        "            downsample = nn.Sequential(\n",
        "                nn.Conv2d(self.inplanes, planes * block.expansion,\n",
        "                          kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(planes * block.expansion),\n",
        "            )\n",
        "\n",
        "        layers = []\n",
        "        layers.append(block(self.inplanes, planes, stride, downsample))\n",
        "        self.inplanes = planes * block.expansion\n",
        "        for i in range(1, blocks):\n",
        "            layers.append(block(self.inplanes, planes))\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "\n",
        "        x = self.avgpool(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "def se_resnet50(**kwargs):\n",
        "    \"\"\"Constructs a SE-ResNet-50 model.\n",
        "    Args:\n",
        "        num_classes = 1000 (default)\n",
        "    \"\"\"\n",
        "    model = SEResNet(Bottleneck, [3, 4, 6, 3], **kwargs)\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZKw_b6NnJHr4"
      },
      "source": [
        "# 모델 선언\n",
        "model = se_resnet50()\n",
        "model.to(device)# gpu에 모델 할당\n",
        "from torchsummary import summary\n",
        "summary(model, (1,256,256))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FG_xn613izRx"
      },
      "source": [
        "[#####](https://github.com/moskomule/senet.pytorch/blob/23839e07525f9f5d39982140fccc8b925fe4dee9/senet/se_resnet.py#L147)\n",
        "\n",
        "\n",
        "[##### 엄마](https://github.com/hujie-frank/SENet)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dge3PhY6ivyV"
      },
      "source": [
        "import torch.nn as nn\n",
        "from torch.hub import load_state_dict_from_url\n",
        "from torchvision.models import ResNet\n",
        "#from senet.se_module import SELayer\n",
        "\n",
        "class SELayer(nn.Module):\n",
        "    def __init__(self, channel, reduction=16):\n",
        "        super(SELayer, self).__init__()\n",
        "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(channel, channel // reduction, bias=False),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(channel // reduction, channel, bias=False),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, c, _, _ = x.size()\n",
        "        y = self.avg_pool(x).view(b, c)\n",
        "        y = self.fc(y).view(b, c, 1, 1)\n",
        "        return x * y.expand_as(x)\n",
        "\n",
        "\n",
        "def conv3x3(in_planes, out_planes, stride=1):\n",
        "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "\n",
        "\n",
        "class SEBasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, inplanes, planes, stride=1, downsample=None, groups=1,\n",
        "                 base_width=64, dilation=1, norm_layer=None,\n",
        "                 *, reduction=16):\n",
        "        super(SEBasicBlock, self).__init__()\n",
        "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.conv2 = conv3x3(planes, planes, 1)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "        self.se = SELayer(planes, reduction)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "        out = self.se(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            residual = self.downsample(x)\n",
        "\n",
        "        out += residual\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class SEBottleneck(nn.Module):\n",
        "    expansion = 4\n",
        "\n",
        "    def __init__(self, inplanes, planes, stride=1, downsample=None, groups=1,\n",
        "                 base_width=64, dilation=1, norm_layer=None,\n",
        "                 *, reduction=16):\n",
        "        super(SEBottleneck, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride,\n",
        "                               padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "        self.conv3 = nn.Conv2d(planes, planes * 4, kernel_size=1, bias=False)\n",
        "        self.bn3 = nn.BatchNorm2d(planes * 4)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.se = SELayer(planes * 4, reduction)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv3(out)\n",
        "        out = self.bn3(out)\n",
        "        out = self.se(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            residual = self.downsample(x)\n",
        "\n",
        "        out += residual\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "def se_resnet18(num_classes=1_000):\n",
        "    \"\"\"Constructs a ResNet-18 model.\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "    \"\"\"\n",
        "    model = ResNet(SEBasicBlock, [2, 2, 2, 2], num_classes=num_classes)\n",
        "    model.avgpool = nn.AdaptiveAvgPool2d(1)\n",
        "    return model\n",
        "\n",
        "\n",
        "def se_resnet34(num_classes=1_000):\n",
        "    \"\"\"Constructs a ResNet-34 model.\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "    \"\"\"\n",
        "    model = ResNet(SEBasicBlock, [3, 4, 6, 3], num_classes=num_classes)\n",
        "    model.avgpool = nn.AdaptiveAvgPool2d(1)\n",
        "    return model\n",
        "\n",
        "\n",
        "def se_resnet50(num_classes=1_000, pretrained=False):\n",
        "    \"\"\"Constructs a ResNet-50 model.\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "    \"\"\"\n",
        "    model = ResNet(SEBottleneck, [3, 4, 6, 3], num_classes=num_classes)\n",
        "    model.avgpool = nn.AdaptiveAvgPool2d(1)\n",
        "    if pretrained:\n",
        "        model.load_state_dict(load_state_dict_from_url(\n",
        "            \"https://github.com/moskomule/senet.pytorch/releases/download/archive/seresnet50-60a8950a85b2b.pkl\"))\n",
        "    return model\n",
        "\n",
        "\n",
        "def se_resnet101(num_classes=1_000):\n",
        "    \"\"\"Constructs a ResNet-101 model.\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "    \"\"\"\n",
        "    model = ResNet(SEBottleneck, [3, 4, 23, 3], num_classes=num_classes)\n",
        "    model.avgpool = nn.AdaptiveAvgPool2d(1)\n",
        "    return model\n",
        "\n",
        "\n",
        "def se_resnet152(num_classes=1_000):\n",
        "    \"\"\"Constructs a ResNet-152 model.\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "    \"\"\"\n",
        "    model = ResNet(SEBottleneck, [3, 8, 36, 3], num_classes=num_classes)\n",
        "    model.avgpool = nn.AdaptiveAvgPool2d(1)\n",
        "    return model\n",
        "\n",
        "\n",
        "class CifarSEBasicBlock(nn.Module):\n",
        "    def __init__(self, inplanes, planes, stride=1, reduction=16):\n",
        "        super(CifarSEBasicBlock, self).__init__()\n",
        "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.conv2 = conv3x3(planes, planes)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "        self.se = SELayer(planes, reduction)\n",
        "        if inplanes != planes:\n",
        "            self.downsample = nn.Sequential(nn.Conv2d(inplanes, planes, kernel_size=1, stride=stride, bias=False),\n",
        "                                            nn.BatchNorm2d(planes))\n",
        "        else:\n",
        "            self.downsample = lambda x: x\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = self.downsample(x)\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "        out = self.se(out)\n",
        "\n",
        "        out += residual\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class CifarSEResNet(nn.Module):\n",
        "    def __init__(self, block, n_size, num_classes=10, reduction=16):\n",
        "        super(CifarSEResNet, self).__init__()\n",
        "        self.inplane = 16\n",
        "        self.conv1 = nn.Conv2d(\n",
        "            1, self.inplane, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(self.inplane)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.layer1 = self._make_layer(\n",
        "            block, 16, blocks=n_size, stride=1, reduction=reduction)\n",
        "        self.layer2 = self._make_layer(\n",
        "            block, 32, blocks=n_size, stride=2, reduction=reduction)\n",
        "        self.layer3 = self._make_layer(\n",
        "            block, 64, blocks=n_size, stride=2, reduction=reduction)\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d(1)\n",
        "        self.fc = nn.Linear(64, num_classes)\n",
        "        self.initialize()\n",
        "\n",
        "    def initialize(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight)\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "    def _make_layer(self, block, planes, blocks, stride, reduction):\n",
        "        strides = [stride] + [1] * (blocks - 1)\n",
        "        layers = []\n",
        "        for stride in strides:\n",
        "            layers.append(block(self.inplane, planes, stride, reduction))\n",
        "            self.inplane = planes\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "\n",
        "        x = self.avgpool(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = torch.sigmoid(self.fc(x))\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "class CifarSEPreActResNet(CifarSEResNet):\n",
        "    def __init__(self, block, n_size, num_classes=10, reduction=16):\n",
        "        super(CifarSEPreActResNet, self).__init__(\n",
        "            block, n_size, num_classes, reduction)\n",
        "        self.bn1 = nn.BatchNorm2d(self.inplane)\n",
        "        self.initialize()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "\n",
        "        x = self.avgpool(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc(x)\n",
        "\n",
        "def se_resnet20(**kwargs):\n",
        "    \"\"\"Constructs a ResNet-18 model.\n",
        "    \"\"\"\n",
        "    model = CifarSEResNet(CifarSEBasicBlock, 3, **kwargs)\n",
        "    return model\n",
        "\n",
        "\n",
        "def se_resnet32(**kwargs):\n",
        "    \"\"\"Constructs a ResNet-34 model.\n",
        "    \"\"\"\n",
        "    model = CifarSEResNet(CifarSEBasicBlock, 5, **kwargs)\n",
        "    return model\n",
        "\n",
        "\n",
        "def se_resnet56(**kwargs):\n",
        "    \"\"\"Constructs a ResNet-34 model.\n",
        "    \"\"\"\n",
        "    model = CifarSEResNet(CifarSEBasicBlock, 9, **kwargs)\n",
        "    return model\n",
        "\n",
        "\n",
        "def se_preactresnet20(**kwargs):\n",
        "    \"\"\"Constructs a ResNet-18 model.\n",
        "    \"\"\"\n",
        "    model = CifarSEPreActResNet(CifarSEBasicBlock, 3, **kwargs)\n",
        "    return model\n",
        "\n",
        "\n",
        "def se_preactresnet32(**kwargs):\n",
        "    \"\"\"Constructs a ResNet-34 model.\n",
        "    \"\"\"\n",
        "    model = CifarSEPreActResNet(CifarSEBasicBlock, 5, **kwargs)\n",
        "    return model\n",
        "\n",
        "\n",
        "def se_preactresnet56(**kwargs):\n",
        "    \"\"\"Constructs a ResNet-34 model.\n",
        "    \"\"\"\n",
        "    model = CifarSEPreActResNet(CifarSEBasicBlock, 9, **kwargs)\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LnGoJOJtO6Go"
      },
      "source": [
        "# 램 터짐"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BoanEImzJXx0"
      },
      "source": [
        "import torch.nn as nn\n",
        "from torch.hub import load_state_dict_from_url\n",
        "from torchvision.models import ResNet\n",
        "\n",
        "\n",
        "def conv3x3(in_planes, out_planes, stride=1):\n",
        "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "\n",
        "class SELayer(nn.Module):\n",
        "    def __init__(self, channel, reduction=16):\n",
        "        super(SELayer, self).__init__()\n",
        "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(channel, channel // reduction, bias=False),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(channel // reduction, channel, bias=False),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, c, _, _ = x.size()\n",
        "        y = self.avg_pool(x).view(b, c)\n",
        "        y = self.fc(y).view(b, c, 1, 1)\n",
        "        return x * y.expand_as(x)\n",
        "\n",
        "class CifarSEBasicBlock(nn.Module):\n",
        "    def __init__(self, inplanes, planes, stride=1, reduction=16):\n",
        "        super(CifarSEBasicBlock, self).__init__()\n",
        "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.conv2 = conv3x3(planes, planes)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "        self.se = SELayer(planes, reduction)\n",
        "        if inplanes != planes:\n",
        "            self.downsample = nn.Sequential(nn.Conv2d(inplanes, planes, kernel_size=1, stride=stride, bias=False),\n",
        "                                            nn.BatchNorm2d(planes))\n",
        "        else:\n",
        "            self.downsample = lambda x: x\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = self.downsample(x)\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "        out = self.se(out)\n",
        "\n",
        "        out += residual\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "class CifarSEResNet(nn.Module):\n",
        "    def __init__(self, block, n_size, num_classes=10, reduction=16):\n",
        "        super(CifarSEResNet, self).__init__()\n",
        "        self.inplane = 16\n",
        "        self.conv1 = nn.Conv2d(\n",
        "            1, self.inplane, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(self.inplane)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.layer1 = self._make_layer(\n",
        "            block, 16, blocks=n_size, stride=1, reduction=reduction)\n",
        "        self.layer2 = self._make_layer(\n",
        "            block, 32, blocks=n_size, stride=2, reduction=reduction)\n",
        "        self.layer3 = self._make_layer(\n",
        "            block, 64, blocks=n_size, stride=2, reduction=reduction)\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d(1)\n",
        "        self.fc = nn.Linear(64, num_classes)\n",
        "        self.initialize()\n",
        "\n",
        "    def initialize(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight)\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "    def _make_layer(self, block, planes, blocks, stride, reduction):\n",
        "        strides = [stride] + [1] * (blocks - 1)\n",
        "        layers = []\n",
        "        for stride in strides:\n",
        "            layers.append(block(self.inplane, planes, stride, reduction))\n",
        "            self.inplane = planes\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "\n",
        "        x = self.avgpool(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "model = CifarSEResNet(CifarSEBasicBlock, 3)\n",
        "model.to(device)# gpu에 모델 할당\n",
        "summary(model, (1,256,256))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "brDnIyvDY3K5"
      },
      "source": [
        "# Shufflenet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5U1Hw10rgoCq"
      },
      "source": [
        "model = torch.hub.load('pytorch/vision:v0.6.0', 'shufflenet_v2_x1_0', pretrained=True)\n",
        "model.eval()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SJF1rFoWgpy3"
      },
      "source": [
        "model.to(device)# gpu에 모델 할당\n",
        "summary(model, (3,256,256))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pPbm-deNZu9K"
      },
      "source": [
        "model = shufflenet_v2_x1_0()\n",
        "model.eval()\n",
        "model.to(device)# gpu에 모델 할당\n",
        "summary(model, (1,256,256))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cWuHdnarZw6q"
      },
      "source": [
        "import torch\n",
        "from torch import Tensor\n",
        "import torch.nn as nn\n",
        "#from .utils import load_state_dict_from_url\n",
        "from torch.hub import load_state_dict_from_url\n",
        "from typing import Callable, Any, List\n",
        "\n",
        "\n",
        "__all__ = [\n",
        "    'ShuffleNetV2', 'shufflenet_v2_x0_5', 'shufflenet_v2_x1_0',\n",
        "    'shufflenet_v2_x1_5', 'shufflenet_v2_x2_0'\n",
        "]\n",
        "\n",
        "model_urls = {\n",
        "    'shufflenetv2_x0.5': 'https://download.pytorch.org/models/shufflenetv2_x0.5-f707e7126e.pth',\n",
        "    'shufflenetv2_x1.0': 'https://download.pytorch.org/models/shufflenetv2_x1-5666bf0f80.pth',\n",
        "    'shufflenetv2_x1.5': None,\n",
        "    'shufflenetv2_x2.0': None,\n",
        "}\n",
        "\n",
        "\n",
        "def channel_shuffle(x: Tensor, groups: int) -> Tensor:\n",
        "    batchsize, num_channels, height, width = x.size()\n",
        "    channels_per_group = num_channels // groups\n",
        "\n",
        "    # reshape\n",
        "    x = x.view(batchsize, groups,\n",
        "               channels_per_group, height, width)\n",
        "\n",
        "    x = torch.transpose(x, 1, 2).contiguous()\n",
        "\n",
        "    # flatten\n",
        "    x = x.view(batchsize, -1, height, width)\n",
        "\n",
        "    return x\n",
        "\n",
        "\n",
        "class InvertedResidual(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        inp: int,\n",
        "        oup: int,\n",
        "        stride: int\n",
        "    ) -> None:\n",
        "        super(InvertedResidual, self).__init__()\n",
        "\n",
        "        if not (1 <= stride <= 3):\n",
        "            raise ValueError('illegal stride value')\n",
        "        self.stride = stride\n",
        "\n",
        "        branch_features = oup // 2\n",
        "        assert (self.stride != 1) or (inp == branch_features << 1)\n",
        "\n",
        "        if self.stride > 1:\n",
        "            self.branch1 = nn.Sequential(\n",
        "                self.depthwise_conv(inp, inp, kernel_size=3, stride=self.stride, padding=1),\n",
        "                nn.BatchNorm2d(inp),\n",
        "                nn.Conv2d(inp, branch_features, kernel_size=1, stride=1, padding=0, bias=False),\n",
        "                nn.BatchNorm2d(branch_features),\n",
        "                nn.ReLU(inplace=True),\n",
        "            )\n",
        "        else:\n",
        "            self.branch1 = nn.Sequential()\n",
        "\n",
        "        self.branch2 = nn.Sequential(\n",
        "            nn.Conv2d(inp if (self.stride > 1) else branch_features,\n",
        "                      branch_features, kernel_size=1, stride=1, padding=0, bias=False),\n",
        "            nn.BatchNorm2d(branch_features),\n",
        "            nn.ReLU(inplace=True),\n",
        "            self.depthwise_conv(branch_features, branch_features, kernel_size=3, stride=self.stride, padding=1),\n",
        "            nn.BatchNorm2d(branch_features),\n",
        "            nn.Conv2d(branch_features, branch_features, kernel_size=1, stride=1, padding=0, bias=False),\n",
        "            nn.BatchNorm2d(branch_features),\n",
        "            nn.ReLU(inplace=True),\n",
        "        )\n",
        "\n",
        "    @staticmethod\n",
        "    def depthwise_conv(\n",
        "        i: int,\n",
        "        o: int,\n",
        "        kernel_size: int,\n",
        "        stride: int = 1,\n",
        "        padding: int = 0,\n",
        "        bias: bool = False\n",
        "    ) -> nn.Conv2d:\n",
        "        return nn.Conv2d(i, o, kernel_size, stride, padding, bias=bias, groups=i)\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        if self.stride == 1:\n",
        "            x1, x2 = x.chunk(2, dim=1)\n",
        "            out = torch.cat((x1, self.branch2(x2)), dim=1)\n",
        "        else:\n",
        "            out = torch.cat((self.branch1(x), self.branch2(x)), dim=1)\n",
        "\n",
        "        out = channel_shuffle(out, 2)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class ShuffleNetV2(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        stages_repeats: List[int],\n",
        "        stages_out_channels: List[int],\n",
        "        num_classes: int = 26,\n",
        "        ####################################################################################\n",
        "        #num_classes: int = 1000,\n",
        "        ####################################################################################\n",
        "        inverted_residual: Callable[..., nn.Module] = InvertedResidual\n",
        "    ) -> None:\n",
        "        super(ShuffleNetV2, self).__init__()\n",
        "\n",
        "        if len(stages_repeats) != 3:\n",
        "            raise ValueError('expected stages_repeats as list of 3 positive ints')\n",
        "        if len(stages_out_channels) != 5:\n",
        "            raise ValueError('expected stages_out_channels as list of 5 positive ints')\n",
        "        self._stage_out_channels = stages_out_channels\n",
        "        input_channels = 1\n",
        "        ####################################################################################\n",
        "        #input_channels = 3\n",
        "        ####################################################################################\n",
        "        output_channels = self._stage_out_channels[0]\n",
        "        self.conv1 = nn.Sequential(\n",
        "            nn.Conv2d(input_channels, output_channels, 3, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(output_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "        )\n",
        "        input_channels = output_channels\n",
        "\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "\n",
        "        # Static annotations for mypy\n",
        "        self.stage2: nn.Sequential\n",
        "        self.stage3: nn.Sequential\n",
        "        self.stage4: nn.Sequential\n",
        "        stage_names = ['stage{}'.format(i) for i in [2, 3, 4]]\n",
        "        for name, repeats, output_channels in zip(\n",
        "                stage_names, stages_repeats, self._stage_out_channels[1:]):\n",
        "            seq = [inverted_residual(input_channels, output_channels, 2)]\n",
        "            for i in range(repeats - 1):\n",
        "                seq.append(inverted_residual(output_channels, output_channels, 1))\n",
        "            setattr(self, name, nn.Sequential(*seq))\n",
        "            input_channels = output_channels\n",
        "\n",
        "        output_channels = self._stage_out_channels[-1]\n",
        "        self.conv5 = nn.Sequential(\n",
        "            nn.Conv2d(input_channels, output_channels, 1, 1, 0, bias=False),\n",
        "            nn.BatchNorm2d(output_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "        )\n",
        "\n",
        "        self.fc = nn.Linear(output_channels, num_classes)\n",
        "\n",
        "    def _forward_impl(self, x: Tensor) -> Tensor:\n",
        "        # See note [TorchScript super()]\n",
        "        x = self.conv1(x)\n",
        "        x = self.maxpool(x)\n",
        "        x = self.stage2(x)\n",
        "        x = self.stage3(x)\n",
        "        x = self.stage4(x)\n",
        "        x = self.conv5(x)\n",
        "        x = x.mean([2, 3])  # globalpool\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        return self._forward_impl(x)\n",
        "\n",
        "\n",
        "def _shufflenetv2(arch: str, pretrained: bool, progress: bool, *args: Any, **kwargs: Any) -> ShuffleNetV2:\n",
        "    model = ShuffleNetV2(*args, **kwargs)\n",
        "\n",
        "    if pretrained:\n",
        "        model_url = model_urls[arch]\n",
        "        if model_url is None:\n",
        "            raise NotImplementedError('pretrained {} is not supported as of now'.format(arch))\n",
        "        else:\n",
        "            state_dict = load_state_dict_from_url(model_url, progress=progress)\n",
        "            model.load_state_dict(state_dict)\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "def shufflenet_v2_x0_5(pretrained: bool = False, progress: bool = True, **kwargs: Any) -> ShuffleNetV2:\n",
        "    \"\"\"\n",
        "    Constructs a ShuffleNetV2 with 0.5x output channels, as described in\n",
        "    `\"ShuffleNet V2: Practical Guidelines for Efficient CNN Architecture Design\"\n",
        "    <https://arxiv.org/abs/1807.11164>`_.\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "        progress (bool): If True, displays a progress bar of the download to stderr\n",
        "    \"\"\"\n",
        "    return _shufflenetv2('shufflenetv2_x0.5', pretrained, progress,\n",
        "                         [4, 8, 4], [24, 48, 96, 192, 1024], **kwargs)\n",
        "\n",
        "\n",
        "def shufflenet_v2_x1_0(pretrained: bool = False, progress: bool = True, **kwargs: Any) -> ShuffleNetV2:\n",
        "    \"\"\"\n",
        "    Constructs a ShuffleNetV2 with 1.0x output channels, as described in\n",
        "    `\"ShuffleNet V2: Practical Guidelines for Efficient CNN Architecture Design\"\n",
        "    <https://arxiv.org/abs/1807.11164>`_.\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "        progress (bool): If True, displays a progress bar of the download to stderr\n",
        "    \"\"\"\n",
        "    return _shufflenetv2('shufflenetv2_x1.0', pretrained, progress,\n",
        "                         [4, 8, 4], [24, 116, 232, 464, 1024], **kwargs)\n",
        "\n",
        "\n",
        "def shufflenet_v2_x1_5(pretrained: bool = False, progress: bool = True, **kwargs: Any) -> ShuffleNetV2:\n",
        "    \"\"\"\n",
        "    Constructs a ShuffleNetV2 with 1.5x output channels, as described in\n",
        "    `\"ShuffleNet V2: Practical Guidelines for Efficient CNN Architecture Design\"\n",
        "    <https://arxiv.org/abs/1807.11164>`_.\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "        progress (bool): If True, displays a progress bar of the download to stderr\n",
        "    \"\"\"\n",
        "    return _shufflenetv2('shufflenetv2_x1.5', pretrained, progress,\n",
        "                         [4, 8, 4], [24, 176, 352, 704, 1024], **kwargs)\n",
        "\n",
        "\n",
        "def shufflenet_v2_x2_0(pretrained: bool = False, progress: bool = True, **kwargs: Any) -> ShuffleNetV2:\n",
        "    \"\"\"\n",
        "    Constructs a ShuffleNetV2 with 2.0x output channels, as described in\n",
        "    `\"ShuffleNet V2: Practical Guidelines for Efficient CNN Architecture Design\"\n",
        "    <https://arxiv.org/abs/1807.11164>`_.\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "        progress (bool): If True, displays a progress bar of the download to stderr\n",
        "    \"\"\"\n",
        "    return _shufflenetv2('shufflenetv2_x2.0', pretrained, progress,\n",
        "                         [4, 8, 4], [24, 244, 488, 976, 2048], **kwargs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hkhrwqL1XxVd"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OSvHdnCPjqwV"
      },
      "source": [
        "# note"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OOvV4Cqbjtki"
      },
      "source": [
        " - conv2d\n",
        "\n",
        "[[Pytorch] torch.nn.Conv2d](https://hichoe95.tistory.com/14)\n",
        "\n",
        "[Conv2d layer를 겹겹이 쌓을 때 최종 output volume 계산법](https://teddylee777.github.io/pytorch/conv2d-output-size-계산법)\n",
        "\n",
        "[Pytorch Conv2d 함수 다루기](https://gaussian37.github.io/dl-pytorch-conv2d/)\n",
        "\n",
        " - tensor manipulation\n",
        "\n",
        "[텐서 조작하기(Tensor Manipulation)](https://wikidocs.net/52846)\n",
        "\n",
        " - 코랩 단축키\n",
        "\n",
        "[[파이썬] 구글 코랩(Colab) 단축키 모음](https://surfonmedia.tistory.com/1)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O99c5-C3kSpD"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zrzjCGu5s_yM"
      },
      "source": [
        "# 학습"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 120,
          "referenced_widgets": [
            "99211bc1dc08465e9475b3df03e42a26",
            "6a8b72c204d64a9aba11363d1e0b6713",
            "0181676b436c4a028ccf4aeaa14b7687",
            "67b38fa9008a48798b7eb042f6ab5fe5",
            "27c1be520e0c43f69ed0c8dd14e3848a",
            "f07f551fbd954ddcb867f63d8974fad4",
            "a69a6c648ee3470aa7d11a6251e1bf15",
            "707a90ccee714b3d8b418aa2d4c53ca7"
          ]
        },
        "id": "O-sOj3dnvc5u",
        "outputId": "34e7757c-a8d5-4ec7-aa2d-6371a4b6ebc2"
      },
      "source": [
        "import torch\n",
        "shu_model = torch.hub.load('pytorch/vision:v0.6.0', 'shufflenet_v2_x1_0', pretrained=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading: \"https://github.com/pytorch/vision/archive/v0.6.0.zip\" to /root/.cache/torch/hub/v0.6.0.zip\n",
            "Downloading: \"https://download.pytorch.org/models/shufflenetv2_x1-5666bf0f80.pth\" to /root/.cache/torch/hub/checkpoints/shufflenetv2_x1-5666bf0f80.pth\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "99211bc1dc08465e9475b3df03e42a26",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=9218294.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZwQEddfkwCJ6"
      },
      "source": [
        "import torch\n",
        "from torch import Tensor\n",
        "import torch.nn as nn\n",
        "from torch.hub import load_state_dict_from_url\n",
        "from typing import Callable, Any, List\n",
        "\n",
        "\n",
        "__all__ = [\n",
        "    'ShuffleNetV2', 'shufflenet_v2_x0_5', 'shufflenet_v2_x1_0',\n",
        "    'shufflenet_v2_x1_5', 'shufflenet_v2_x2_0'\n",
        "]\n",
        "\n",
        "model_urls = {\n",
        "    'shufflenetv2_x0.5': 'https://download.pytorch.org/models/shufflenetv2_x0.5-f707e7126e.pth',\n",
        "    'shufflenetv2_x1.0': 'https://download.pytorch.org/models/shufflenetv2_x1-5666bf0f80.pth',\n",
        "    'shufflenetv2_x1.5': None,\n",
        "    'shufflenetv2_x2.0': None,\n",
        "}\n",
        "\n",
        "\n",
        "def channel_shuffle(x: Tensor, groups: int) -> Tensor:\n",
        "    batchsize, num_channels, height, width = x.size()\n",
        "    channels_per_group = num_channels // groups\n",
        "\n",
        "    # reshape\n",
        "    x = x.view(batchsize, groups,\n",
        "               channels_per_group, height, width)\n",
        "\n",
        "    x = torch.transpose(x, 1, 2).contiguous()\n",
        "\n",
        "    # flatten\n",
        "    x = x.view(batchsize, -1, height, width)\n",
        "\n",
        "    return x\n",
        "\n",
        "\n",
        "class InvertedResidual(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        inp: int,\n",
        "        oup: int,\n",
        "        stride: int\n",
        "    ) -> None:\n",
        "        super(InvertedResidual, self).__init__()\n",
        "\n",
        "        if not (1 <= stride <= 3):\n",
        "            raise ValueError('illegal stride value')\n",
        "        self.stride = stride\n",
        "\n",
        "        branch_features = oup // 2\n",
        "        assert (self.stride != 1) or (inp == branch_features << 1)\n",
        "\n",
        "        if self.stride > 1:\n",
        "            self.branch1 = nn.Sequential(\n",
        "                self.depthwise_conv(inp, inp, kernel_size=3, stride=self.stride, padding=1),\n",
        "                nn.BatchNorm2d(inp),\n",
        "                nn.Conv2d(inp, branch_features, kernel_size=1, stride=1, padding=0, bias=False),\n",
        "                nn.BatchNorm2d(branch_features),\n",
        "                nn.ReLU(inplace=True),\n",
        "            )\n",
        "        else:\n",
        "            self.branch1 = nn.Sequential()\n",
        "\n",
        "        self.branch2 = nn.Sequential(\n",
        "            nn.Conv2d(inp if (self.stride > 1) else branch_features,\n",
        "                      branch_features, kernel_size=1, stride=1, padding=0, bias=False),\n",
        "            nn.BatchNorm2d(branch_features),\n",
        "            nn.ReLU(inplace=True),\n",
        "            self.depthwise_conv(branch_features, branch_features, kernel_size=3, stride=self.stride, padding=1),\n",
        "            nn.BatchNorm2d(branch_features),\n",
        "            nn.Conv2d(branch_features, branch_features, kernel_size=1, stride=1, padding=0, bias=False),\n",
        "            nn.BatchNorm2d(branch_features),\n",
        "            nn.ReLU(inplace=True),\n",
        "        )\n",
        "\n",
        "    @staticmethod\n",
        "    def depthwise_conv(\n",
        "        i: int,\n",
        "        o: int,\n",
        "        kernel_size: int,\n",
        "        stride: int = 1,\n",
        "        padding: int = 0,\n",
        "        bias: bool = False\n",
        "    ) -> nn.Conv2d:\n",
        "        return nn.Conv2d(i, o, kernel_size, stride, padding, bias=bias, groups=i)\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        if self.stride == 1:\n",
        "            x1, x2 = x.chunk(2, dim=1)\n",
        "            out = torch.cat((x1, self.branch2(x2)), dim=1)\n",
        "        else:\n",
        "            out = torch.cat((self.branch1(x), self.branch2(x)), dim=1)\n",
        "\n",
        "        out = channel_shuffle(out, 2)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class ShuffleNetV2(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        stages_repeats: List[int],\n",
        "        stages_out_channels: List[int],\n",
        "        num_classes: int = 1000,\n",
        "        inverted_residual: Callable[..., nn.Module] = InvertedResidual\n",
        "    ) -> None:\n",
        "        super(ShuffleNetV2, self).__init__()\n",
        "\n",
        "        if len(stages_repeats) != 3:\n",
        "            raise ValueError('expected stages_repeats as list of 3 positive ints')\n",
        "        if len(stages_out_channels) != 5:\n",
        "            raise ValueError('expected stages_out_channels as list of 5 positive ints')\n",
        "        self._stage_out_channels = stages_out_channels\n",
        "\n",
        "        input_channels = 3\n",
        "        output_channels = self._stage_out_channels[0]\n",
        "        self.conv1 = nn.Sequential(\n",
        "            nn.Conv2d(input_channels, output_channels, 3, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(output_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "        )\n",
        "        input_channels = output_channels\n",
        "\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "\n",
        "        # Static annotations for mypy\n",
        "        self.stage2: nn.Sequential\n",
        "        self.stage3: nn.Sequential\n",
        "        self.stage4: nn.Sequential\n",
        "        stage_names = ['stage{}'.format(i) for i in [2, 3, 4]]\n",
        "        for name, repeats, output_channels in zip(\n",
        "                stage_names, stages_repeats, self._stage_out_channels[1:]):\n",
        "            seq = [inverted_residual(input_channels, output_channels, 2)]\n",
        "            for i in range(repeats - 1):\n",
        "                seq.append(inverted_residual(output_channels, output_channels, 1))\n",
        "            setattr(self, name, nn.Sequential(*seq))\n",
        "            input_channels = output_channels\n",
        "\n",
        "        output_channels = self._stage_out_channels[-1]\n",
        "        self.conv5 = nn.Sequential(\n",
        "            nn.Conv2d(input_channels, output_channels, 1, 1, 0, bias=False),\n",
        "            nn.BatchNorm2d(output_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "        )\n",
        "\n",
        "        self.fc = nn.Linear(output_channels, num_classes)\n",
        "\n",
        "    def _forward_impl(self, x: Tensor) -> Tensor:\n",
        "        # See note [TorchScript super()]\n",
        "        x = self.conv1(x)\n",
        "        x = self.maxpool(x)\n",
        "        x = self.stage2(x)\n",
        "        x = self.stage3(x)\n",
        "        x = self.stage4(x)\n",
        "        x = self.conv5(x)\n",
        "        x = x.mean([2, 3])  # globalpool\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        return self._forward_impl(x)\n",
        "\n",
        "\n",
        "def _shufflenetv2(arch: str, pretrained: bool, progress: bool, *args: Any, **kwargs: Any) -> ShuffleNetV2:\n",
        "    model = ShuffleNetV2(*args, **kwargs)\n",
        "\n",
        "    if pretrained:\n",
        "        model_url = model_urls[arch]\n",
        "        if model_url is None:\n",
        "            raise NotImplementedError('pretrained {} is not supported as of now'.format(arch))\n",
        "        else:\n",
        "            state_dict = load_state_dict_from_url(model_url, progress=progress)\n",
        "            model.load_state_dict(state_dict)\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "def shufflenet_v2_x0_5(pretrained: bool = False, progress: bool = True, **kwargs: Any) -> ShuffleNetV2:\n",
        "    \"\"\"\n",
        "    Constructs a ShuffleNetV2 with 0.5x output channels, as described in\n",
        "    `\"ShuffleNet V2: Practical Guidelines for Efficient CNN Architecture Design\"\n",
        "    <https://arxiv.org/abs/1807.11164>`_.\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "        progress (bool): If True, displays a progress bar of the download to stderr\n",
        "    \"\"\"\n",
        "    return _shufflenetv2('shufflenetv2_x0.5', pretrained, progress,\n",
        "                         [4, 8, 4], [24, 48, 96, 192, 1024], **kwargs)\n",
        "\n",
        "\n",
        "def shufflenet_v2_x1_0(pretrained: bool = False, progress: bool = True, **kwargs: Any) -> ShuffleNetV2:\n",
        "    \"\"\"\n",
        "    Constructs a ShuffleNetV2 with 1.0x output channels, as described in\n",
        "    `\"ShuffleNet V2: Practical Guidelines for Efficient CNN Architecture Design\"\n",
        "    <https://arxiv.org/abs/1807.11164>`_.\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "        progress (bool): If True, displays a progress bar of the download to stderr\n",
        "    \"\"\"\n",
        "    return _shufflenetv2('shufflenetv2_x1.0', pretrained, progress,\n",
        "                         [4, 8, 4], [24, 116, 232, 464, 1024], **kwargs)\n",
        "\n",
        "\n",
        "def shufflenet_v2_x1_5(pretrained: bool = False, progress: bool = True, **kwargs: Any) -> ShuffleNetV2:\n",
        "    \"\"\"\n",
        "    Constructs a ShuffleNetV2 with 1.5x output channels, as described in\n",
        "    `\"ShuffleNet V2: Practical Guidelines for Efficient CNN Architecture Design\"\n",
        "    <https://arxiv.org/abs/1807.11164>`_.\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "        progress (bool): If True, displays a progress bar of the download to stderr\n",
        "    \"\"\"\n",
        "    return _shufflenetv2('shufflenetv2_x1.5', pretrained, progress,\n",
        "                         [4, 8, 4], [24, 176, 352, 704, 1024], **kwargs)\n",
        "\n",
        "\n",
        "def shufflenet_v2_x2_0(pretrained: bool = False, progress: bool = True, **kwargs: Any) -> ShuffleNetV2:\n",
        "    \"\"\"\n",
        "    Constructs a ShuffleNetV2 with 2.0x output channels, as described in\n",
        "    `\"ShuffleNet V2: Practical Guidelines for Efficient CNN Architecture Design\"\n",
        "    <https://arxiv.org/abs/1807.11164>`_.\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "        progress (bool): If True, displays a progress bar of the download to stderr\n",
        "    \"\"\"\n",
        "    return _shufflenetv2('shufflenetv2_x2.0', pretrained, progress,\n",
        "                         [4, 8, 4], [24, 244, 488, 976, 2048], **kwargs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tPymVdbQvcyw"
      },
      "source": [
        "class my_sufflenet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(my_sufflenet, self).__init__()\n",
        "        self.conv2d = nn.Conv2d(1, 3, 3, stride=1)\n",
        "        self.ShuffleNetV2 =ShuffleNetV2()\n",
        "        self.FC = nn.Linear(1000, 26)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # resnet의 입력은 [3, N, N]으로\n",
        "        # 3개의 채널을 갖기 때문에\n",
        "        # resnet 입력 전에 conv2d를 한 층 추가\n",
        "        x = F.relu(self.conv2d(x))\n",
        "\n",
        "        # resnet18을 추가\n",
        "        x = F.relu(self.ShuffleNetV2(x))\n",
        "\n",
        "        # 마지막 출력에 nn.Linear를 추가\n",
        "        # multilabel을 예측해야 하기 때문에\n",
        "        # softmax가 아닌 sigmoid를 적용\n",
        "        x = torch.sigmoid(self.FC(x))\n",
        "        return x\n",
        "# 모델 선언\n",
        "suf_model = my_sufflenet()\n",
        "suf_model.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WV-7-qKMvcjZ"
      },
      "source": [
        "from torchsummary import summary\n",
        "summary(suf_model, (1,256,256))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 463
        },
        "id": "S6JXxUmctA5D",
        "outputId": "307715a7-a341-4365-f0d7-45e49405891d"
      },
      "source": [
        "# cross validation을 적용하기 위해 KFold 생성\n",
        "from sklearn.model_selection import KFold\n",
        "kfold = KFold(n_splits=5, shuffle=True, random_state=0)\n",
        "\n",
        "# dirty_mnist_answer에서 train_idx와 val_idx를 생성\n",
        "best_models = [] # 폴드별로 가장 validation acc가 높은 모델 저장\n",
        "for fold_index, (trn_idx, val_idx) in enumerate(kfold.split(dirty_mnist_answer),1):\n",
        "    print(f'[fold: {fold_index}]')\n",
        "    # cuda cache 초기화\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    #train fold, validation fold 분할\n",
        "    train_answer = dirty_mnist_answer.iloc[trn_idx]\n",
        "    test_answer  = dirty_mnist_answer.iloc[val_idx]\n",
        "\n",
        "    #Dataset 정의\n",
        "    train_dataset = DatasetMNIST(\"dirty_mnist_2nd/\", train_answer)\n",
        "    valid_dataset = DatasetMNIST(\"dirty_mnist_2nd/\", test_answer)\n",
        "\n",
        "    #DataLoader 정의\n",
        "    train_data_loader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size = 128,\n",
        "        shuffle = False,\n",
        "        num_workers = 3\n",
        "    )\n",
        "\n",
        "    valid_data_loader = DataLoader(\n",
        "        valid_dataset,\n",
        "        batch_size = 32,\n",
        "        shuffle = False,\n",
        "        num_workers = 3\n",
        "    )\n",
        "\n",
        "    # 모델 선언\n",
        "    #model = torch.hub.load('moskomule/senet.pytorch','se_resnet20',num_classes=10)\n",
        "    #model = SENet18()\n",
        "    model = my_sufflenet()\n",
        "    #model = hub_model\n",
        "    #model = CifarSEResNet(CifarSEBasicBlock, 3)\n",
        "    #model = shufflenet_v2_x1_0()\n",
        "    model.to(device)# gpu에 모델 할당\n",
        "\n",
        "    # 훈련 옵션 설정\n",
        "    optimizer = torch.optim.Adam(model.parameters(),\n",
        "                                lr = 0.001)\n",
        "    lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n",
        "                                                step_size = 5,\n",
        "                                                gamma = 0.75)\n",
        "    criterion = torch.nn.BCELoss()\n",
        "\n",
        "\n",
        "    # 훈련 시작\n",
        "    valid_acc_max = 0\n",
        "    for epoch in range(10):\n",
        "        print(epoch)\n",
        "        # 1개 epoch 훈련\n",
        "        train_acc_list = []\n",
        "\n",
        "        with tqdm(train_data_loader,#train_data_loader를 iterative하게 반환\n",
        "                total=train_data_loader.__len__(), # train_data_loader의 크기\n",
        "                unit=\"batch\") as train_bar:# 한번 반환하는 smaple의 단위는 \"batch\"\n",
        "            for sample in train_bar:\n",
        "                train_bar.set_description(f\"Train Epoch {epoch}\")\n",
        "\n",
        "                # 갱신할 변수들에 대한 모든 변화도를 0으로 초기화\n",
        "                # 참고)https://tutorials.pytorch.kr/beginner/pytorch_with_examples.html\n",
        "                optimizer.zero_grad()\n",
        "                images, labels = sample['image'], sample['label']\n",
        "                # tensor를 gpu에 올리기 \n",
        "                images = images.to(device)\n",
        "                labels = labels.to(device)\n",
        "               \n",
        "                # 모델의 dropoupt, batchnormalization를 train 모드로 설정\n",
        "                model.train()\n",
        "                # .forward()에서 중간 노드의 gradient를 계산\n",
        "                with torch.set_grad_enabled(True):\n",
        "                    # 모델 예측\n",
        "                    probs = model(images)\n",
        "                    # loss 계산\n",
        "                    loss = criterion(probs, labels)\n",
        "                    # 중간 노드의 gradient로\n",
        "                    # backpropagation을 적용하여\n",
        "                    # gradient 계산\n",
        "                    loss.backward()\n",
        "                    # weight 갱신\n",
        "                    optimizer.step()\n",
        "\n",
        "                    # train accuracy 계산\n",
        "                    probs  = probs.cpu().detach().numpy()\n",
        "                    labels = labels.cpu().detach().numpy()\n",
        "                    preds = probs > 0.5\n",
        "                    batch_acc = (labels == preds).mean()\n",
        "                    train_acc_list.append(batch_acc)\n",
        "                    train_acc = np.mean(train_acc_list)\n",
        "\n",
        "                # 현재 progress bar에 현재 미니배치의 loss 결과 출력\n",
        "                train_bar.set_postfix(train_loss= loss.item(),\n",
        "                                      train_acc = train_acc)\n",
        "                \n",
        "\n",
        "        # 1개 epoch학습 후 Validation 점수 계산\n",
        "        valid_acc_list = []\n",
        "        with tqdm(valid_data_loader,\n",
        "                total=valid_data_loader.__len__(),\n",
        "                unit=\"batch\") as valid_bar:\n",
        "            for sample in valid_bar:\n",
        "                valid_bar.set_description(f\"Valid Epoch {epoch}\")\n",
        "                optimizer.zero_grad()\n",
        "                images, labels = sample['image'], sample['label']\n",
        "                images = images.to(device)\n",
        "                labels = labels.to(device)\n",
        "\n",
        "                # 모델의 dropoupt, batchnormalization를 eval모드로 설정\n",
        "                model.eval()\n",
        "                # .forward()에서 중간 노드의 gradient를 계산\n",
        "                with torch.no_grad():\n",
        "                    # validation loss만을 계산\n",
        "                    probs  = model(images)\n",
        "                    valid_loss = criterion(probs, labels)\n",
        "\n",
        "                    # train accuracy 계산\n",
        "                    probs  = probs.cpu().detach().numpy()\n",
        "                    labels = labels.cpu().detach().numpy()\n",
        "                    preds = probs > 0.5\n",
        "                    batch_acc = (labels == preds).mean()\n",
        "                    valid_acc_list.append(batch_acc)\n",
        "\n",
        "                valid_acc = np.mean(valid_acc_list)\n",
        "                valid_bar.set_postfix(valid_loss = valid_loss.item(),\n",
        "                                      valid_acc = valid_acc)\n",
        "            \n",
        "        # Learning rate 조절\n",
        "        lr_scheduler.step()\n",
        "\n",
        "        # 모델 저장\n",
        "        if valid_acc_max < valid_acc:\n",
        "            valid_acc_max = valid_acc\n",
        "            best_model = model\n",
        "            MODEL = \"ShuffleNetV2\"\n",
        "            # 모델을 저장할 구글 드라이브 경로\n",
        "            path = \"/content/drive/MyDrive/21-1/models_2/\"\n",
        "            torch.save(best_model, f'{path}{fold_index}_{MODEL}_{valid_loss.item():2.4f}_epoch_{epoch}.pth')\n",
        "\n",
        "    # 폴드별로 가장 좋은 모델 저장\n",
        "    best_models.append(best_model)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/313 [00:00<?, ?batch/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[fold: 1]\n",
            "0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Train Epoch 0:  67%|██████▋   | 210/313 [54:21<26:39, 15.53s/batch, train_acc=0.538, train_loss=0.689]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-ac317411a519>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     77\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_grad_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m                     \u001b[0;31m# 모델 예측\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m                     \u001b[0mprobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m                     \u001b[0;31m# loss 계산\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m                     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-e709354f1cd2>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;31m# resnet18을 추가\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshufflenet_v2_x1_0\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;31m# 마지막 출력에 nn.Linear를 추가\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-dcb1d37dfdd7>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 161\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-dcb1d37dfdd7>\u001b[0m in \u001b[0;36m_forward_impl\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    150\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaxpool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 152\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstage2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    153\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstage3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstage4\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-dcb1d37dfdd7>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m             \u001b[0mx1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbranch2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbranch1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbranch2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/batchnorm.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    134\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunning_mean\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrack_running_stats\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunning_var\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrack_running_stats\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 136\u001b[0;31m             self.weight, self.bias, bn_training, exponential_average_factor, self.eps)\n\u001b[0m\u001b[1;32m    137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mbatch_norm\u001b[0;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[1;32m   2056\u001b[0m     return torch.batch_norm(\n\u001b[1;32m   2057\u001b[0m         \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrunning_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrunning_var\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2058\u001b[0;31m         \u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmomentum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackends\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcudnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2059\u001b[0m     )\n\u001b[1;32m   2060\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UvpPqlcBt5t0"
      },
      "source": [
        "# gpu에 올라가 있는 tensor -> cpu로 이동 -> numpy array로 변환\n",
        "sample_images = images.cpu().detach().numpy()\n",
        "sample_prob = probs\n",
        "sample_labels = labels\n",
        "\n",
        "idx = 1\n",
        "plt.imshow(sample_images[idx][0])\n",
        "plt.title(\"sample input image\")\n",
        "plt.show()\n",
        "\n",
        "print('예측값 : ',dirty_mnist_answer.columns[1:][sample_prob[idx] > 0.5])\n",
        "print('정답값 : ', dirty_mnist_answer.columns[1:][sample_labels[idx] > 0.5])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4G-HiLe0t_Za"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}